---
title: "Mathematical description of models used in bage"
output:
  bookdown::html_document2:
    base_format: rmarkdown::html_vignette
    fig_caption: yes
    toc: true
    toc_depth: 2
    number_sections: true
pkgdown:
  as_is: true
vignette: >
  %\VignetteIndexEntry{Mathematical description of models used in bage}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
bibliography: references.bib
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

$\newcommand{\ind}{\, \overset{\text{ind}}{\sim} \,}$


```{r setup}
library(bage)
```

# Introduction

Specification document. Aims to give complete mathematical description of models used by bage.

Note: this is describing the models we want to get to soon, not the ones we currently implement.

# Likelihood {#sec:lik}

## Models

### Poisson {#sec:pois}

Let $y_i$ be a count of events in cell $i = 1, \cdots, n$ and let $w_i$ be the corresponding exposure measure, with the possibility that $w_i \equiv 1$. The likelihood under the Poisson model is then
\begin{align}
  y_i \mid \gamma_i, w_i & \ind \text{Poisson}(\gamma_i w_i) (\#eq:lik-pois-1) \\
  \gamma_i \mid \mu_i, \xi & \ind \text{Gamma}\left(\xi^{-1}, (\mu_i \xi)^{-1}\right),  (\#eq:lik-pois-2)
\end{align}
using the shape-rates parameterisation of the Gamma distribution. Parameter $\xi$ governs dispersion, with
\begin{equation}
  \text{var}(\gamma_i \mid \mu_i, \xi) = \xi \mu_i^2
\end{equation}
and
\begin{equation}
  \text{var}(y_i \mid \mu_i, \xi, w_i) = (1 + \xi \mu_i w_i ) \times \mu_i w_i.
\end{equation}
We allow $\xi$ to equal 0, in which case the model reduces to 
\begin{equation}
  y_i \mid \mu_i, w_i \ind \text{Poisson}(\mu_i w_i).
\end{equation}

For $\xi > 0$, Equations \@ref(eq:lik-pois-1) and \@ref(eq:lik-pois-2) are equivalent to
\begin{equation}
  y_i \sim \text{NegBinom}\left(\xi^{-1}, (1 + \mu_i w_i \xi)^{-1}\right)
\end{equation}
[@norton2018sampling; @simpson2022priors].
This is the format we use internally for estimation. When values for $\gamma_i$ are needed, we generate them on the fly, using the fact that
\begin{equation}
  \gamma_i \mid y_i, \mu_i, \xi \ind \text{Gamma}\left(y_i + \xi^{-1}, w_i + (\xi \mu_i)^{-1}\right).
\end{equation}


### Binomial {#sec:binom}

The likelihood under the binomial model is
\begin{align}
  y_i \mid \gamma_i, w_i & \ind \text{Binomial}(w_i, \gamma_i) (\#eq:lik-binom-1) \\
  \gamma_i \mid \mu_i, \xi & \ind \text{Beta}\left(\xi^{-1} \mu_i, \xi^{-1}(1 - \mu_i)\right). (\#eq:lik-binom-2)
\end{align}
Parameter $\xi$ again governs dispersion, with
\begin{equation}
  \text{var}(\gamma_i \mid \mu_i, \xi) =  \frac{\xi}{1 + \xi} \times \mu_i (1 -\mu_i)
\end{equation}
and
\begin{equation}
  \text{var}(y_i \mid w_i, \mu_i, \xi) =  \frac{\xi w_i + 1}{\xi + 1} \times w_i \mu_i (1 - \mu_i).
\end{equation}

We allow $\xi$ to equal 0, in which case the model reduces to 
\begin{equation}
  y_i \mid w_i, \mu_i \ind \text{Binom}(w_i, \mu_i).
\end{equation}
Equations \@ref(eq:lik-binom-1) and \@ref(eq:lik-binom-2) are then equivalent to
\begin{equation}
  y_i | w_i, \mu_i, \xi \ind \text{BetaBinom}\left(w_i, \xi^{-1} \mu_i, \xi^{-1} (1 - \mu_i) \right),
\end{equation}
which is what we use internally. Values for $\gamma_i$ can be generated using
\begin{equation}
  \gamma_i \mid y_i, w_i, \mu_i, \xi \sim \text{Beta}\left(y_i + \xi^{-1} \mu_i, w_i - y_i + \xi^{-1}(1-\mu_i) \right).
\end{equation}


### Normal {#sec:norm}

\begin{equation}
  y_i \sim \mathcal{N}(\mu_i, w_i^{-1}\xi^2)
\end{equation}
where the $w_i$ are some form of weights, with the possibility that $w_i \equiv 1$.

Response $y_i$ is standardised to have mean 0 and standard deviation 1. We set
\begin{equation}
  y_i = \frac{y_i^{*} - \bar{y}^*}{s^*}
\end{equation}
where the $y_i^*$ are the values originally supplied by the user, and $\bar{y}^*$ and $s^*$ are the mean and standard deviation of the $y_i^*$.

## Missing values

For any given cell $i$, one or both of $y_i$ and $w_i$ may be missing. In this case, cell $i$ is omitted from the likelihood.

# Model for means

Let $\pmb{\mu} = (\mu_1, \cdots, \mu_n)^{\top}$. Our model for $\pmb{\mu}$ is
\begin{equation}
  g(\pmb{\mu}) = \sum_{m=0}^{M} \pmb{X}^{(m)} (\pmb{\beta}^{(m)} + \pmb{\lambda}^{(m)}) + \pmb{Z} \pmb{\zeta} (\#eq:means)
\end{equation}
where

- $g$ is a log function in Poisson models, a logit function in binomial models, and an identity function in normal models;
- $\beta^{(0)}$ is an intercept;
- $\pmb{\beta}^{(m)}$, $m=1,\cdots,M$ is a vector with $J_m$ elements describing a main effect or interaction formed from the dimensions of data $\pmb{y}$;
- $\pmb{\lambda}^{(m)}$ is a seasonal effect, which is non-zero only if $\pmb{\beta}^{(m)}$ includes a time dimension;
- $\pmb{X}^{(m)}$ is an $n \times J_m$ matrix of 1s and 0s, the $i$th row of which picks out the element of $\pmb{\beta}^{(m)}$ and (if $\pmb{\lambda}^{(m)}$ exists) of $\pmb{\lambda}^{(m)}$, that is used with cell $i$;
- $\pmb{Z}$ is a $n \times P$ matrix of covariates; and
- $\pmb{\zeta}$ is a coefficent vector with $P$ elements.

Priors for the $\pmb{\beta}^{(m)}$ are discussed in sections \@ref(sec:pr-int)--\@ref(sec:pr-interact), priors for seasonal effect $\pmb{\lambda}^{(m)}$ are discussed in Section \@ref(sec:pr-season), and priors for coefficient vector $\pmb{\zeta}$ are discussed in Section \@ref(sec:pr-cov).


# Design of priors for main effects {#sec:pr-main-over}

## Model

In all priors for  main effects, we require, for identifiability, that
\begin{equation}
  \text{E}\left(\sum_{j=1}^{J_m} \beta_j^{(m)}\right) = 0.
\end{equation}

## Contribution to posterior density

TMB requires that we provide a function to calculate contribution to the posterior density of each prior. The function does not need to be normalised: any quantities that do not vary with the parameters can be omitted.

In TMB itself, densitities are specified on the log scale. Here, however, we present them on the original scale.

## Prediction

In some applications, we need to predict outside that data, which requires generating values $\beta_{J_m + h}^{(m)}$, $h = 1, 2, \cdots$. We do not allow predicted values with multivariate normal priors (Section \@ref(sec:pr-mnorm)), P-Spline priors (Section \@ref(sec:pr-spline)), or SVD priors (Section \@ref(sec:pr-svd)). For all other priors, we are able to obtain expressions for
\begin{equation}
  p(\beta_{J_m+h+1}^{(m)} \mid \beta_{J_m+h}^{(m)}, \cdots, \beta_1^{(m)}, \pmb{\psi}^{(m)}), \quad h = 0, 1, \cdots, (\#eq:predict) 
\end{equation}
where $\pmb{\psi}^{(m)}$ is a vector containing hyper-parameters. Equation \@ref(eq:predict) can be applied recursively to generate predictions for any prediction interval. Index $J_m+h$ normally refers to time, but can refer to any dimension.

Predicted values are not centered or constrained, and in general,
\begin{equation}
  \text{E}\left(\sum_{j=1}^{J_m + h} \beta_j^{(m)}\right) \neq 0.
\end{equation}


## Code

```
set_prior(formula)
```

- `formula` is a two-sided R formula with the name of the main effect on the left hand side, and the call to the prior function on the right

eg

```
set_prior(region ~ N())
set_prior(time ~ Lin(scale = 0.2))
```


# Priors for main effects {#sec:pr-main-specific}

## Independent normal {#sec:pr-inorm}

### Model

\begin{align}
  \beta_j^{(m)} | \tau_m & \ind \mathcal{N}\left(0, \tau_m^2 \right) \\
  \tau_m & \sim \mathcal{N}^+\left(0, A_{\tau}^{(m)2}\right)
\end{align}

### Contribution to posterior density

\begin{equation}
  \mathcal{N}(\tau_m | 0, A_{\tau}^{(m)2}) \prod_{j=1}^{J_m} \mathcal{N}(\beta_j^{(m)} \mid 0, \tau_m^2) 
\end{equation}


### Prediction

\begin{equation}
    \beta_{J_m+h+1}^{(m)} \sim \mathcal{N}(0, \tau_m^2)
\end{equation}


### Code

```
N(s = 1)
```

- `s` is $A_{\tau}^{(m)}$. Defaults to 1.

## Fixed normal {#sec:pr-fnorm}

### Model

\begin{equation}
  \beta_j^{(m)} | A_{\beta}^{(m)} \ind \mathcal{N}\left(0, A_{\beta}^{(m)2}\right)
\end{equation}

### Contribution to posterior density

\begin{equation}
  \prod_{j=1}^{J_m} \mathcal{N}(\beta_j^{(m)} \mid 0, A_{\beta}^{(m)2}) 
\end{equation}


### Prediction

\begin{equation}
    \beta_{J_m+h+1}^{(m)} \sim \mathcal{N}(0, A_{\beta}^{(m)2})
\end{equation}


### Code

```
NFix(sd = 1)
```

- `sd` is $A_{\tau}^{(m)}$. Defaults to 1.


## Multivariate normal {#sec:pr-mnorm}


### Model

\begin{align}
  \pmb{\beta}^{(m)} & \sim \mathcal{N}\left(\pmb{0},  \tau_m^2 \pmb{V}^{(m)} \right) \\
  \tau_m & \sim \mathcal{N}^+\left(0, A_{\tau}^{(m)2}\right)
\end{align}
where $\pmb{V}^{(m)}$ is a $J_m \times J_m$ symmetric positive-definite matrix supplied by the user.

### Contribution to posterior density

\begin{equation}
  \mathcal{N}(\tau_m | 0, A_{\tau}^{(m)2}) \mathcal{N}\left(\pmb{\beta}^{(m)} \mid \pmb{0},  \tau_m^2 \pmb{V}^{(m)} \right)
\end{equation}


### Prediction

Main effects with a multivariate normal prior cannot be predicted.


### Code

```
MVN(V, s = 1)
```

- `V` is $\pmb{V}^{(m)}$
- `s` is $A_{\tau}^{(m)}$. Defaults to 1.


## First-order random walk {#sec:pr-rw}

### Model

\begin{equation}
  \beta_{j+1}^{(m)} \sim \mathcal{N}(\beta_j^{(m)}, \tau_m^2),
\end{equation}
$j = 1, \cdots, J_m - 1$, together with the soft constraint
\begin{equation}
  \sum_{j=1}^{J_m} \beta_j^{(m)} \sim \mathcal{N}(0, 1).
\end{equation}

### Contribution to posterior density

\begin{equation}
  \mathcal{N}(\tau_m | 0, A_{\tau}^{(m)2}) \mathcal{N}\left( {\textstyle\sum}_{j=1}^{J_m} \beta_j^{(m)} \mid 0, 1 \right)  \prod_{j=2}^{J_m} \mathcal{N}\left(\beta_j^{(m)} - \beta_{j-1}^{(m)} \mid 0, \tau_m^2 \right) 
\end{equation}

### Prediction

\begin{equation}
  \beta_{J_m+h+1}^{(m)} \sim \mathcal{N}(\beta_{J_m+h}^{(m)}, \tau_m^2)
\end{equation}


### Code

```
RW(s = 1)
```

- `s` is $A_{\tau}^{(m)}$. Defaults to 1.


## Second-order random walk {#sec:pr-rw2}

### Model

\begin{equation}
  \beta_{j+2}^{(m)} \sim \mathcal{N}(2 \beta_{j+1}^{(m)} - \beta_{j}^{(m)}, \tau_m^2),
\end{equation}
$j = 1, \cdots, J_m - 2$, together with the soft constraint
\begin{equation}
  \sum_{j=1}^{J_m} \beta_j^{(m)} \sim \mathcal{N}(0, 1).
\end{equation}

### Contribution to posterior density

\begin{equation}
  \mathcal{N}(\tau_m | 0, A_{\tau}^{(m)2}) \mathcal{N}\left( {\textstyle\sum}_{j=1}^{J_m} \beta_j^{(m)} \mid 0, 1 \right)  \prod_{j=3}^{J_m} \mathcal{N}\left(\beta_j^{(m)} - 2 \beta_{j-1}^{(m)} + \beta_{j-2}^{(m)} \mid 0, \tau_m^2 \right) 
\end{equation}

### Prediction

\begin{equation}
  \beta_{J_m+h+1}^{(m)} \sim \mathcal{N}(2 \beta_{J_m + h}^{(m)} - \beta_{J_m + h -1}^{(m)}, \tau_m^2)
\end{equation}


### Code

```
RW2(s = 1)
```

- `s` is $A_{\tau}^{(m)}$. Defaults to 1.


## AR1 {#sec:pr-ar1}

### Model

\begin{align}
  \beta_1^{(m)} & \sim \mathcal{N}(0, \tau_m^2) \\
  \beta_j^{(m)} & = \phi_m \beta_{j-1}^{(m)} + \epsilon_j^{(m)}, \quad j = 2, \cdots, J_m \\
  \epsilon_j^{(m)} & \sim \mathcal{N}\left(0,  (1 - \phi^2) \tau_m^2\right), \quad j = 2, \cdots, J_m \\
  \phi_m & = a_{0,m} + (a_{1,m} - a_{0,m}) \phi_m^{\prime} \\
  \phi_m^{\prime} & \sim \text{Beta}(2, 2) \\
  \tau_m & \sim \mathcal{N}^+\left(0, A_{\tau}^{(m)2}\right).
\end{align}
This is adapted from the specification used for AR1 densities in [TMB](http://kaskr.github.io/adcomp/classdensity_1_1AR1__t.html). We require that $-1 < a_{0m} < a_{1m} < 1$.


### Contribution to posterior density

\begin{equation}
  \mathcal{N}(\tau_m | 0, A_{\tau}^{(m)2}) \text{Beta}( \phi_m^{\prime} | 2, 2) \mathcal{N}\left(\beta_1^{(m)} \mid 0, \tau_m^2 \right)  \prod_{j=2}^{J_m} \mathcal{N}\left(\beta_j^{(m)} \mid \phi_m \beta_{j-1}^{(m)}, (1 - \phi_m^2) \tau_m^2 \right) 
\end{equation}


### Prediction

\begin{equation}
  \beta_{J_m + h + 1}^{(m)} \sim \mathcal{N}\left(\phi_m \beta_{J_m + h}^{(m)}, (1 - \phi_m^2) \tau_m^2\right)
\end{equation}



### Code

```
AR1(min = 0.8, max = 0.98, s = 1)
```

- `min` is $a_{0m}$
- `max` is $a_{1m}$
- `s` is $A_{\tau}^{(m)}$. Defaults to 1.

The defaults for `min` and `max` are based on the defaults for function `ets()` in R package **forecast** [@hyndman2008automatic].


## Linear with independent errors {#sec:pr-lin}

### Model

\begin{align}
  \beta_j^{(m)} & = h_j^{(m)} \eta_m + \epsilon_j^{(m)} \\
   h_j^{(m)} & = -\frac{J_m + 1}{J_m - 1} + \frac{2}{J_m - 1} j \\
  \eta_m & \sim \mathcal{N}\left(0, A_{\eta}^{(m)2}\right) \\
  \epsilon_j^{(m)} & \sim \mathcal{N}(0, \tau_m^2) \\
  \tau_m & \sim \mathcal{N}^+\left(0, A_{\tau}^{(m)2}\right)
\end{align}


### Contribution to posterior density

\begin{equation}
  \mathcal{N}(\tau_m | 0, A_{\tau}^{(m)2})   \mathcal{N}(\eta_m | 0, A_{\eta}^{(m)2}) \prod_{j=1}^{J_m} \mathcal{N}\left(\beta_j^{(m)} \mid h_j^{(m)} \eta_m, \tau_m^2 \right) 
\end{equation}


### Prediction

\begin{equation}
  \beta_{J_m + h + 1}^{(m)} \sim \mathcal{N}(\beta_{J_m + h}^{(m)} + \tfrac{2}{J_m - 1}\eta_m, \tau_m^2)
\end{equation}


### Code

```
Lin(sd = NULL, s = 1)
```

- `sd` is $A_{\eta}^{(m)}$. Defaults to 1.
- `s` is $A_{\tau}^{(m)}$. Defaults to 1.


## Linear with AR1 errors {#sec:pr-lin-ar1}

### Model

\begin{align}
  \beta_j^{(m)} & = h_j^{(m)} \eta_m + \epsilon_j^{(m)} \\
   h_j^{(m)} & = -\frac{J_m + 1}{J_m - 1} + \frac{2}{J_m - 1} j \\
  \eta_m & \sim \mathcal{N}\left(0, A_{\eta}^{(m)2}\right) \\
  \epsilon_1^{(m)} & \sim \mathcal{N}(0, \tau_m^2) \\
  \epsilon_j^{(m)} & \sim \mathcal{N}\left(\phi_m \epsilon_{j-1}^{(m)}, (1 - \phi_m^2) \tau_m^2\right), \quad j = 2, \cdots, J_m \\
  \phi_m & = a_{0,m} + (a_{1,m} - a_{0,m}) \phi_m^{\prime} \\
  \phi_m^{\prime} & \sim \text{Beta}(2, 2) \\
  \tau_m & \sim \mathcal{N}^+\left(0, A_{\tau}^{(m)2}\right)
\end{align}
where $-1 < a_{0m} < a_{1m} < 1$.


### Contribution to posterior density

\begin{equation}
  \mathcal{N}(\tau_m | 0, A_{\tau}^{(m)2}) \text{Beta}( \phi_m^{\prime} | 2, 2) \mathcal{N}\left(\epsilon_1^{(m)} \mid 0, \tau_m^2 \right)  \mathcal{N}(\eta_m | 0, A_{\eta}^{(m)2}) \prod_{j=2}^{J_m} \mathcal{N}\left(\epsilon_j^{(m)} \mid \phi_m \epsilon_{j-1}^{(m)}, (1 - \phi_m^2) \tau_m^2  \right) 
\end{equation}


### Prediction

\begin{equation}
  \beta_{J_m+h+1}^{(m)} \sim \mathcal{N}(\hat{\beta}_{J_m+h+1}^{(m)} + \phi_m \hat{\epsilon}_{J_m+h}^{(m)}, (1 - \phi_m^2) \tau_m^2)
\end{equation}
where
\begin{align}
  \hat{\beta}_{J_m+h+1}^{(m)} & = \beta_{J_m+h}^{(m)} + \tfrac{2}{J_m - 1}\eta_m \\
  \hat{\epsilon}_{J_m+h}^{(m)} & = \beta_{J_m+h}^{(m)} - \beta_{J_m+h-1}^{(m)} - \tfrac{2}{J_m - 1}\eta_m
\end{align}


### Code

```
LinAR1(sd = NULL, min = 0.8, max = 0.98, s = 1)
```

- `sd` is $A_{\eta}^{(m)}$. Defaults to 1.
- `min` is $a_{0m}$.
- `max` is $a_{1m}$.
- `s` is $A_{\tau}^{(m)}$. Defaults to 1.


## P-Spline {#sec:pr-spline}

### Model

\begin{equation}
  \pmb{\beta}^{(m)} = \pmb{B}^{(m)} \pmb{\alpha}^{(m)}
\end{equation}
where $\pmb{B}^{(m)}$ is a $J_m \times K_m$ matrix of B-splines, and $\pmb{\alpha}^{(m)}$ has a second-order random walk prior (Section \@ref(sec:pr-rw2)).

$\pmb{B}^{(m)} = (\pmb{b}_1^{(m)}(\pmb{j}), \cdots, \pmb{b}_{K_m}^{(m)}(\pmb{j}))$, with $\pmb{j} = (1, \cdots, J_m)^{\top}$. The B-splines are centered, so that $\pmb{1}^{\top} \pmb{b}_k^{(m)}(\pmb{j}) = 0$, $k = 1, \cdots, K_m$.


### Contribution to posterior density

\begin{equation}
  \mathcal{N}(\tau_m | 0, A_{\tau}^{(m)2}) \mathcal{N}\left( {\textstyle\sum}_{k=1}^{K_m} \alpha_k^{(m)} \mid 0, 1 \right)  \prod_{k=3}^{K_m} \mathcal{N}\left(\alpha_k^{(m)} - 2 \alpha_{k-1}^{(m)} + \alpha_{k-2}^{(m)} \mid 0, \tau_m^2 \right) 
\end{equation}


### Prediction

Main effects with a P-Spline prior cannot be predicted.


### Code

```
Spline(n = NULL, s = 1)
```

- `n` is $K_m$. Defaults to $\max(0.7 J_m, 4)$. *This is the current default for BayesRates. Could we use a smaller multiplier?*
- `s` is the $A_{\tau}^{(m)}$ from the second-order random walk prior. Defaults to 1.


## SVD {#sec:pr-svd}

### Model

\begin{align}
  \pmb{\beta}^{(m)} & = \pmb{F}^{(m)} \pmb{\alpha}^{(m)} + \pmb{g}^{(m)} \\
  \alpha_j^{(m)} & \sim \mathcal{N}(0, 1)
\end{align}
where $\pmb{F}^{(m)}$ is a $J_m \times K_m$ matrix, and $\pmb{g}^{(m)}$ is a vector with $J_m$ elements, both derived from a singular value decomposition (SVD) of an external dataset of age-specific values. The construction of $\pmb{F}^{(m)}$ and $\pmb{g}^{(m)}$ is described in Appendix \@ref(app:svd). The construction involves centering and scaling, which allows us to use a simple prior for $\pmb{\alpha}^{(m)}$.

TODO - Describe age-sex interactions.

### Contribution to posterior density

\begin{equation}
  \prod_{k=1}^{K_m} \mathcal{N}\left(\alpha_k^{(m)} \mid 0, 1 \right) 
\end{equation}

### Prediction

Main effects with a SVD prior cannot be predicted.


### Code

```
SVD(scaled_svd, n = 5, indep = TRUE)
```

## Known {#sec:pr-known}

### Model

Elements of $\pmb{\beta}^{(m)}$ are treated as known with certainty.

### Contribution to posterior density

Known priors make no contribution to the posterior density.

### Prediction

Main effects with a known prior cannot be predicted.


### Code

```
Known(values)
```

- `values` is a vector containing the $\beta_j^{(m)}$.


# Priors for intercept {#sec:pr-int}

The intercept can be given an fixed-normal prior (Section \@ref(sec:pr-fnorm)) or a Known prior (Section \@ref(sec:pr-known)). If a fixed-norm prior is used, the default value for scale $A_{\beta}^{(0)}$ is 10 rather than 1.


# Priors for interactions {#sec:pr-interact}

*Not written yet*

# Seasonal effects {#sec:pr-season}

## Model

Seasonal effects are modelled by a vector $\pmb{\lambda}^{(m)}$ with $J_m$ elements. Seasonal effects are divided into $S_m$ batches, corresponding to $S_m$ "seasons", where $2 \le S_m \le \tfrac{1}{2} J_m$. Each batch of seasonal effects follows a random walk, and there is a soft constrain on the sum across all batches.

\begin{align}
  \lambda_{j}^{(m)} & \sim \mathcal{N}(\lambda_{j - S_m}^{(m)}, \kappa_m^2 ) \\ 
  \kappa_m & \sim \mathcal{N}^+\left(0, A_{\kappa}^{(m)2}\right) \\
  \sum_{j=1}^{J_m} \lambda_j & \sim \mathcal{N}(0, 1)
\end{align}


## Contribution to posterior density

\begin{equation}
  \mathcal{N}(\kappa_m | 0, A_{\kappa}^{(m)2}) \mathcal{N}\left( {\textstyle\sum}_{j=1}^{J_m} \lambda_j^{(m)} \mid 0, 1 \right)  \prod_{j=2}^{J_m} \mathcal{N}\left(\lambda_j^{(m)} - \lambda_{j-1}^{(m)} \mid 0, \kappa_m^2 \right) 
\end{equation}


## Code

```
set_season(n, s = 1)
```

- `n` is $S_m$
- `s` is $A_{\kappa}^{(m)}$


# Covariates {#sec:pr-cov}

## Model

The columns of matrix $\pmb{Z}$ are assumed to be standardised to have mean 0 and standard deviation 1.  $\pmb{Z}$ does not contain a column for an intercept.

We implement two priors for coefficient vector $\pmb{\zeta}$. The first prior is designed for the case where $P$, the number of colums of $\pmb{Z}$, is small, and most $\zeta_p$ are likely to distinguishable from zero. The second prior is designed for the case where $P$ is large, and only a few $\zeta_p$ are likely to be distinguishable from zero.


### Standard prior

\begin{align}
  \zeta_p | \varphi & \ind \mathcal{N}(0, \varphi^2) \\
  \varphi & \sim \mathcal{N}^+(0, 1)
\end{align}


### Shrinkage prior

Regularised horseshoe prior [@piironen2017hyperprior]

\begin{align}
  \zeta_p | \omega_p, \varphi & \ind \mathcal{N}(0, \omega_p^2 \varphi^2) \\
  \omega_p & \sim \text{Cauchy}^+(0, 1) \\
  \varphi & \sim \text{Cauchy}^+(0, A_{\varphi}^2) \\
  A_{\varphi} & = \frac{p_0}{p_0 + P} \frac{\hat{\sigma}}{\sqrt{n}}
\end{align}
where $p_0$ is an initial guess at the number of $\zeta_p$ that are non-zero, and $\hat{\sigma}$ is obtained as follows:

- Poisson. Using maximum likelihood, fit the GLM
\begin{align}
  y_i & \sim \text{Poisson}(w_i \gamma_i) \\
  \log \gamma_i & = \sum_{m=0}^{M} \pmb{X}^{(m)} \pmb{\beta}^{(m)},
\end{align}
and set
\begin{equation}
  \hat{\sigma} = \frac{1}{n}\sum_{i=1}^n \frac{1}{w_i \hat{\gamma}_i}.
\end{equation}
- Binomial. Using maximum likelihood, fit the GLM
\begin{align}
  y_i & \sim \text{binomial}(w_i, \gamma_i) \\
  \text{logit} \gamma_i & = \sum_{m=0}^{M} \pmb{X}^{(m)} \pmb{\beta}^{(m)},
\end{align}
and set
\begin{equation}
  \hat{\sigma} = \frac{1}{n}\sum_{i=1}^n \frac{1}{w_i \hat{\gamma}_i (1 - \hat{\gamma}_i)}.
\end{equation}
- Normal. Using maximum likelihood, fit the linear model
\begin{equation}
  y_i \sim \mathcal{N}\left(\sum_{m=0}^{M} \pmb{X}^{(m)} \pmb{\beta}^{(m)}, w_i^{-1}\xi^2 \right)
\end{equation}
and set $\hat{\sigma} = \hat{\xi}$.


The quantities used for Poisson and binomial likelihoods are derived from normal approximations to GLMs [@piironen2017hyperprior; @gelman2014bayesian, section 16.2].

## Code

```
set_covariates(formula, data = NULL, n_coef = NULL)
```

- `formula` is a one-sided R formula describing the covariates to be used
- `data` A data frame. If a value for `data` is supplied, then `formula` is interpreted in the context of this data frame. If a value for `data` is not supplied, then `formula` is interpreted in the context of the data frame used for the original call to `mod_pois()`, `mod_binom()`, or `mod_norm()`.
- `n_coef` is the effective number of non-zero coefficients. If a value is supplied, the shrinkage prior is used; otherwise the standard prior is used.


Examples:
```
set_covariates(~ mean_income + distance * employment)
set_covariates(~ ., data = cov_data, n_coef = 5)
```


# Priors for dispersion terms

## Model

### Poisson

\begin{equation}
  p(\xi) = \frac{A_{\xi}}{2 \sqrt{\xi}}e^{-A_{\xi} \sqrt{\xi}}
\end{equation}
[@simpson2022priors]

TODO - choice of $A_{\xi}$.


### Binomial

It seems sensible to use the same prior for $\xi$ as we do with the Poisson likelihood, since in both cases, the $\xi^{-1}$ is the number of prior cases. But we need to verify this. I started in Appendix \@ref(app:disp), but got stuck.


### Normal

\begin{equation}
  \xi \sim \text{Exp}(A_{\xi})
\end{equation}

[@simpson2022priors]

## Code

```
set_disp(s = 1)
```

- scale is $A_{\xi}$


# Deriving outputs

Running TMB yields a set of means $\pmb{m}$, and a precision matrix $\pmb{Q}^{-1}$, which together define the approximate joint posterior distribution of

- $\pmb{\beta}^{(m)}$ for terms with independent normal, fixed normal, multivariate normal, random walk, second-order random walk, AR1, Linear, and Linear-AR1 priors,
- $\pmb{\alpha}$ for terms with Spline and SVD priors,
- hyper-parameters for $\pmb{\beta}^{(m)}$ and $\pmb{\alpha}^{(m)}$ typically transformed to another scale, such as a log scale,
- dispersion term $\xi$, and
- seasonal effects $\pmb{\lambda}^{(m)}$, together with associated hyper-parameters $\kappa_m$ (on a log scale).

We use $\tilde{\pmb{\theta}}$ to denote a vector containing all these quantities.

We perform a Cholesky decomposition of $\pmb{Q}^{-1}$, to obtain $\pmb{R}$ such that
\begin{equation}
  \pmb{R}^{\top} \pmb{R} = \pmb{Q}^{-1}
\end{equation}
We store $\pmb{R}$ as part of the model object.

We draw generate values for $\tilde{\pmb{\theta}}$ by generating a vector of standard normal variates $\pmb{z}$, back-solving the equation
\begin{equation}
  \pmb{R} \pmb{v} = \pmb{z}
\end{equation}
and setting
\begin{equation}
  \tilde{\pmb{\theta}} = \pmb{v} + \pmb{m}.
\end{equation}

Next we convert any transformed hyper-parameters back to the original units, and insert values for $\pmb{\beta}^{(m)}$ for terms that have Known priors. We denote the resulting vector $\pmb{\theta}$.

Finally we draw from the distribution of $\pmb{\gamma} \mid \pmb{y}, \pmb{\theta}$ using the methods described in Section \@ref(sec:lik).



# Simulation

# Appendices

## Definitions {#app:defn}


| Quantity | Definition                                          |
|:---------|:----------------------------------------------------|
| $i$      | Index for cell, $i = 1, \cdots, n$.
| $y_i$    | Value for outcome variable.              |
| $w_i$    | Exposure, number of trials, or weight. |
| $\gamma_i$ | Super-population rate, probability, or mean. |
| $\mu_i$ | Cell-specific mean. |
| $\xi$   | Dispersion parameter. |
| $g()$ | Log, logit, or identity function. |
| $m$ | Index for intercept, main effect, or interaction. $m = 0, \cdots, M$. |
| $\beta^{(0)}$ | Intercept. |
| $\pmb{\beta}^{(m)}$ | Main effect or interaction. $m = 1, \cdots, M$.  |
| $\beta_j^{(m)}$ | $j$th element of $\pmb{\beta}^{(m)}$. $j = 1, \cdots, J_m$. |
| $\pmb{X}^{(m)}$ | Matrix mapping $\pmb{\beta}^{(m)}$ to $\pmb{y}$. |
| $\pmb{Z}^{(m)}$ | Matrix of covariates. |
| $\pmb{\zeta}^{(m)}$ | Parameter vector for covariates $\pmb{Z}^{(m)}$. |
| $A_0$ | Scale parameter in prior for intercept $\beta^{(0)}$. |
| $\tau_m$ | Standard deviation parameter for main effect or interaction. |
| $A_{\tau}^{(m)}$ | Scale parameter in prior for $\tau_m$. |
| $\pmb{\alpha}^{(m)}$ | Parameter vector for P-spline and SVD priors. |
| $\alpha_k^{(m)}$ | $k$th element of $\pmb{\alpha}^{(m)}$. $k = 1, \cdots, K_m$. |
| $\pmb{V}^{(m)}$ | Covariance matrix for multivariate normal prior. |
| $h_j^{(m)}$ | Linear covariate |
| $\eta^{(m)}$ | Parameter specific to $m$th main effect or interaction. |
| $A_{\eta}^{(m)}$ | Standard deviation in normal prior for $\eta_m$. |
| $\phi_m$ | Correlation coefficient in AR1 densities. |
| $a_{0m}$, $a_{1m}$ | Minimum and maximum values for $\phi_m$. |
| $\pmb{B}^{(m)}$ | B-spline matrix in P-spline prior. |
| $\pmb{b}_k^{(m)}$ | B-spline. $k = 1, \cdots, K_m$. |
| $\pmb{F}^{(m)}$ | Matrix in SVD prior. |
| $\pmb{g}^{(m)}$ | Offset in SVD prior. |
| $\pmb{\lambda}^{(m)}$ | Seasonal effect. |
| $\kappa_m$ | Standard deviation parameter for seasonal effect. |
| $\varphi$ | Global shrinkage parameter in shrinkage prior. |
| $A_{\varphi}$ | Scale term in prior for $\varphi$. |
| $\omega_p$ | Local shrinkage parameter in shrinkage prior. |
| $p_0$ | Expected number of non-zero coefficients in $\pmb{\zeta}$. |
| $\hat{\sigma}$ | Empirical scale estimate in prior for $\varphi$. |

 
## SVD prior for age  {#app:svd}

Let $\pmb{M}$ be a matrix of age-specific estimates from an international database, transformed to take values in the range $(-\infty, \infty)$. Each column of $\pmb{M}$ represesents one set of age-specific estimates, such as log mortality rates in Japan in 2010, or logit labour participation rates in Germany in 1980.

Let $\pmb{U}$, $\pmb{D}$, $\pmb{V}$ be the matrices from a singular value decomposition of $\pmb{M}$, where we have retained the first $K$ components (with $K$ much less than $A$). Then
\begin{equation}
  \pmb{M} \approx \pmb{U} \pmb{D} \pmb{V}. (\#eq:svd1)
\end{equation}

Let $g_k$ and $h_k$ be the mean and sample standard deviation of the elements of the $k$th row of $\pmb{V}$, with $\pmb{g} = (g_1, \cdots, g_K)^{\top}$ and $\pmb{h} = (h_1, \cdots, h_K)^{\top}$. Then
\begin{equation}
  \tilde{\pmb{V}} = (\text{diag}(\pmb{h}))^{-1} (\pmb{V} - \pmb{g} \pmb{1}^{\top})
\end{equation}
is a standardized version of $\pmb{V}$.

We can rewrite \@ref(eq:svd1) as
\begin{align}
  \pmb{M} & \approx \pmb{U} \pmb{D} (\text{diag}(\pmb{h}) \tilde{\pmb{V}} + \pmb{g} \pmb{1}^{\top}) \\
  & = \pmb{F} \tilde{\pmb{V}} + \pmb{g} \pmb{1}^{\top}, (\#eq:svd2)
\end{align}
where $\pmb{F} = \pmb{U} \pmb{D} \text{diag}(\pmb{h})$ and $\pmb{g} = \pmb{U} \pmb{D} \pmb{g}$.

Let $\tilde{\pmb{v}}_l$ be a randomly-selected column from $\tilde{\pmb{V}}$. From the construction of $\tilde{\pmb{V}}$, and the orthogonality of the rows of $\pmb{V}$, we have $\text{E}[\tilde{\pmb{v}}_l] = \pmb{0}$ and $\text{var}[\tilde{\pmb{v}}_l] = \pmb{I}$. This implies that if $\pmb{z}$ is a vector of standard normal variables, then
\begin{equation}
  \pmb{F} \pmb{z} + \pmb{g}
\end{equation}
looks approximately like a randomly-selected column from the original data matrix $\pmb{M}$.


## Derivation of prior for dispersion term in binomial model {#app:disp}


The Kullback-Liebler divergence $\text{KL}(f_1 || f_0)$ of two beta distributions is

\begin{equation}
  \log \frac{\Gamma(\alpha_1 + \beta_1) \Gamma(\alpha_0) \Gamma(\beta_0)}{\Gamma(\alpha_0 + \beta_0) \Gamma(\alpha_1) \Gamma(\beta_1)}  + (\alpha_1 - \alpha_0) \left(\psi(\alpha_1) - \psi(\alpha_1 + \beta_1)\right)
  + (\beta_1 - \beta_0) \left(\psi(\beta_1) - \psi(\alpha_1 + \beta_1)\right) (\#eq:kl-beta) 
\end{equation}
https://math.stackexchange.com/questions/257821/kullback-liebler-divergence#comment564291_257821

Using the parameterisation $v = 1 / \xi = \alpha + \beta$, $\mu = \alpha / (\alpha + \beta)$, this becomes
\begin{equation}
  \log \frac{\Gamma(v_1) \Gamma(\mu v_0) \Gamma((1 - \mu) v_0)}{\Gamma(v_0) \Gamma(\mu v_1) \Gamma((1 - \mu) v_1)} + (v_1 - v_0) \mu (\psi(\mu v_1) - \psi(v_1)) + (v_1 - v_0) (1 - \mu) (\psi((1-\mu) v_1) - \psi(v_1))
\end{equation}



# References


