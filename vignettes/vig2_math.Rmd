---
title: "Mathematical description of models used in bage"
output:
  bookdown::html_document2:
    base_format: rmarkdown::html_vignette
    fig_caption: yes
    toc: true
    toc_depth: 2
    number_sections: true
pkgdown:
  as_is: true
vignette: >
  %\VignetteIndexEntry{Mathematical description of models used in bage}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
bibliography: references.bib
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

$\newcommand{\ind}{\, \overset{\text{ind}}{\sim} \,}$


```{r setup}
library(bage)
```

# Introduction

Specification document. Aims to give complete mathematical description of models used by bage.

Note: this is describing the models we want to get to soon, not the ones we currently implement.

# Likelihood

## Poisson

Let $y_i$ be a count of events in cell $i = 1, \cdots, n$ and let $w_i$ be the corresponding exposure measure, with the possibility that $w_i \equiv 1$. The likelihood under the Poisson model is then
\begin{align}
  y_i \mid \gamma_i, w_i & \ind \text{Poisson}(\gamma_i w_i) (\#eq:lik-pois-1) \\
  \gamma_i \mid \mu_i, \xi & \ind \text{Gamma}\left(\xi^{-1}, (\mu_i \xi)^{-1}\right),  (\#eq:lik-pois-2)
\end{align}
where the Gamma distribution has a shape-rate parameterisation. Parameter $\xi$ governs dispersion, with
\begin{equation}
  \text{var}(\gamma_i \mid \mu_i, \xi) = \xi \mu_i^2
\end{equation}
and
\begin{equation}
  \text{var}(y_i \mid \mu_i, \xi, w_i) = (1 + \xi \mu_i w_i ) \times \mu_i w_i.
\end{equation}
We allow $\xi$ to equal 0, in which case the model reduces to 
\begin{equation}
  y_i \mid \mu_i, w_i \ind \text{Poisson}(\mu_i w_i).
\end{equation}

For $\xi > 0$, Equations \@ref(eq:lik-pois-1) and \@ref(eq:lik-pois-2) are equivalent to
\begin{equation}
  y_i \sim \text{NegBinom}\left(\xi^{-1}, (1 + \mu_i w_i \xi)^{-1}\right)
\end{equation}
[@norton2018sampling; @simpson2022priors].
This is the format we use internally, for estimation. When values for $\gamma_i$ are needed, we generate them on the fly, using the fact that
\begin{equation}
  \gamma_i \mid y_i, \mu_i, \xi \ind \text{Gamma}\left(y_i + \xi^{-1}, w_i + (\xi \mu_i)^{-1}\right).
\end{equation}


## Binomial

The likelihood under the binomial model is
\begin{align}
  y_i \mid \gamma_i, w_i & \ind \text{Binomial}(w_i, \gamma_i) (\#eq:lik-binom-1) \\
  \gamma_i \mid \mu_i, \xi & \ind \text{Beta}\left(\xi^{-1} \mu_i, \xi^{-1}(1 - \mu_i)\right). (\#eq:lik-binom-2)
\end{align}
Parameter $\xi$ again governs dispersion, with
\begin{equation}
  \text{var}(\gamma_i \mid \mu_i, \xi) =  \frac{\xi}{1 + \xi} \times \mu_i (1 -\mu_i)
\end{equation}
and
\begin{equation}
  \text{var}(y_i \mid w_i, \mu_i, \xi) =  \frac{\xi w_i + 1}{\xi + 1} \times w_i \mu_i (1 - \mu_i).
\end{equation}

We allow $\xi$ to equal 0, in which case the model reduces to 
\begin{equation}
  y_i \mid w_i, \mu_i \ind \text{Binom}(w_i, \mu_i).
\end{equation}
For $\xi > 0$, Equations \@ref(eq:lik-binom-1) and \@ref(eq:lik-binom-1) are equivalent to
\begin{equation}
  y_i | w_i, \mu_i, \xi \ind \text{BetaBinom}\left(w_i, (1-\xi)\xi^{-1}\mu_i, (1-\xi)\xi^{-1}(1 - \mu_i)\right),
\end{equation}
which is what we use internally. Values for $\gamma_i$ can be generated using
\begin{equation}
  \gamma_i \mid y_i, w_i, \mu_i, \xi \sim \text{Beta}\left(y_i + (1-\xi)\xi^{-1}\mu_i, w_i - y_i + (1-\xi)\xi^{-1}(1-\mu_i)\right).
\end{equation}


## Normal

\begin{equation}
  y_i \sim \text{N}(\mu_i, w_i^{-1}\xi^2).
\end{equation}

## Log-Normal

\begin{equation}
  y_i \sim \text{LogNorm}(\mu_i, w_i^{-1}\xi^2).
\end{equation}


# Model for means

Our model for means is
\begin{equation}
  g(\pmb{\mu}) = \sum_{m=0}^{M} \pmb{X}^{(m)} \pmb{\beta}^{(m)} + \pmb{Z} \pmb{\zeta} + \pmb{o} (\#eq:means)
\end{equation}
where

- $g$ is log in Poisson models, logit in binomial models, and identity in normal and log-normal models;
- $\beta^{(0)}$ is an intercept;
- $\pmb{\beta}^{(m)}$, $m=1,\cdots,M$ is a main effect or interaction with $J_m$ elements;
- $\pmb{X}^{(m)}$ is an $n \times J_m$ matrix of 1s and 0s, the $i$th row of which picks out the element of $\pmb{\beta}^{(m)}$ that is used with cell $i$;
- $\pmb{Z}$ is a $n \times P$ matrix of covariates;
- $\pmb{\zeta}$ is a vector with $P$ elements; and
- $\pmb{o}$ is an offset containing known values.

Priors for the $\pmb{\beta}^{(m)}$ are described in sections \@ref(sec:pr-int)--\@ref(sec:pr-interact-high). Covariate matrix $\pmb{Z}$ and the prior for vector $\pmb{\zeta}$ are discussed in Section \@ref(sec:pr-cov).


# Scale term

Many priors use a value $s$ as the default for parameters involving. The value for $s$ depends on the likelihood:
\begin{equation}
  s = \begin{cases} 1 & \text{ with Poisson, binomial, or log-normal likelihoods} \\
  \sqrt{\sum_{i=1}^n (y_i - \bar{y})^2/(n-1)} & \text{ with normal likelihoods.} \end{cases}  (\#eq:defn-s)
\end{equation}



# Prior for intercept {#sec:pr-int}

\begin{equation}
  \beta^{(0)} \sim \text{N}(0, A_0^2)
\end{equation}

By default, $A_0 = 10s$, with $s$ defined in Equation \@ref(eq:defn-s).

# Overall design of priors for main effects {#sec:pr-main-over}

## Model

In all priors for  main effects, we require, for identifiability, that $\text{E}\left(\sum_{j=1}^{J_m} \beta_j^{(m)}\right) = 0$. In the case of random walk priors (sections \@ref(sec:pr-rw) and \@ref(sec:pr-rw2)), the easiest way to implement this requirement is to impose the stronger condition that $\sum_{j=1}^{J_m} \beta_j^{(m)} = 0$.

We design the priors so that, under the defaults, the marginal variance of a randomly-chosen element $\beta_j^{(m)}$ is equal to that of our simplest prior, independent normal prior (Section \@ref(sec:pr-inorm)). With the exception of the SVD prior (Section \@ref(sec:pr-svd)), priors all have the form
\begin{equation}
  \text{<value>} = \text{<optional linear trend>} + \text{<error>}.
\end{equation}
In all priors, after removing any linear trend, the average value for the marginal variance of the $\beta_j^{(m)}$ equals that of the independent normal prior:
\begin{equation}
 \frac{1}{J_m} \sum_{j=1}^{J_m} \text{var}(\beta_j^{(m)}) = \tau_m^2. (\#eq:constraint-var)
\end{equation}
We assign a half-normal prior to $\tau_m$,
\begin{equation}
  \tau_m \sim \text{N}^+(A_m^2).
\end{equation}
The default for $A_m$ is $s$, as defined in Equation \@ref(eq:defn-s).

## Standard format

All priors can be put into the format
\begin{align}
  \pmb{\beta}^{(m)} & = \pmb{\Phi}^{(m)} \pmb{\alpha}^{(m)} \\
  \alpha_k^{(m)} & \sim \text{N}(0, (\nu_k^{(m)})^2), \quad k = 1, \dots, K_m
\end{align}
where $\pmb{\Phi}^{(m)}$ is a $J_m \times K_m$ matrix, and $\pmb{\alpha}^{(m)}$ is a vector with $K_m$ elements. In AR1 priors (Section \@ref(sec:pr-ar1)) and linear priors with AR1 errors (Section \@ref(sec:pr-lin-ar1)), $\pmb{\Phi}^{(m)}$ depends on correlation coefficient $\phi_m$. In all other priors, $\pmb{\Phi}^{(m)}$ is fixed and known.


## Code

`set_prior_effect(<effect> ~ <fun>)`

eg

`set_prior_effect(region ~ N())`

`set_prior_effect(time ~ Lin(scale = 0.2))`



# Priors for main effects {#sec:pr-main-specific}

## Independent normal {#sec:pr-inorm}

### Model

\begin{align}
  \beta_j^{(m)} | \tau_m & \ind \text{N}\left(0, \tau_m^2 \right) \\
  \tau_m & \sim \text{N}^+(A_m^2)
\end{align}

### Standard format

\begin{align}
  \pmb{\Phi}^{(m)} & = \pmb{I}_{J_m} \\
  K_m & = J_m \\
  \nu_k^{(m)} & = \tau_m, \quad k = 1, \cdots, J_m
\end{align}
where $\pmb{I}_{J_m}$ is a $J_m \times J_m$ identity matrix.

### Code

`N(scale = NULL)`

- `scale` is $A_m$. Defaults to $s$, as defined in Equation \@ref(eq:defn-s), if user does not supply a value.

## Multivariate normal {#sec:pr-cnorm}

### Model

\begin{align}
  \pmb{\beta}^{(m)} & \sim \text{N}\left(\pmb{0}, c_m \tau_m^2 \pmb{V}^{(m)} \right) \\
  \tau_m & \sim \text{N}^+(A_m^2)
\end{align}
where $\pmb{V}^{(m)}$ is a $J_m \times J_m$ symmetric positive-definite matrix supplied by the user, and $c_m = J_m  \left(\text{tr}(\pmb{V}^{(m)})\right)^{-1}$.


### Standard format

\begin{align}
  \pmb{\Phi}^{(m)} & =  \pmb{L}^{(m)} \\
  K_m & = J_m \\
  \nu_k^{(m)} & = c_m \tau_m, \quad k = 1, \cdots, K_m
\end{align}
where $\pmb{L}^{(m)}$ is the lower triangular matrix obtained from a Cholesky decomposition of $\pmb{V}^{(m)}$.

### Code

`MVN(V, scale = NULL)`

- `V` is $\pmb{V}^{(m)}$
- `scale` is $A_m$. Defaults to $s$, as defined in Equation \@ref(eq:defn-s), if user does not supply a value.


## First-order random walk {#sec:pr-rw}

### Model

\begin{align}
  \pmb{\beta}^{(m)} & = \pmb{D}_m^{\top} (\pmb{D}_m \pmb{D}_m^{\top})^{-1} \pmb{\epsilon}^{(m)} \\
  \pmb{\epsilon}^{(m)} & \sim \text{N}\left(\pmb{0}, c_m \tau_m^2 \pmb{I}_{J_m - 1}\right) \\ (\#eq:pr-rw-eps)
  \tau_m & \sim \text{N}^+(A_m^2)
\end{align}
where $\pmb{D}_m$ is a $(J_m - 1) \times J_m$ first-order difference matrix of the form
\begin{equation}
  \begin{bmatrix} -1 & 1 & 0 & 0 \\
                  0 & -1 & 1 & 0 \\
		  0 & 0 & -1 & 1 \end{bmatrix}
\end{equation}

Note that $\pmb{D}_m \pmb{\beta}^{(m)} =  \pmb{D}_m \pmb{D}_m^{\top} (\pmb{D}_m \pmb{D}_m^{\top})^{-1} \pmb{\epsilon}^{(m)} = \pmb{\epsilon}^{(m)}$ and that $\pmb{1}^{\top} \pmb{\beta}^{(m)} =  \pmb{1}^{\top} \pmb{D}_m^{\top} (\pmb{D}_m \pmb{D}_m^{\top})^{-1} \pmb{\epsilon}^{(m)} = 0$.

The $c_m$ in Equation \@ref(eq:pr-rw-eps) equals $J_m \left(\text{tr}(\pmb{D}_m^{\top} (\pmb{D}_m \pmb{D}_m^{\top})^{-1} ((\pmb{D}_m \pmb{D}_m^{\top})^{-1})^{\top} \pmb{D}_m)\right)^{-1}$.

### Standard format

\begin{align}
  \pmb{\Phi}^{(m)} & = \pmb{D}_m^{\top} (\pmb{D}_m \pmb{D}_m^{\top})^{-1} \\
  K_m & = J_m - 1 \\
  \nu_k^{(m)} & = c_m \tau_m, \quad k = 1, \cdots, K_m
\end{align}

### Code

`RW(scale = NULL)`

- `scale` is $A_m$. Defaults to $s$, as defined in Equation \@ref(eq:defn-s), if user does not supply a value.


## Second-order random walk {#sec:pr-rw2}

### Model

\begin{align}
  \pmb{\beta}^{(m)} & =  \pmb{h}^{(m)} \eta_m + \pmb{D}_{2m}^{\top} (\pmb{D}_{2m} \pmb{D}_{2m}^{\top})^{-1} \pmb{\epsilon}^{(m)} \\
  \eta_m & \sim \text{N}(0, H_m^2) \\
  \pmb{\epsilon}^{(m)} & \sim \text{N}(\pmb{0}, c_m \tau_m^2 \pmb{I}_{J_m - 2}) (\#eq:pr-rw2-eps) \\
  \tau_m & \sim \text{N}^+(A_m^2)
\end{align}
where $\pmb{h}^{(m)}$ is a vector, the $j$th element of which is
\begin{equation}
  l_j^{(m)}  = -\frac{J_m + 1}{J_m - 1} + \frac{2}{J_m - 1} j,
\end{equation}
and $\pmb{D}_{2m}$ is a second-order difference matrix formed by multiplying two first-order difference matrices.
The $\pmb{h}^{(m)} \eta_m$ term is needed because the columns of $\pmb{D}_{2m}^{\top} (\pmb{D}_{2m} \pmb{D}_{2m}^{\top})^{-1}$ do not span the space defined by $\sum_{j=1}^{J_m}{\beta_j^{(m)}} = 0$. See, for instance, @currie2002flexible [336].

We have $\pmb{D}_{2m} \pmb{\beta}^{(m)} =  \pmb{D}_{2m}^{\top} \pmb{h}^{(m)} \eta_m + \pmb{D}_{2m} \pmb{D}_{2m}^{\top} (\pmb{D}_{2m} \pmb{D}_{2m}^{\top})^{-1} \pmb{\epsilon}^{(m)} = \pmb{\epsilon}^{(m)}$ and $\pmb{1}^{\top} \pmb{\beta}^{(m)} = 0$.

The $c_m$ in Equation \@ref(eq:pr-rw2-eps) equals $J_m \left(\text{tr}(\pmb{D}_{2m}^{\top} (\pmb{D}_{2m} \pmb{D}_{2m}^{\top})^{-1} ((\pmb{D}_{2m} \pmb{D}_{2m}^{\top})^{-1})^{\top} \pmb{D}_{2m})\right)^{-1}$.


### Standard format

\begin{align}
  \pmb{\Phi}^{(m)} & = \begin{bmatrix} \pmb{h}^{(m)} & \pmb{D}_{2m}^{\top} (\pmb{D}_{2m} \pmb{D}_{2m}^{\top})^{-1} \end{bmatrix} \\
  K_m & = J_m - 1 \\
  \nu_1^{(m)} & = H_m^2 \\
  \nu_k^{(m)} & = c_m \tau_m, \quad k = 2, \cdots, K_m
\end{align}



### Code

`RW2(sd = NULL, scale = NULL)`

- `sd` is $H_m$. Defaults to $s$, as defined in Equation \@ref(eq:defn-s), if user does not supply a value.
- `scale` is $A_m$. Defaults to $s$ if user does not supply a value.


## AR1 {#sec:pr-ar1}

### Model

\begin{align}
  \beta_1^{(m)} & \sim \text{N}(0, \tau_m^2) \\
  \beta_j^{(m)} & \sim \phi_m \beta_{j-1}^{(m)} + \epsilon_j^{(m)}, \quad j = 2, \cdots, J_m \\
  \epsilon_j^{(m)} & \sim \text{N}\left(0,  (1 - \phi^2) \tau_m^2\right), \quad j = 2, \cdots, J_m \\
  \phi_m & = a_{0,m} + (a_{1,m} - a_{0,m}) \phi_m^{\prime} \\
  \phi_m^{\prime} & \sim \text{Beta}(2, 2) \\
  \tau_m & \sim \text{N}^+(A_m^2).
\end{align}
This is adapted from the specification used for AR1 densities in [TMB](http://kaskr.github.io/adcomp/classdensity_1_1AR1__t.html). We require that $-1 < a_{0m} < a_{1m} < 1$.

### Standard format

\begin{align}
  \pmb{\Phi}^{(m)} & = \pmb{L}^{(m)} \\
  K_m & = J_m \\
  \nu_k^{(m)} & =  \tau_m, \quad k = 1, \cdots, K_m
\end{align}
where $\pmb{L}^{(m)}$ is the lower triangular matrix from the Cholesky decomposition of

\begin{equation}
  \begin{bmatrix} 1 & \phi & \phi^2 & \phi^3 & \phi^4\\
                  \phi & 1 & \phi & \phi^2 & \phi^3\\
                  \phi^2 & \phi & 1 & \phi & \phi^2 \\
                  \phi^3 & \phi^2 & \phi & 1 & \phi \\
		  \phi^4 & \phi^3 & \phi^2 & \phi & 1 \end{bmatrix},
\end{equation}

[@rue2005gaussian, 2] and has the form
\begin{equation}
  \begin{bmatrix} 1 & 0 & 0 & 0 & 0 \\
                  \phi & \sqrt(1 - \phi^2) & 0 & 0 & 0 \\
                  \phi^2 & \phi \sqrt(1 - \phi^2) & \sqrt(1 - \phi^2) & 0 & 0 \\
                  \phi^3 & \phi^2 \sqrt(1 - \phi^2) & \phi \sqrt(1 - \phi^2) & \sqrt(1 - \phi^2) & \phi \\
		  \phi^4 & \phi^3 \sqrt(1 - \phi^2) & \phi^2 \sqrt(1 - \phi^2) & \phi \sqrt(1 - \phi^2) & \sqrt(1 - \phi^2) \end{bmatrix}.
\end{equation}



### Code

`AR1(min = 0.8, max = 0.98, scale = NULL)`

- `min` is $a_{0m}$
- `max` is $a_{1m}$
- `scale` is $A_m$. Defaults to $s$, as defined in Equation \@ref(eq:defn-s), if user does not supply a value.

The defaults for `min` and `max` are based on the defaults for function `ets()` in R package **forecast** [@hyndman2008automatic].


## Linear with independent errors {#sec:pr-lin}

### Model

\begin{align}
  \pmb{\beta}^{(m)} & = \pmb{h}^{(m)} \eta_m + \pmb{\epsilon}^{(m)} \\
  \epsilon_j^{(m)} & \sim \text{N}(0, \tau_m^2) \\
  \eta_m & \sim \text{N}(0, H_m^2) \\
  \tau_m & \sim \text{N}^+(A_m^2)
\end{align}
where $\pmb{h}^{(m)}$ is defined as in Section \@ref(sec:pr-rw2).

### Standard format

\begin{align}
  \pmb{\Phi}^{(m)} & = \begin{bmatrix} \pmb{h}^{(m)} & \pmb{I}_{J_m} \end{bmatrix} \\
  K_m & = J_m + 1\\
  \nu_1^{(m)} & =  H_m^2 \\
  \nu_k^{(m)} & =  \tau_m, \quad k = 2, \cdots, K_m
\end{align}


### Code

`Lin(sd = NULL, scale = NULL)`

- `sd` is $H_m$. Defaults to $s$, as defined in Equation \@ref(eq:defn-s), if user does not supply a value.
- `scale` is $A_m$. Defaults to $s$ if user does not supply a value.


## Linear with AR1 errors {#sec:pr-lin-ar1}

### Model

\begin{align}
  \pmb{\beta}^{(m)} & = \pmb{h}^{(m)} \eta_m + \pmb{\epsilon}^{(m)} \\
  \epsilon_1^{(m)} & \sim \text{N}(0, \tau_m^2) \\
  \epsilon_j^{(m)} & \sim \phi_m \epsilon_{j-1}^{(m)} + \epsilon_j^{(m)}, \quad j = 2, \cdots, J_m \\
  \phi_m & = a_{0,m} + (a_{1,m} - a_{0,m}) \phi_m^{\prime} \\
  \phi_m^{\prime} & \sim \text{Beta}(2, 2) \\
  \tau_m & \sim \text{N}^+(A_m^2)
\end{align}
where $-1 < a_{0m} < a_{1m} < 1$.


### Standard format

\begin{align}
  \pmb{\Phi}^{(m)} & = \begin{bmatrix} \pmb{h}^{(m)} & \pmb{L}^{(m)} \end{bmatrix} \\
  K_m & = J_m + 1 \\
  \nu_1^{(m)} & =  H_m^2 \\
  \nu_k^{(m)} & =  \tau_m, \quad k = 2, \cdots, K_m
\end{align}
where $\pmb{h}^{(m)}$ is defined as in Section \@ref(sec:pr-rw2), and $\pmb{L}^{(m)}$ is defined as in Section \@ref(sec:pr-ar1).


### Code

`LinAR1(sd = NULL, min = 0.8, max = 0.98, scale = NULL)`

- `sd` is $H_m$. Defaults to $s$, as defined in Equation \@ref(eq:defn-s), if user does not supply a value.
- `min` is $a_{0m}$.
- `max` is $a_{1m}$.
- `scale` is $A_m$. Defaults to $s$ if user does not supply a value.


## P-Spline {#sec:pr-spline}

### Model

\begin{equation}
  \pmb{\beta}^{(m)} = \pmb{B}^{(m)} \pmb{\zeta}^{(m)}
\end{equation}
where $\pmb{B}^{(m)}$ is a $J_m \times G_m$ matrix of B-splines, and $\pmb{\zeta}^{(m)}$ has a second-order random walk prior (Section \@ref(sec:pr-rw2)).

$\pmb{B}^{(m)} = (\pmb{b}_1^{(m)}(\pmb{j}), \cdots, \pmb{b}_{G_m}^{(m)}(\pmb{j}))$, with $\pmb{j} = (1, \cdots, J_m)^{\top}$. The B-splines are centered, so that $\pmb{1}^{\top} \pmb{b}_g^{(m)}(\pmb{j}) = 0$, $g = 1, \cdots, G_m$.

### Standard format

\begin{align}
  \pmb{\Phi}^{(m)} & = \begin{bmatrix} \pmb{B}^{(m)} \pmb{h}^{(m)} & \pmb{B}^{(m)} \pmb{D}_{2m}^{\top} (\pmb{D}_{2m} \pmb{D}_{2m}^{\top})^{-1} \end{bmatrix} \\
  K_m & = G_m - 1 \\
  \nu_1 & = H_m^2 \\
  \nu_k &  = c_m \tau_m, \quad k = 2, \cdots, K_m,
\end{align}
where $\pmb{h}^{(m)}$ is a vector with $G_m$ elements, the $g$th element of which is
\begin{equation}
  l_g^{(m)}  = -\frac{G_m + 1}{G_m - 1} + \frac{2}{G_m - 1} g,
\end{equation}
and $c_m$ is defined as in Section \@ref(sec:pr-rw2).

Matrix $\pmb{\Phi}^{(m)}$ is derived by noting that, when $\pmb{\zeta}^{(m)}$ has a second-order random walk prior,
\begin{equation}
  \pmb{B}^{(m)} \pmb{\zeta}^{(m)} = \pmb{B}^{(m)} \begin{bmatrix} \pmb{h}^{(m)} & \pmb{D}_{2m}^{\top} (\pmb{D}_{2m} \pmb{D}_{2m}^{\top})^{-1} \end{bmatrix}
  \begin{bmatrix} \eta_m \\ \pmb{\epsilon}^{(m)} \end{bmatrix}.
\end{equation}


### Code

`Spline(df, scale = NULL)`

- `df` is $K_m$. Defaults to $\max(0.7 J_m, 4)$. *This is the current default for BayesRates. Could we use a smaller multiplier?*
- `scale` is the $A_m$ from the second-order random walk prior. Defaults to $s$, as defined in Equation \@ref(eq:defn-s), if user does not supply a value.

## SVD {#sec:pr-svd}

### Model

\begin{align}
  \pmb{\beta}^{(m)} & = \pmb{F}^{(m)} \pmb{\zeta}^{(m)} + \pmb{q}^{(m)} \\
  \zeta_j^{(m)} & \sim \text{N}(0, 1)
\end{align}
where $\pmb{F}^{(m)}$ is a $J_{(m)} \times K_{(m)}$ matrix, and $\pmb{q}^{(m)}$ is a vector with $J_{(m)}$ elements, both derived from a singular value decomposition (SVD) of an external dataset of age-specific values. The construction of $\pmb{F}^{(m)}$ and $\pmb{q}^{(m)}$ is described in Appendix \@ref(app:svd). The construction involves centering and scaling, which allows us to use a simple prior for $\pmb{\zeta}$.


### Standard format

Internally, we include $\pmb{q}^{(m)}$ in the offset term $\pmb{o}$ in Equation \@ref(eq:means). The SVD prior without $\pmb{q}^{(m)}$ has format

\begin{align}
  \pmb{\Phi}^{(m)} & = \pmb{F}^{(m)} \\
  K_m & = G_m \\
  \nu_k &  = 1, \quad k = 1, \cdots, K_m
\end{align}

### Code

`SVD(transform, translate)`

`SVD_HMD()`

`SVD_HFD()`

- `transform` is $\pmb{F}^{\text{age}}$
- `translate` is $\pmb{q}^{(m)}$
- functions such as `SVD_HMD()` and `SVD_HFD()` (for Human Mortality Database and Human Fertility Database) provide pre-packaged values.

### Combined dimensions

Designed for use with age, but can be used with other dimensions.

Can also combine dimensions by taking Cartesian product of categories, eg age-sex.


# Priors for second-order interactions {#sec:pr-interact-2}

- $\pmb{\beta}^{(m)}$ is made up of two dimensions, dimension 1 and dimension 2
- Redefine $\pmb{\beta}^{(m)}$ to be a $J_1^{(m)} \times J_2^{(m)}$ matrix, where $J_1^{(m)}$ is the number of elements in dimension 1, and $J_2^{(m)}$ is the number of elements of the second.
- Choose a prior for dimension 1. Let $\Phi_1^{(m)}$ be the matrix from the standard format for this prior. $\Phi_1^{(m)}$ has $J_1^{(m)}$ rows and $K_1^{(m)}$ columns.
- Let
\begin{equation}
  \pmb{\beta}^{(m)} = \pmb{\Phi}_1^{(m)} \pmb{\varsigma}^{(m)} (\#eq:interact-1)
\end{equation}
where $\pmb{\varsigma}^{(m)}$ is a $K_1^{(m)} \times J_2^{(m)}$ parameter matrix. The $j$th column of $\pmb{\varsigma}^{(m)}$ contains the coefficients that are used to construct the $j$th column of $\pmb{\beta}^{(m)}$ from the columns of $\pmb{\Phi}_1^{(m)}$.
- Choose a prior for dimension 2. Let $\Phi_2^{(m)}$ be the matrix from the standard format for this prior. $\Phi_2^{(m)}$ has $J_2^{(m)}$ rows and $K_2^{(m)}$ columns.
- Let
\begin{equation}
  \pmb{\varsigma}^{(m)} = \pmb{\chi}^{(m)} \pmb{\Phi}_2^{(m)\top} (\#eq:interact-2)
\end{equation}
where $\pmb{\chi}^{(m)}$ is a $K_1^{(m)} \times K_2^{(m)}$ parameter matrix. The $k$th row of $\pmb{\chi}^{(m)}$ contains the coefficients that are used to construct the $k$th row of $\pmb{\varsigma}^{(m)}$ from the columns of $\pmb{\Phi}_2^{(m)}$ (i.e. the rows of $\pmb{\Phi}_2^{(m)\top}$). Each row of $\pmb{\chi}^{(m)}$ has the same prior distribution,
\begin{equation}
  \chi_{k_1,k_2}^{(m)} \sim \text{N}(0, (\nu_{k_2}^{(m)})^2)
\end{equation}
where $\nu_{k_2}^{(m)}$ is obtained from the standard format for the prior for the second dimension.

Equations \@ref(eq:interact-1) and \@ref(eq:interact-2) can be combined to give
\begin{equation}
  \pmb{\beta}^{(m)} = \pmb{\Phi}_1^{(m)} \pmb{\chi}^{(m)} \pmb{\Phi}_2^{(m)\top}.
\end{equation}



# Priors for higher-order interactions {#sec:pr-interact-high}

Apply second-order interactions within combinations of other dimensions



# Covariates {#sec:pr-cov}


bla bla bla


## Generic covariates

Regularised horseshoe prior [@piironen2017hyperprior]

\begin{align}
  \zeta_p | \kappa_p, \varphi & \ind \text{N}(0, \kappa_p^2 \varphi^2) \\
  \kappa_p \sim \text{C}^+(1) \\
\end{align}

## Seasonal effects {#sec:season}

### Model

A main effect for time can be extended through the addition of a seasonal effect. Seasonal effects are modelled by a vector $\pmb{\Lambda}$ with $S \ge 2$ elements. The seasonal effect in period $j$ is $\Lambda_{s_j}$ where $s_j = (j \bmod S) + 1$.

We model $\pmb{\Lambda}$ as a draw from a multivariate normal distribution, constrained to sum to zero,
\begin{align}
  \pmb{\Lambda} & = \pmb{G}_m \pmb{\varepsilon}^{(m)} \\
  \pmb{\varepsilon}^{(m)} & \sim \text{N}\left(\pmb{0}, c_m \tau_m^2 \pmb{I}_{J_m-1}\right) \\ 
  \tau_m & \sim \text{N}^+(A_m^2)
\end{align}
where
\begin{equation}
  \pmb{G}_m = \frac{1}{\sqrt{J_m}} 
  \begin{bmatrix} \pmb{1}^{\top} \\
                  \pmb{I}_{J_m - 1} - \frac{1}{J_m - 1} \pmb{1} \pmb{1}^{\top} \end{bmatrix},
\end{equation}
and $c_m = J_m / (J_m - 1)$.

### Code

`set_season(<term> ~ Season(m, scale = NULL))`

- `m` is the number of seasons, i.e. the number of elements of $\Lambda$
- `scale` is $A_m$. Defaults to $s$, as defined in Equation \@ref(eq:defn-s), if user does not supply a value.




# Priors for dispersion terms

## Poisson

\begin{equation}
  p(\xi) = \frac{A}{2 \sqrt{\xi}}e^{-A \sqrt{\xi}}
\end{equation}
[@simpson2022priors] for some value of $A$. TODO - choice of $A$.

## Binomial

\begin{equation}
  p(\xi) = \frac{A}{2 \sqrt{\xi}}e^{-A \sqrt{\xi}}
\end{equation}
for some value of $A$. TODO - choice of $A$.


## Normal

\begin{equation}
  \xi \sim \text{N}^+(0, A_m^2)
\end{equation}

## Log-Normal

\begin{equation}
  \xi \sim \text{N}^+(0, A_m^2)
\end{equation}




# Appendices

## Definitions {#app:defn}


| Quantity | Definition                                          |
|:---------|:----------------------------------------------------|
| $i$      | Index for cell, $i = 1, \cdots, n$.
| $y_i$    | Value for outcome variable.              |
| $w_i$    | Exposure, number of trials, or weight. |
| $\gamma_i$ | Super-population rate, probability, or mean. |
| $\mu_i$ | Cell-specific mean. |
| $\xi$   | Dispersion parameter. |
| $s$ | Generic scale term. Equal to 1 or to $\text{sd}(y_i)$. |
| $g()$ | Log, logit, or identity function. |
| $m$ | Index for intercept, main effect, or interaction. $m = 0, \cdots, M$. |
| $\beta^{(0)}$ | Intercept. |
| $\pmb{\beta}^{(m)}$ | Main effect or interaction. $m = 1, \cdots, M$.  |
| $\beta_j^{(m)}$ | $j$th element of $\pmb{\beta}^{(m)}$. $j = 1, \cdots, J_m$. |
| $\pmb{X}^{(m)}$ | Matrix mapping $\pmb{\beta}^{(m)}$ to $\pmb{y}$. |
| $\pmb{Z}^{(m)}$ | Matrix of covariates. |
| $\pmb{\zeta}^{(m)}$ | Parameter vector for covariates $\pmb{Z}^{(m)}$. |
| $\pmb{o}$ | Offset |
| $\tau_m^2$ | Variance parameter for main effect or interaction. |
| $A_m$ | Scale parameter in half-normal prior for $\tau_m$. |
| $\pmb{\alpha}^{(m)}$ | Free parameters $\alpha^{(m)}$  |
| $\alpha_k^{(m)}$ | $k$th element of $\pmb{\alpha}^{(m)}$. $k = 1, \cdots, K_m$. |
| $\pmb{\Phi}^{(m)}$ | Matrix mapping $\pmb{\alpha}^{(m)}$ to $\pmb{\beta}^{(m)}$. |
| $\nu_k^{(m)}$ | Standard deviation of $\alpha_k^{(m)}$ in prior for main effect. |
| $\pmb{I}_n$ | $n \times n$ identity matrix |
| $\pmb{1}$ | Column vectors of 1s. |
| $\pmb{0}$ | Column vectors of 0s. |
| $c_m$ | Scaling factor applied to variance terms in priors. |
| $\pmb{h}^{(m)}$ | Vector for linear covariate |
| $\eta^{(m)}$ | Parameter specific to $m$th main effect or interaction. |
| $H_m$ | Standard deviation in normal prior for $\eta_m$. |
| $\phi_m$ | Correlation coefficient in AR1 densities. |
| $a_{0m}$, $a_{1m}$ | Minimum and maximum values for $\phi_m$. |
| $\pmb{B}^{(m)}$ | B-spline matrix in P-spline prior. |
| $\pmb{b}_g^{(m)}$ | B-spline. $g = 1, \cdots, G_m$. |
| $\pmb{F}^{(m)}$ | Matrix in SVD prior. |
| $\pmb{q}^{(m)}$ | Offset in SVD prior. |
| $G_m$ | Number of columns of $\pmb{B}^{(m)}$ or $\pmb{F}^{(m)}$. |

 



## SVD prior for age  {#app:svd}

Let $\pmb{M}$ be a matrix of age-specific estimates from an international database, transformed to take values in the range $(-\infty, \infty)$. Each column of $\pmb{M}$ represesents one set of age-specific estimates, such as log mortality rates in Japan in 2010, or logit labour participation rates in Germany in 1980.

Let $\pmb{U}$, $\pmb{D}$, $\pmb{V}$ be the matrices from a singular value decomposition of $\pmb{M}$, where we have retained the first $K$ components (with $K$ much less than $A$). Then
\begin{equation}
  \pmb{M} \approx \pmb{U} \pmb{D} \pmb{V}. (\#eq:svd1)
\end{equation}

Let $g_k$ and $h_k$ be the mean and sample standard deviation of the elements of the $k$th row of $\pmb{V}$, with $\pmb{g} = (g_1, \cdots, g_K)^{\top}$ and $\pmb{h} = (h_1, \cdots, h_K)^{\top}$. Then
\begin{equation}
  \tilde{\pmb{V}} = (\text{diag}(\pmb{h}))^{-1} (\pmb{V} - \pmb{g} \pmb{1}^{\top})
\end{equation}
is a standardized version of $\pmb{V}$.

We can rewrite \@ref(eq:svd1) as
\begin{align}
  \pmb{M} & \approx \pmb{U} \pmb{D} (\text{diag}(\pmb{h}) \tilde{\pmb{V}} + \pmb{g} \pmb{1}^{\top}) \\
  & = \pmb{F} \tilde{\pmb{V}} + \pmb{q} \pmb{1}^{\top}, (\#eq:svd2)
\end{align}
where $\pmb{F} = \pmb{U} \pmb{D} \text{diag}(\pmb{h})$ and $\pmb{q} = \pmb{U} \pmb{D} \pmb{g}$.

Let $\tilde{\pmb{v}}_l$ be a randomly-selected column from $\tilde{\pmb{V}}$. From the construction of $\tilde{\pmb{V}}$, and the orthogonality of the rows of $\pmb{V}$, we have $\text{E}[\tilde{\pmb{v}}_l] = \pmb{0}$ and $\text{var}[\tilde{\pmb{v}}_l] = \pmb{I}$. This implies that if $\pmb{z}$ is a vector of standard normal variables, then
\begin{equation}
  \pmb{F} \pmb{z} + \pmb{q}
\end{equation}
looks approximately like a randomly-selected column from the original data matrix $\pmb{M}$.


## Derivation of prior for dispersion term in binomial model {#app:disp}


The Kullback-Liebler divergence $KL(f_1 || f_0)$ of two beta distributions is

\begin{equation}
  \log \frac{\Gamma(\alpha_1 + \beta_1) \Gamma(\alpha_0) \Gamma(\beta_0)}{\Gamma(\alpha_0 + \beta_0) \Gamma(\alpha_1) \Gamma(\beta_1)}  + (\alpha_1 - \alpha_0) \left(\psi(\alpha_1) - \psi(\alpha_1 + \beta_1)\right)
  + (\beta_1 - \beta_0) \left(\psi(\beta_1) - \psi(\alpha_1 + \beta_1)\right)
\end{equation}
https://math.stackexchange.com/questions/257821/kullback-liebler-divergence#comment564291_257821

Using the parameterisation $v = \alpha + \beta$, $\mu = \alpha / (\alpha + \beta)$, this becomes
\begin{equation}
  \log \frac{\Gamma(v_1) \Gamma(\mu v_0) \Gamma((1 - \mu) v_0)}{\Gamma(v_0) \Gamma(\mu v_1) \Gamma((1 - \mu) v_1)} + (v_1 - v_0) \mu (\psi(\mu v_1) - \psi(v_1)) + (v_1 - v_0) (1 - \mu) (\psi((1-\mu) v_1) - \psi(v_1))
\end{equation}

If we substitute the approximation (valid for large $x$) $\log \Gamma(x) \approx (x - \frac{1}{2})\log x - x + \frac{1}{2} \log(2\pi)$
[https://en.wikipedia.org/wiki/Gamma_function] into the first term we obtain

\begin{align}
  & \left(v_1 - \frac{1}{2}\right) \log v_1 - v_1 \\
  & + \left(\mu v_0 - \frac{1}{2}\right) \left(\log \mu + \log v_0 \right) - \mu v_0 \\
  & + \left((1-\mu) v_0 - \frac{1}{2}\right) \left( \log (1-\mu) + \log v_0 \right) - (1-\mu) v_0 \\
  & - \left(v_0 - \frac{1}{2}\right) \log v_0 + v_0 \\
  & - \left(\mu v_1 - \frac{1}{2}\right) \left(\log \mu + \log v_1 \right) + \mu v_1 \\
  & - \left((1-\mu) v_1 - \frac{1}{2}\right) \left( \log (1-\mu) + \log v_1 \right) + (1-\mu) v_1 \\
=  & \left(v_1 - \frac{1}{2} - \mu v_1 + \frac{1}{2} - (1-\mu) v_1 + \frac{1}{2} \right) \log v_1 \\
  & + \left(\mu v_0 - \frac{1}{2} + (1-\mu) v_0 - \frac{1}{2} - v_0 + \frac{1}{2} \right) \log v_0 \\
  & + \left(\mu v_0 - \frac{1}{2} - \mu v_1 + \frac{1}{2} \right) \log \mu \\
  & + \left((1 - \mu) v_0 - \frac{1}{2} - (1 - \mu) v_1 + \frac{1}{2} \right) \log (1 - \mu) \\
  & + \left(-1 + \mu + (1 - \mu) \right) v_1 \\
  & + \left(-\mu - (1 - \mu) + 1 \right) v_0 \\
= & \frac{1}{2} \log v_1 - \frac{1}{2} \log v_0 - (v_1 - v_0) \mu \log \mu - (v_1 - v_0) (1 - \mu) \log (1-\mu) \\
= & \frac{1}{2} \log \frac{v_1}{v_0} - (v_1 - v_0) \left(\mu \log \mu + (1 - \mu) \log (1-\mu) \right)
\end{align}


\begin{equation}
 (v_1 - v_0) (\mu \psi(\mu v_1) + (1 - \mu) \psi((1 - \mu) v_1) - \psi(v_1))
\end{equation}

The baseline (binomial) model is obtained by letting $v_0$ go to infinity. When this happens, the first term above goes to zero. The second term goes to infinity, but we are hoping that the $v_0$ will drop out later.

Using the approximation $\psi(x) \approx \log x - \frac{1}{2}x$, the second term becomes
\begin{equation}
  (v_1 - v_0) \left(\mu \log \mu + (1 - \mu) \log (1 - \mu) + \mu (1 - \mu) v_1 \right)
\end{equation}



# References


