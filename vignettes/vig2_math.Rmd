---
title: "Mathematical description of models used in bage"
output:
  bookdown::html_document2:
    base_format: rmarkdown::html_vignette
    fig_caption: yes
    toc: true
    toc_depth: 2
    number_sections: true
pkgdown:
  as_is: true
vignette: >
  %\VignetteIndexEntry{Mathematical description of models used in bage}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
bibliography: references.bib
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```


```{r setup}
library(bage)
```

# Introduction

Specification document - a mathematical description of models used by bage.

Note: this is describing the models we want to get to soon, not the ones we currently implement.

# Likelihood {#sec:lik}

## Input data

- outcome variable: events, numbers of people, or some sort of measure on a continuous variable such as income or expenditure
- exposure/size/weights
- disagg by one or more variables. Almost always includes age, sex/gender, and time. May include other variables eg region, ethnicity, education.
- not all combinations of variables present; may be some missing values


## Models

### Poisson {#sec:pois}

Let $y_i$ be a count of events in cell $i = 1, \cdots, n$ and let $w_i$ be the corresponding exposure measure, with the possibility that $w_i \equiv 1$. The likelihood under the Poisson model is then
\begin{align}
  y_i & \sim \text{Poisson}(\gamma_i w_i) (\#eq:lik-pois-1) \\
  \gamma_i & \sim \text{Gamma}\left(\xi^{-1}, (\mu_i \xi)^{-1}\right),  (\#eq:lik-pois-2)
\end{align}
using the shape-rates parameterisation of the Gamma distribution. Parameter $\xi$ governs dispersion, with
\begin{equation}
  \text{var}(\gamma_i \mid \mu_i, \xi) = \xi \mu_i^2
\end{equation}
and
\begin{equation}
  \text{var}(y_i \mid \mu_i, \xi, w_i) = (1 + \xi \mu_i w_i ) \times \mu_i w_i.
\end{equation}
We allow $\xi$ to equal 0, in which case the model reduces to 
\begin{equation}
  y_i \sim \text{Poisson}(\mu_i w_i).
\end{equation}

For $\xi > 0$, Equations \@ref(eq:lik-pois-1) and \@ref(eq:lik-pois-2) are equivalent to
\begin{equation}
  y_i \sim \text{NegBinom}\left(\xi^{-1}, (1 + \mu_i w_i \xi)^{-1}\right)
\end{equation}
[@norton2018sampling; @simpson2022priors].
This is the format we use internally for estimation. When values for $\gamma_i$ are needed, we generate them on the fly, using the fact that
\begin{equation}
  \gamma_i \sim \text{Gamma}\left(y_i + \xi^{-1}, w_i + (\xi \mu_i)^{-1}\right).
\end{equation}


### Binomial {#sec:binom}

The likelihood under the binomial model is
\begin{align}
  y_i  & \sim \text{Binomial}(w_i, \gamma_i) (\#eq:lik-binom-1) \\
  \gamma_i & \sim \text{Beta}\left(\xi^{-1} \mu_i, \xi^{-1}(1 - \mu_i)\right). (\#eq:lik-binom-2)
\end{align}
Parameter $\xi$ again governs dispersion, with
\begin{equation}
  \text{var}(\gamma_i \mid \mu_i, \xi) =  \frac{\xi}{1 + \xi} \times \mu_i (1 -\mu_i)
\end{equation}
and
\begin{equation}
  \text{var}(y_i \mid w_i, \mu_i, \xi) =  \frac{\xi w_i + 1}{\xi + 1} \times w_i \mu_i (1 - \mu_i).
\end{equation}

We allow $\xi$ to equal 0, in which case the model reduces to 
\begin{equation}
  y_i  \text{Binom}(w_i, \mu_i).
\end{equation}
Equations \@ref(eq:lik-binom-1) and \@ref(eq:lik-binom-2) are then equivalent to
\begin{equation}
  y_i  \sim \text{BetaBinom}\left(w_i, \xi^{-1} \mu_i, \xi^{-1} (1 - \mu_i) \right),
\end{equation}
which is what we use internally. Values for $\gamma_i$ can be generated using
\begin{equation}
  \gamma_i \sim \text{Beta}\left(y_i + \xi^{-1} \mu_i, w_i - y_i + \xi^{-1}(1-\mu_i) \right).
\end{equation}


### Normal {#sec:norm}

\begin{equation}
  y_i \sim \text{N}(\mu_i, w_i^{-1}\xi^2)
\end{equation}
where the $w_i$ are weights.

Response $y_i$ is standardized to have mean 0 and standard deviation 1. We set
\begin{equation}
  y_i = \frac{y_i^{*} - \bar{y}^*}{s^*}
\end{equation}
where the $y_i^*$ are the values originally supplied by the user, and $\bar{y}^*$ and $s^*$ are the mean and standard deviation of the $y_i^*$.


# Model for means

Let $\pmb{\mu} = (\mu_1, \cdots, \mu_n)^{\top}$. Our model for $\pmb{\mu}$ is
\begin{equation}
  g(\pmb{\mu}) = \sum_{m=0}^{M} \pmb{X}^{(m)} \pmb{\beta}^{(m)} + \pmb{Z} \pmb{\zeta} (\#eq:means)
\end{equation}
where

- $g$ is a log function in Poisson models, a logit function in binomial models, and an identity function in normal models;
- $\beta^{(0)}$ is an intercept;
- $\pmb{\beta}^{(m)}$, $m=1,\cdots,M$ is a vector with $J_m$ elements describing a main effect or interaction formed from the dimensions of data $\pmb{y}$;
- $\pmb{X}^{(m)}$ is an $n \times J_m$ matrix of 1s and 0s, the $i$th row of which picks out the element of $\pmb{\beta}^{(m)}$ that is used with cell $i$;
- $\pmb{Z}$ is a $n \times P$ matrix of covariates; and
- $\pmb{\zeta}$ is a coefficent vector with $P$ elements.

If a main effect or interaction contains includes a time dimensions, then prior for the term can be composed of a trend, cyclical, seasonal, and error terms,
\begin{equation}
  \pmb{\beta}^{(m)} = \pmb{\beta}_{\text{trend}}^{(m)} + \pmb{\beta}_{\text{cyc}}^{(m)} + \pmb{\beta}_{\text{seas}}^{(m)} + \pmb{\beta}_{\text{err}}^{(m)}.
\end{equation}
The trend term is compulsory. The remaining terms are optional.

# Standardization {#sec:standard}

NOTE - We are ignoring covariates for the moment. We can probably just subtract the covariates term from $\pmb{\mu}$ and then proceed as before.

Set
\begin{equation}
  \pmb{r}^{(0)} = \pmb{\mu}
\end{equation}

For $m = 0, \cdots, M$:

- Set $\tilde{\pmb{\beta}}^{(m)} = (\pmb{X}^{(m)\top} \pmb{X}^{(m)})^{-1} \pmb{X}^{(m)\top} \pmb{r}^{(m)}$
- Set $\pmb{r}^{(m+1)} = \pmb{r}^{(m)} - \pmb{X}^{(m)} \tilde{\pmb{\beta}}^{(m)}$



# Priors for Intercept, Main Effects, and Interactions {#sec:priors}


The algorithm for assigning default priors:

- If $\pmb{\beta}^{(m)}$ has one or two elements, assign $\pmb{\beta}^{(m)}$ a fixed-normal prior (Section \@ref(sec:pr-fnorm));
- otherwise, if $\pmb{\beta}^{(m)}$ is an age or time main effect, assign $\pmb{\beta}^{(m)}$ a random walk prior (Section \@ref(sec:pr-rw));
- otherwise, assign $\pmb{\beta}^{(m)}$ a normal prior (Section \@ref(sec:pr-norm))

The intercept term $\pmb{\beta}^{(0)}$ can only be given a fixed-normal prior (Section \@ref(sec:pr-fnorm)) or a Known prior (Section \@ref(sec:pr-known)).


## Normal {#sec:pr-norm}

### Model

\begin{align}
  \beta_j^{(m)} & \sim \text{N}\left(0, \tau_m^2 \right) \\
  \tau_m & \sim \text{N}^+\left(0, A_{\tau}^{(m)2}\right)
\end{align}

### Contribution to posterior density

\begin{equation}
  \text{N}(\tau_m | 0, A_{\tau}^{(m)2}) \prod_{j=1}^{J_m} \text{N}(\beta_j^{(m)} \mid 0, \tau_m^2) 
\end{equation}


### Simulation

\begin{align}
  \tau_m & \sim \text{N}^+\left(0, A_{\tau}^{(m)2}\right) \\
  \beta_j^{(m)} & \sim \text{N}\left(0, \tau_m^2 \right)
\end{align}


### Prediction

\begin{equation}
    \beta_{J_m+h+1}^{(m)} \sim \text{N}(0, \tau_m^2)
\end{equation}


### Code

```
N(s = 1)
```

- `s` is $A_{\tau}^{(m)}$. Defaults to 1.

## Fixed normal {#sec:pr-fnorm}

### Model

\begin{equation}
  \beta_j^{(m)} \sim \text{N}\left(0, A_{\beta}^{(m)2}\right)
\end{equation}

### Contribution to posterior density

\begin{equation}
  \prod_{j=1}^{J_m} \text{N}(\beta_j^{(m)} \mid 0, A_{\beta}^{(m)2}) 
\end{equation}


### Simulation

\begin{equation}
  \beta_j^{(m)} \sim \text{N}\left(0, A_{\beta}^{(m)2}\right)
\end{equation}


### Prediction

\begin{equation}
    \beta_{J_m+h+1}^{(m)} \sim \text{N}(0, A_{\beta}^{(m)2})
\end{equation}


### Code

```
NFix(sd = 1)
```

- `sd` is $A_{\tau}^{(m)}$. Defaults to 1.


## First-order random walk {#sec:pr-rw}

### Model

\begin{align}
  \beta_1^{(m)} & \sim \text{N}(0, 1) \\
  \beta_j^{(m)} & \sim \text{N}(\beta_{j-1}^{(m)}, \tau_m^2), \quad j = 2, \cdots, J_m \\
  \tau_m & \sim \text{N}^+\left(0, A_{\tau}^{(m)2}\right)
\end{align}

### Contribution to posterior density

\begin{equation}
  \text{N}(\tau_m | 0, A_{\tau}^{(m)2}) \text{N}(\beta_1^{(m)} \mid 0, 1)  \prod_{j=2}^{J_m} \text{N}\left(\beta_j^{(m)} \mid \beta_{j-1}^{(m)}, \tau_m^2 \right) 
\end{equation}


### Simulation

\begin{align}
  \tau_m & \sim \text{N}^+\left(0, A_{\tau}^{(m)2}\right) \\
  \beta_1^{(m)} & \sim \text{N}(0, 1) \\
  \beta_j^{(m)} & \sim \text{N}(\beta_{j-1}^{(m)}, \tau_m^2), \quad j = 2, \cdots, J_m
\end{align}


### Prediction

\begin{equation}
  \beta_{J_m+h}^{(m)} \sim \text{N}(\beta_{J_m+h-1}^{(m)}, \tau_m^2)
\end{equation}


### Code

```
RW(s = 1)
```

- `s` is $A_{\tau}^{(m)}$. Defaults to 1.



## Exchangeable first-order random walk {#sec:pr-erw}

### Model

\begin{align}
  \beta_{u1}^{(m)} & \sim \text{N}(0, 1), \quad u = 1, \cdots, U_m \\
  \beta_{uv}^{(m)} & \sim \text{N}(\beta_{u,v-1}^{(m)}, \tau_m^2), \quad u = 1, \cdots, U_m, v = 2, \cdots, V_m \\
  \tau_m & \sim \text{N}^+\left(0, A_{\tau}^{(m)2}\right)
\end{align}

### Contribution to posterior density

\begin{equation}
  \text{N}(\tau_m | 0, A_{\tau}^{(m)2}) \prod_{u=1}^{U_m} \text{N}(\beta_{u1}^{(m)} \mid 0, 1)  \prod_{u=1}^{U_m} \prod_{v=2}^{V_m} \text{N}\left(\beta_{uv}^{(m)} \mid \beta_{u,v-1}^{(m)}, \tau_m^2 \right) 
\end{equation}


### Simulation

\begin{align}
  \tau_m & \sim \text{N}^+\left(0, A_{\tau}^{(m)2}\right) \\
  \beta_{u,1}^{(m)} & \sim \text{N}(0, 1), \quad u = 1, \cdots, U_m, \\
  \beta_{u,v}^{(m)} & \sim \text{N}(\beta_{u,v-1}^{(m)}, \tau_m^2), \quad u = 1, \cdots, U_m, v = 2, \cdots, V_m
\end{align}


### Prediction

\begin{equation}
  \beta_{u,V_m+h}^{(m)} \sim \text{N}(\beta_{u,V_m+h-1}^{(m)}, \tau_m^2), \quad u = 1, \cdots, U_m
\end{equation}


### Code

```
ERW(s = 1)
```

- `s` is $A_{\tau}^{(m)}$. Defaults to 1.


## Second-order random walk {#sec:pr-rw2}

### Model

\begin{align}
  \beta_1^{(m)} & \sim \text{N}(0, 1) \\
  \beta_2^{(m)} & \sim \text{N}(\beta_1^{(m)}, A_{\eta}^{(m)2}) \\
  \beta_{j}^{(m)} & \sim \text{N}(2 \beta_{j-1}^{(m)} - \beta_{j-2}^{(m)}, \tau_m^2), \quad j = 3, \cdots, J_m
\end{align}


### Contribution to posterior density

\begin{equation}
  \text{N}(\tau_m | 0, A_{\tau}^{(m)2}) \text{N}(\beta_1^{(m)} \mid 0, 1) \text{N}(\beta_2^{(m)}  \mid \beta_1^{(m)}, A_{\eta}^{(m)2}) \prod_{j=3}^{J_m} \text{N}\left(\beta_j^{(m)} - 2 \beta_{j-1}^{(m)} + \beta_{j-2}^{(m)} \mid 0, \tau_m^2 \right) 
\end{equation}


### Simulation

\begin{align}
  \tau_m & \sim \text{N}^+\left(0, A_{\tau}^{(m)2}\right) \\
  \beta_1^{(m)} & \sim \text{N}(0, 1) \\
  \beta_2^{(m)} & \sim \text{N}(\beta_1^{(m)}, A_{\eta}^{(m)2}) \\
  \beta_j^{(m)} & \sim \text{N}(\beta_{j-1}^{(m)}, \tau_m^2), \quad j = 2, \cdots, J_m
\end{align}

### Prediction

\begin{equation}
  \beta_{J_m+h}^{(m)} \sim \text{N}(2 \beta_{J_m+h-1}^{(m)} - \beta_{J_m+h-2}^{(m)}, \tau_m^2)
\end{equation}


### Code

```
RW2(s = 1, sd = 1)
```

- `s` is $A_{\tau}^{(m)}$
- `sd` is $A_{\eta}^{(m)}$


## Autoregressive {#sec:pr-ar}


### Model

\begin{equation}
  \beta_j^{(m)} \sim \text{N}\left(\phi_1^{(m)} \beta_{j-1}^{(m)} + \cdots + \phi_{K_m}^{(m)} \beta_{j-{K_m}}^{(m)},  \omega_m^2\right), \quad j = K_m + 1, \cdots, J_m.
\end{equation}
Internally, TMB derives a value for $\omega_m$ that gives every term $\beta_j^{(m)}$ the same marginal variance. We denote this marginal variance $\tau_m^2$, and assign it a prior
\begin{equation}
  \tau_m  \sim \text{N}^+(0, A_{\tau}^{(m)2}).
\end{equation}
Each of the $\phi_k^{(m)}$ is restricted to the interval $(-1, 1)$. We assign each $\phi_k^{(m)}$ the prior
\begin{align}
  \phi_k^{(m)\prime} & \sim \text{Beta}(2, 2) \\
  \phi_k^{(m)} & =  2 \phi_k^{(m)\prime} - 1.
\end{align}


### Contribution to posterior density

\begin{equation}
  \text{N}^+\left(\tau_m | 0, A_{\tau}^{(m)2} \right) \left( \prod_{k=1}^{K_m} \text{Beta}\left( \phi_k^{(m)\prime} | 2, 2 \right) \right) p\left( \beta_1^{(m)}, \cdots, \beta_{J_m}^{(m)} | \phi_1^{(m)}, \cdots, \phi_{K_m}^{(m)}, \tau_m \right) 
\end{equation}
where $p\left( \beta_1^{(m)}, \cdots, \beta_{J_m}^{(m)} | \phi_1^{(m)}, \cdots, \phi_{K_m}^{(m)}, \tau_m \right)$ is calculated internally by TMB.

### Prediction

\begin{equation}
  \beta_{J_m + h}^{(m)} \sim \text{N}\left(\phi_1^{(m)} \beta_{J_m + h - 1}^{(m)} + \cdots + \phi_{K_m}^{(m)} \beta_{J_m+h-K_m}^{(m)}, \tau_m^2\right)
\end{equation}


### Code

```
AR(n = 2, s = 1)
```

- `n` is $K_m$
- `s` is $A_{\tau}^{(m)}$


## AR1 {#sec:pr-ar1}

### Model

\begin{align}
  \beta_1^{(m)} & \sim \text{N}(0, \tau_m^2) \\
  \beta_j^{(m)} & \sim \text{N}(\phi_m \beta_{j-1}^{(m)}, (1 - \phi^2) \tau_m^2), \quad j = 2, \cdots, J_m \\
  \phi_m & = a_{0,m} + (a_{1,m} - a_{0,m}) \phi_m^{\prime} \\
  \phi_m^{\prime} & \sim \text{Beta}(2, 2) \\
  \tau_m & \sim \text{N}^+\left(0, A_{\tau}^{(m)2}\right).
\end{align}
This is adapted from the specification used for AR1 densities in [TMB](http://kaskr.github.io/adcomp/classdensity_1_1AR1__t.html). We require that $-1 < a_{0m} < a_{1m} < 1$.


### Contribution to posterior density

\begin{equation}
  \text{N}(\tau_m | 0, A_{\tau}^{(m)2}) \text{Beta}( \phi_m^{\prime} | 2, 2) \text{N}\left(\beta_1^{(m)} \mid 0, \tau_m^2 \right)  \prod_{j=2}^{J_m} \text{N}\left(\beta_j^{(m)} \mid \phi_m \beta_{j-1}^{(m)}, (1 - \phi_m^2) \tau_m^2 \right) 
\end{equation}


### Prediction

\begin{equation}
  \beta_{J_m + h}^{(m)} \sim \text{N}\left(\phi_m \beta_{J_m + h - 1}^{(m)}, (1 - \phi_m^2) \tau_m^2\right)
\end{equation}



### Code

```
AR1(min = 0.8, max = 0.98, s = 1)
```

- `min` is $a_{0m}$
- `max` is $a_{1m}$
- `s` is $A_{\tau}^{(m)}$. Defaults to 1.

The defaults for `min` and `max` are based on the defaults for function `ets()` in R package **forecast** [@hyndman2008automatic].


## Exchangeable Autoregressive {#sec:pr-ear}


### Model

\begin{equation}
  \beta_{uv}^{(m)} \sim \text{N}\left(\phi_1^{(m)} \beta_{u,v-1}^{(m)} + \cdots + \phi_{K_m}^{(m)} \beta_{u,v-{K_m}}^{(m)},  \omega_m^2\right), \quad u = 1, \cdots, U_m; v = V_m + 1, \cdots, V_m.
\end{equation}
Internally, TMB derives a value for $\omega_m$ that gives every term $\beta_{uv}^{(m)}$ the same marginal variance. We denote this marginal variance $\tau_m^2$, and assign it a prior
\begin{equation}
  \tau_m  \sim \text{N}^+(0, A_{\tau}^{(m)2}).
\end{equation}
Each of the $\phi_k^{(m)}$ is restricted to the interval $(-1, 1)$. We assign each $\phi_k^{(m)}$ the prior
\begin{align}
  \phi_k^{(m)\prime} & \sim \text{Beta}(2, 2) \\
  \phi_k^{(m)} & =  2 \phi_k^{(m)\prime} - 1.
\end{align}


### Contribution to posterior density

\begin{equation}
  \text{N}^+\left(\tau_m | 0, A_{\tau}^{(m)2} \right) \left( \prod_{k=1}^{K_m} \text{Beta}\left( \phi_k^{(m)\prime} | 2, 2 \right) \right) \prod_{u=1}^{U_m} p\left( \beta_{u,1}^{(m)}, \cdots, \beta_{u,V_m}^{(m)} | \phi_1^{(m)}, \cdots, \phi_{K_m}^{(m)}, \tau_m \right) 
\end{equation}
where $p\left( \beta_{u,1}^{(m)}, \cdots, \beta_{u,V_m}^{(m)} | \phi_1^{(m)}, \cdots, \phi_{K_m}^{(m)}, \tau_m \right)$ is calculated internally by TMB.

### Prediction

\begin{equation}
  \beta_{u,V_m + h}^{(m)} \sim \text{N}\left(\phi_1^{(m)} \beta_{u,V_m + h - 1}^{(m)} + \cdots + \phi_{K_m}^{(m)} \beta_{u,V_m+h-K_m}^{(m)}, \tau_m^2\right)
\end{equation}


### Code

```
EAR(n = 2, s = 1, along = NULL)
```

- `n` is $K_m$
- `s` is $A_{\tau}^{(m)}$
- `along` is the name of the "along" dimension. If no value given, the algorithm is:
    - if interaction includes a time dimension, use that;
    - otherwise, if the interaction includes an age dimension, use that;
    - otherwise, throw an error.


## Exchangeable AR1 {#sec:pr-ear1}

TODO - complete

## Linear {#sec:pr-lin}

### Model

\begin{align}
  \beta_j^{(m)} & \sim \text{N}(q_j^{(m)} \eta^{(m)}, \tau_m^2) \\
  \eta^{(m)} & \sim \text{N}\left(0, A_{\eta}^{(m)2}\right) \\
  \tau_m & \sim \text{N}^+\left(0, A_{\tau}^{(m)2}\right)
\end{align}
where
\begin{equation}
   q_j^{(m)} = -\frac{J_m + 1}{J_m - 1} + \frac{2}{J_m - 1} j
\end{equation}


### Contribution to posterior density

\begin{equation}
  \text{N}(\tau_m | 0, A_{\tau}^{(m)2})   \text{N}(\eta^{(m)} | 0, A_{\eta}^{(m)2}) \prod_{j=1}^{J_m} \text{N}\left(\beta_j^{(m)} \mid q_j^{(m)} \eta^{(m)}, \tau_m^2 \right) 
\end{equation}


### Prediction

\begin{equation}
  \beta_{J_m + h}^{(m)} \sim \text{N}(q_{J_m + h} \eta^{(m)}, \tau_m^2)
\end{equation}


### Code

```
Lin(s = 1, sd = 1)
```

- `s` is $A_{\tau}^{(m)}$
- `sd` is $A_{\eta}^{(m)}$


## Exchangeable Linear {#sec:pr-elin}

### Model

\begin{align}
  \beta_{uv}^{(m)} & \sim \text{N}(q_v^{(m)} \eta_c^{(m)}, \tau_m^2) \\
  \eta_c^{(m)} & \sim \text{N}\left(\eta^{(m)}, \omega^{(m)2}\right) \\
  \eta^{(m)} & \sim \text{N}\left(0, A_{\eta}^{(m)2}\right) \\
  \tau_m & \sim \text{N}^+\left(0, A_{\tau}^{(m)2}\right) \\
  \omega_m & \sim \text{N}^+\left(0, A_{\omega}^{(m)2}\right)
\end{align}
where
\begin{equation}
   q_v^{(m)} = -\frac{V_m + 1}{V_m - 1} + \frac{2}{V_m - 1} v
\end{equation}

### Contribution to posterior density

\begin{equation}
  \begin{split}
  & \text{N}(\tau_m | 0, A_{\tau}^{(m)2}) \text{N}(\omega_m | 0, A_{\omega}^{(m)2})   \text{N}(\eta^{(m)} | 0, A_{\eta}^{(m)2}) \\
  & \quad \times \prod_{u=1}^{U_m} \text{N}\left(\eta_u^{(m)} \mid 0, \omega_m^2 \right) \\
  & \quad \times \prod_{u=1}^{U_m} \prod_{v=1}^{V_m} \text{N}\left(\beta_{uv}^{(m)} \mid q_v^{(m)} \eta_u^{(m)}, \tau_m^2 \right) 
  \end{split}
\end{equation}


### Prediction

\begin{equation}
  \beta_{u, V_m + h}^{(m)} \sim \text{N}(q_{V_m + h} \eta_u^{(m)}, \tau_m^2)
\end{equation}


### Code

```
ELin(s = 1, sd = 1, ms = 1, along = NULL)
```

- `s` is $A_{\tau}^{(m)}$
- `sd` is $A_{\eta}^{(m)}$
- `ms` is $A_{\omega}^{(m)}$
- `along` is the name of the "along" dimension. If no value given, the algorithm is:
    - if interaction includes a time dimension, use that;
    - otherwise, if the interaction includes an age dimension, use that;
    - otherwise, throw an error.


## P-Spline {#sec:pr-spline}

### Model

\begin{equation}
  \pmb{\beta}^{(m)} = \pmb{B}^{(m)} \pmb{\alpha}^{(m)}
\end{equation}
where $\pmb{B}^{(m)}$ is a $J_m \times K_m$ matrix of B-splines, and $\pmb{\alpha}^{(m)}$ has a second-order random walk prior (Section \@ref(sec:pr-rw2)).

$\pmb{B}^{(m)} = (\pmb{b}_1^{(m)}(\pmb{j}), \cdots, \pmb{b}_{K_m}^{(m)}(\pmb{j}))$, with $\pmb{j} = (1, \cdots, J_m)^{\top}$. The B-splines are centered, so that $\pmb{1}^{\top} \pmb{b}_k^{(m)}(\pmb{j}) = 0$, $k = 1, \cdots, K_m$.


### Contribution to posterior density

\begin{equation}
  \text{N}(\tau_m | 0, A_{\tau}^{(m)2}) \text{N}(\eta_0^{(m)} \mid 0, A_{\eta,0}^{(m)})  \text{N}(\eta_1^{(m)} \mid 0, A_{\eta,1}^{(m)}) \prod_{k=3}^{K_m} \text{N}\left(\alpha_k^{(m)} - 2 \alpha_{k-1}^{(m)} + \alpha_{k-2}^{(m)} \mid 0, \tau_m^2 \right) 
\end{equation}
where
\begin{align}
  \eta_0^{(m)} & = \frac{1}{K_m} \sum_{k=1}^{K_m} \alpha_k^{(m)} \\
  \eta_1^{(m)} & = \frac{\sum_{k=1}^{K_m} \alpha_k^{(m)} q_k^{(m)}}{\sum_{k=1}^{K_m} q_k^{(m)2}} \\
  q_k^{(m)} & = -\frac{K_m + 1}{K_m - 1} + \frac{2}{K_m - 1} j \\
\end{align}


### Prediction

Main effects with a P-Spline prior cannot be predicted.


### Code

```
Sp(n = NULL, s = 1)
```

- `n` is $K_m$. Defaults to $\max(0.7 J_m, 4)$. *This is the current default for BayesRates. Could we use a smaller multiplier?*
- `s` is the $A_{\tau}^{(m)}$ from the second-order random walk prior. Defaults to 1.


## SVD {#sec:pr-svd}

### Model

\begin{align}
  \pmb{\beta}^{(m)} & = \pmb{F}^{(m)} \pmb{\alpha}^{(m)} + \pmb{g}^{(m)} \\
  \alpha_k^{(m)} & \sim \text{N}(0, 1), \quad k = 1, \cdots, K_m
\end{align}
where $\pmb{F}^{(m)}$ is a $J_m \times K_m$ matrix, and $\pmb{g}^{(m)}$ is a vector with $J_m$ elements, both derived from a singular value decomposition (SVD) of an external dataset of age-specific values for females and males combined. The construction of $\pmb{F}^{(m)}$ and $\pmb{g}^{(m)}$ is described in Appendix \@ref(app:svd). The construction involves centering and scaling, which allows us to use a simple prior for $\pmb{\alpha}^{(m)}$.


### Contribution to posterior density

\begin{equation}
  \prod_{k=1}^{K_m} \text{N}\left(\alpha_k^{(m)} \mid 0, 1 \right) 
\end{equation}

### Prediction

Main effects with a SVD prior cannot be predicted.


### Code

```
SVD(scaled_svd, n = 5)
```


## SVD2 {#sec:pr-svd2}

The SVD2 is a two-sex extension of the basic SVD model. SVD2 has two versions. The first is identical to the basic SVD model, except that different, sex-specific values for $\pmb{F}$ and $\pmb{g}$ are used for females and males. The second treats the Cartesian product of age and sex dimensions as if it was a single dimension. Values for $\pmb{F}$ and $\pmb{g}$ estimated from age-sex-specific rates. Each is twice as large as the basic SVD versions of $\pmb{F}$ and $\pmb{g}$.

The code is
```
SVD2(scaled_svd, joint = FALSE, n = 5)
```
When `joint` is FALSE, the first version is used, and when `joint` is TRUE the second version is used.


## Exchangeable SVD {#sec:pr-esvd}

### Model

\begin{align}
  \pmb{\beta}_u^{(m)} & = \pmb{F}^{(m)} \pmb{\alpha}_u^{(m)} + \pmb{g}^{(m)}, \quad u = 1, \cdots, U_m \\
  \alpha_{uk}^{(m)} & \sim \text{N}(0, 1), \quad u = 1, \cdots, U_m, k = 1, \cdots, K_m
\end{align}
where $\pmb{F}^{(m)}$ is a $V_m \times K_m$ matrix, and $\pmb{g}^{(m)}$ is a vector with $V_m$ elements, both derived from a singular value decomposition (SVD) of an external dataset of age-specific values for females and males combined.


### Contribution to posterior density

\begin{equation}
  \prod_{u=1}^{U_m}\prod_{k=1}^{K_m} \text{N}\left(\alpha_{uk}^{(m)} \mid 0, 1 \right) 
\end{equation}

### Prediction

Main effects with a SVD prior cannot be predicted.


### Code

```
ESVD(scaled_svd, n = 5)
```


## Known {#sec:pr-known}

### Model

Elements of $\pmb{\beta}^{(m)}$ are treated as known with certainty.

### Contribution to posterior density

Known priors make no contribution to the posterior density.

### Prediction

Main effects with a known prior cannot be predicted.


### Code

```
Known(values)
```

- `values` is a vector containing the $\beta_j^{(m)}$.


## Seasonal effects {#sec:pr-season}

### Model

\begin{align}
  \beta_j^{(m)} & \sim \text{N}(0, 1), \quad j = 1, \cdots, S_m \\
  \beta_j^{(m)} & \sim \text{N}(\beta_{j - S_m}^{(m)}, \tau_m^2 ), \quad j = S_m+1, \cdots, J_m \\ 
  \tau_m & \sim \text{N}^+\left(0, A_{\tau}^{(m)2} \right) \\
\end{align}


### Contribution to posterior density

\begin{equation}
  \text{N}(\tau_m | 0, A_{\tau}^{(m)2}) \left( \prod_{j=1}^{S_m} \text{N}(\beta_j^{(m)} \mid 0, 1 ) \right)  \left( \prod_{j=S_m+1}^{J_m} \text{N}(\beta_j^{(m)} \mid \beta_{j-S_m}^{(m)}, \tau_m^2) \right) 
\end{equation}n

### Prediction

\begin{equation}
  \beta_{J_m+h}^{(m)} \sim \text{N}(\beta_{J_m+h-S_m}^{(m)}, \tau_m^2)
\end{equation}

### Code

```
Seas(n, s = 1)
```

- `n` is $S_m$
- `s` is $A_{\tau}^{(m)}$

## Exchangeable seasonal {#sec:pr_eseas}

### Model

\begin{align}
  \beta_{uv}^{(m)} & \sim \text{N}(0, 1), \quad u = 1, \cdots, U_m, \quad v = 1, \cdots, S_m \\
  \beta_{uv}^{(m)} & \sim \text{N}(\beta_{u,v - S_m}^{(m)}, \tau_m^2 ), \quad u = 1, \cdots, U_m,\quad v = S_m+1, \cdots, V_m \\ 
  \tau_m & \sim \text{N}^+\left(0, A_{\tau}^{(m)2} \right) \\
\end{align}


### Contribution to posterior density

\begin{equation}
  \text{N}(\tau_m | 0, A_{\tau}^{(m)2}) \left( \prod_{u=1}^{U_m} \prod_{v=1}^{S_m} \text{N}(\beta_{uv}^{(m)} \mid 0, 1 ) \right)  \left( \prod_{u=1}^{U_M} \prod_{v=S_m+1}^{V_m} \text{N}(\beta_{uv}^{(m)} \mid \beta_{u,v-S_m}^{(m)}, \tau_m^2) \right) 
\end{equation}

### Prediction

\begin{equation}
  \beta_{u,V_m+h}^{(m)} \sim \text{N}(\beta_{u,V_m+h-S_m}^{(m)}, \tau_m^2)
\end{equation}

### Code

```
ESeas(n, s = 1)
```

- `n` is $S_m$
- `s` is $A_{\tau}^{(m)}$


# Covariates {#sec:pr-cov}

## Model

The columns of matrix $\pmb{Z}$ are assumed to be standardised to have mean 0 and standard deviation 1.  $\pmb{Z}$ does not contain a column for an intercept.

We implement two priors for coefficient vector $\pmb{\zeta}$. The first prior is designed for the case where $P$, the number of colums of $\pmb{Z}$, is small, and most $\zeta_p$ are likely to distinguishable from zero. The second prior is designed for the case where $P$ is large, and only a few $\zeta_p$ are likely to be distinguishable from zero.


### Standard prior

\begin{align}
  \zeta_p | \varphi & \sim \text{N}(0, \varphi^2) \\
  \varphi & \sim \text{N}^+(0, 1)
\end{align}


### Shrinkage prior

Regularised horseshoe prior [@piironen2017hyperprior]

\begin{align}
  \zeta_p | \vartheta_p, \varphi & \sim \text{N}(0, \vartheta_p^2 \varphi^2) \\
  \vartheta_p & \sim \text{Cauchy}^+(0, 1) \\
  \varphi & \sim \text{Cauchy}^+(0, A_{\varphi}^2) \\
  A_{\varphi} & = \frac{p_0}{p_0 + P} \frac{\hat{\sigma}}{\sqrt{n}}
\end{align}
where $p_0$ is an initial guess at the number of $\zeta_p$ that are non-zero, and $\hat{\sigma}$ is obtained as follows:

- Poisson. Using maximum likelihood, fit the GLM
\begin{align}
  y_i & \sim \text{Poisson}(w_i \gamma_i) \\
  \log \gamma_i & = \sum_{m=0}^{M} \pmb{X}^{(m)} \pmb{\beta}^{(m)},
\end{align}
and set
\begin{equation}
  \hat{\sigma} = \frac{1}{n}\sum_{i=1}^n \frac{1}{w_i \hat{\gamma}_i}.
\end{equation}
- Binomial. Using maximum likelihood, fit the GLM
\begin{align}
  y_i & \sim \text{binomial}(w_i, \gamma_i) \\
  \text{logit} \gamma_i & = \sum_{m=0}^{M} \pmb{X}^{(m)} \pmb{\beta}^{(m)},
\end{align}
and set
\begin{equation}
  \hat{\sigma} = \frac{1}{n}\sum_{i=1}^n \frac{1}{w_i \hat{\gamma}_i (1 - \hat{\gamma}_i)}.
\end{equation}
- Normal. Using maximum likelihood, fit the linear model
\begin{equation}
  y_i \sim \text{N}\left(\sum_{m=0}^{M} \pmb{X}^{(m)} \pmb{\beta}^{(m)}, w_i^{-1}\xi^2 \right)
\end{equation}
and set $\hat{\sigma} = \hat{\xi}$.


The quantities used for Poisson and binomial likelihoods are derived from normal approximations to GLMs [@piironen2017hyperprior; @gelman2014bayesian, section 16.2].

## Contribution to posterior density

TODO - write

## Prediction

TODO - write

## Code

```
set_covariates(formula, data = NULL, n_coef = NULL)
```

- `formula` is a one-sided R formula describing the covariates to be used
- `data` A data frame. If a value for `data` is supplied, then `formula` is interpreted in the context of this data frame. If a value for `data` is not supplied, then `formula` is interpreted in the context of the data frame used for the original call to `mod_pois()`, `mod_binom()`, or `mod_norm()`.
- `n_coef` is the effective number of non-zero coefficients. If a value is supplied, the shrinkage prior is used; otherwise the standard prior is used.


Examples:
```
set_covariates(~ mean_income + distance * employment)
set_covariates(~ ., data = cov_data, n_coef = 5)
```


# Prior for dispersion terms

## Model

Use exponential distribution, parameterised using mean,
\begin{equation}
  \xi \sim \text{Exp}(\mu_{\xi})
\end{equation}


## Contribution to prior density

\begin{equation}
  p(\xi) = \frac{1}{\mu_{\xi}} \exp\left(\frac{-\xi}{\mu_{\xi}}\right)
\end{equation}

## Code

```
set_disp(mean = 1)
```

- `mean` is $\mu_{\xi}$


# Deriving outputs

Running TMB yields a set of means $\pmb{m}$, and a precision matrix $\pmb{Q}^{-1}$, which together define the approximate joint posterior distribution of

- $\pmb{\beta}^{(m)}$ for terms with independent normal, fixed normal, multivariate normal, random walk, second-order random walk, AR1, Linear, and Linear-AR1 priors,
- $\pmb{\alpha}$ for terms with Spline and SVD priors,
- hyper-parameters for $\pmb{\beta}^{(m)}$ and $\pmb{\alpha}^{(m)}$ typically transformed to another scale, such as a log scale,
- dispersion term $\xi$, and
- seasonal effects $\pmb{\lambda}$, together with associated hyper-parameters $\tau_{\lambda}$ (on a log scale).

We use $\tilde{\pmb{\theta}}$ to denote a vector containing all these quantities.

We perform a Cholesky decomposition of $\pmb{Q}^{-1}$, to obtain $\pmb{R}$ such that
\begin{equation}
  \pmb{R}^{\top} \pmb{R} = \pmb{Q}^{-1}
\end{equation}
We store $\pmb{R}$ as part of the model object.

We draw generate values for $\tilde{\pmb{\theta}}$ by generating a vector of standard normal variates $\pmb{z}$, back-solving the equation
\begin{equation}
  \pmb{R} \pmb{v} = \pmb{z}
\end{equation}
and setting
\begin{equation}
  \tilde{\pmb{\theta}} = \pmb{v} + \pmb{m}.
\end{equation}

Next we convert any transformed hyper-parameters back to the original units, and insert values for $\pmb{\beta}^{(m)}$ for terms that have Known priors. We denote the resulting vector $\pmb{\theta}$.

Finally we draw from the distribution of $\pmb{\gamma} \mid \pmb{y}, \pmb{\theta}$ using the methods described in Section \@ref(sec:lik).

# Simulation

To generate one set of simulated values, we start with values for exposure, trials, or weights,  $\pmb{w}$, and possibly covariates $\pmb{Z}, then go through the following steps:

1. Draw values for any parameters in the priors for the $\pmb{\beta}^{(m)}$, $m = 1, \cdots, M$.
1. Conditional on the values drawn in Step 1, draw values the $\pmb{\beta}^{(m)}$, $m = 0, \cdots, M$.
1. If the model contains seasonal effects, draw the standard deviation $\kappa_m$, and then the effects $\pmb{\lambda}^{(m)}$.
1. If the model contains covariates, draw $\varphi$ and $\vartheta_p$ where necessary, draw coefficient vector $\pmb{\zeta}$.
1. Use values from steps 2--4 to form the linear predictor $\sum_{m=0}^{M} \pmb{X}^{(m)} (\pmb{\beta}^{(m)} + \pmb{\lambda}^{(m)}) + \pmb{Z} \pmb{\zeta}$.
1. Back-transform the linear predictor, to obtain vector of cell-specific parameters $\pmb{\mu}$.
1. If the model contains a dispersion parameter $\xi$, draw values from the prior for $\xi$.
1. In Poisson and binomial models, use $\pmb{\mu}$ and, if present, $\xi$ to draw $\pmb{\gamma}$.
1. In Poisson and binomial models, use $\pmb{\gamma}$ and $\pmb{w}$ to draw $\pmb{y}$; in normal models, use $\pmb{\mu}$, $\xi$, and $\pmb{w}$ to draw $\pmb{y}$.


# Replicate data

## Model

### Poisson likelihood

#### Condition on $\pmb{\gamma}$

\begin{equation}
  y_i^{\text{rep}} \sim \text{Poisson}(\gamma_i w_i)
\end{equation}

#### Condition on $(\pmb{\mu}, \xi)$

\begin{align}
  y_i^{\text{rep}} & \sim \text{Poisson}(\gamma_i^{\text{rep}} w_i) \\
  \gamma_i^{\text{rep}} & \sim \text{Gamma}(\xi^{-1}, (\xi \mu_i)^{-1})
\end{align}
which is equivalent to
\begin{equation}
  y_i^{\text{rep}} \sim \text{NegBinom}\left(\xi^{-1}, (1 + \mu_i w_i \xi)^{-1}\right)
\end{equation}

### Binomial likelihood

#### Condition on $\pmb{\gamma}$

\begin{equation}
  y_i^{\text{rep}} \sim \text{Binomial}(w_i, \gamma_i)
\end{equation}

#### Condition on $(\pmb{\mu}, \xi)$

\begin{align}
  y_i^{\text{rep}} & \sim \text{Binomial}(w_im \gamma_i^{\text{rep}}) \\
  \gamma_i^{\text{rep}} & \sim \text{Beta}\left(\xi^{-1} \mu_i, \xi^{-1}(1 - \mu_i)\right)
\end{align}

### Normal likelihood

\begin{equation}
  y_i^{\text{rep}} \sim \text{N}(\gamma_i, \xi^2 / w_i)
\end{equation}

## Code

```
replicate_data(x, condition_on = c("fitted", "expected"), n = 20)
```

# Appendices

## Definitions {#app:defn}


| Quantity | Definition                                          |
|:---------|:----------------------------------------------------|
| $i$      | Index for cell, $i = 1, \cdots, n$.
| $y_i$    | Value for outcome variable.              |
| $w_i$    | Exposure, number of trials, or weight. |
| $\gamma_i$ | Super-population rate, probability, or mean. |
| $\mu_i$ | Cell-specific mean. |
| $\xi$   | Dispersion parameter. |
| $g()$ | Log, logit, or identity function. |
| $m$ | Index for intercept, main effect, or interaction. $m = 0, \cdots, M$. |
| $j$ | Index for element of a main effect or interaction. |
| $u$ | Index for combination of 'by' variables for an interaction. $u = 1, \cdots U_m$. $U_m V_m = J_m$ |
| $v$ | Index for the 'along' dimension of an interaction. $v = 1, \cdots V_m$. $U_m V_m = J_m$ |
| $\beta^{(0)}$ | Intercept. |
| $\pmb{\beta}^{(m)}$ | Main effect or interaction. $m = 1, \cdots, M$.  |
| $\beta_j^{(m)}$ | $j$th element of $\pmb{\beta}^{(m)}$. $j = 1, \cdots, J_m$. |
| $\pmb{X}^{(m)}$ | Matrix mapping $\pmb{\beta}^{(m)}$ to $\pmb{y}$. |
| $\pmb{Z}$ | Matrix of covariates. |
| $\pmb{\zeta}$ | Parameter vector for covariates $\pmb{Z}^{(m)}$. |
| $A_0$ | Scale parameter in prior for intercept $\beta^{(0)}$. |
| $\tau_m$ | Standard deviation parameter for main effect or interaction. |
| $A_{\tau}^{(m)}$ | Scale parameter in prior for $\tau_m$. |
| $\pmb{\alpha}^{(m)}$ | Parameter vector for P-spline and SVD priors. |
| $\alpha_k^{(m)}$ | $k$th element of $\pmb{\alpha}^{(m)}$. $k = 1, \cdots, K_m$. |
| $\pmb{V}^{(m)}$ | Covariance matrix for multivariate normal prior. |
| $h_j^{(m)}$ | Linear covariate |
| $\eta^{(m)}$ | Parameter specific to main effect or interaction $\pmb{\beta}^{(m)}$. |
| $\eta_u^{(m)}$ | Parameter specific to $u$th combination of 'by' variables in interaction $\pmb{\beta}^{(m)}$. |
| $A_{\eta}^{(m)}$ | Standard deviation in normal prior for $\eta_m$. |
| $\omega_m$ | Standard deviation of parameter $\eta_c$ in multivariate priors. |
| $\phi_m$ | Correlation coefficient in AR1 densities. |
| $a_{0m}$, $a_{1m}$ | Minimum and maximum values for $\phi_m$. |
| $\pmb{B}^{(m)}$ | B-spline matrix in P-spline prior. |
| $\pmb{b}_k^{(m)}$ | B-spline. $k = 1, \cdots, K_m$. |
| $\pmb{F}^{(m)}$ | Matrix in SVD prior. |
| $\pmb{g}^{(m)}$ | Offset in SVD prior. |
| $\pmb{\beta}_{\text{trend}}$ | Trend effect. |
| $\pmb{\beta}_{\text{cyc}}$ | Cyclical effect. |
| $\pmb{\beta}_{\text{seas}}$ | Seasonal effect. |
| $\varphi$ | Global shrinkage parameter in shrinkage prior. |
| $A_{\varphi}$ | Scale term in prior for $\varphi$. |
| $\vartheta_p$ | Local shrinkage parameter in shrinkage prior. |
| $p_0$ | Expected number of non-zero coefficients in $\pmb{\zeta}$. |
| $\hat{\sigma}$ | Empirical scale estimate in prior for $\varphi$. |
| $\pi$ | Vector of hyper-parameters |

 
## SVD prior for age  {#app:svd}

Let $\pmb{A}$ be a matrix of age-specific estimates from an international database, transformed to take values in the range $(-\infty, \infty)$. Each column of $\pmb{A}$ represesents one set of age-specific estimates, such as log mortality rates in Japan in 2010, or logit labour participation rates in Germany in 1980.

Let $\pmb{U}$, $\pmb{D}$, $\pmb{V}$ be the matrices from a singular value decomposition of $\pmb{A}$, where we have retained the first $K$ components. Then
\begin{equation}
  \pmb{A} \approx \pmb{U} \pmb{D} \pmb{V}. (\#eq:svd1)
\end{equation}

Let $m_k$ and $s_k$ be the mean and sample standard deviation of the elements of the $k$th row of $\pmb{V}$, with $\pmb{m} = (m_1, \cdots, m_k)^{\top}$ and $\pmb{s} = (s_1, \cdots, s_k)^{\top}$. Then
\begin{equation}
  \tilde{\pmb{V}} = (\text{diag}(\pmb{s}))^{-1} (\pmb{V} - \pmb{m} \pmb{1}^{\top})
\end{equation}
is a standardized version of $\pmb{V}$.

We can rewrite \@ref(eq:svd1) as
\begin{align}
  \pmb{A} & \approx \pmb{U} \pmb{D} (\text{diag}(\pmb{s}) \tilde{\pmb{V}} + \pmb{m} \pmb{1}^{\top}) \\
  & = \pmb{F} \tilde{\pmb{V}} + \pmb{m} \pmb{1}^{\top}, (\#eq:svd2)
\end{align}
where $\pmb{F} = \pmb{U} \pmb{D} \text{diag}(\pmb{s})$ and $\pmb{m} = \pmb{U} \pmb{D} \pmb{m}$.

Let $\tilde{\pmb{v}}_l$ be a randomly-selected column from $\tilde{\pmb{V}}$. From the construction of $\tilde{\pmb{V}}$, and the orthogonality of the rows of $\pmb{V}$, we have $\text{E}[\tilde{\pmb{v}}_l] = \pmb{0}$ and $\text{var}[\tilde{\pmb{v}}_l] = \pmb{I}$. This implies that if $\pmb{z}$ is a vector of standard normal variables, then
\begin{equation}
  \pmb{F} \pmb{z} + \pmb{g}
\end{equation}
looks approximately like a randomly-selected column from the original data matrix $\pmb{A}$.


# References



% ## Multivariate normal {#sec:pr-mnorm}


% ### Model

% \begin{align}
%   \pmb{\beta}^{(m)} & \sim \text{N}\left(\pmb{0},  \tau_m^2 \pmb{V}^{(m)} \right) \\
%   \tau_m & \sim \text{N}^+\left(0, A_{\tau}^{(m)2}\right)
% \end{align}
% where $\pmb{V}^{(m)}$ is a $J_m \times J_m$ symmetric positive-definite matrix supplied by the user.

% ### Contribution to posterior density

% \begin{equation}
%   \text{N}(\tau_m | 0, A_{\tau}^{(m)2}) \text{N}\left(\pmb{\beta}^{(m)} \mid \pmb{0},  \tau_m^2 \pmb{V}^{(m)} \right)
% \end{equation}

% ### Simulation

% \begin{align}
%   \tau_m & \sim \text{N}^+\left(0, A_{\tau}^{(m)2}\right) \\
%   \pmb{\beta}^{(m)} & \sim \text{N}\left(\pmb{0},  \tau_m^2 \pmb{V}^{(m)} \right)
% \end{align}


% ### Prediction

% Main effects with a multivariate normal prior cannot be predicted.


% ### Code

% ```
% MVN(V, s = 1)
% ```

% - `V` is $\pmb{V}^{(m)}$
% - `s` is $A_{\tau}^{(m)}$. Defaults to 1.
