---
title: "Mathematical description of models used in bage"
output:
  bookdown::html_document2:
    base_format: rmarkdown::html_vignette
    fig_caption: yes
    toc: true
    toc_depth: 2
    number_sections: true
pkgdown:
  as_is: true
vignette: >
  %\VignetteIndexEntry{Mathematical description of models used in bage}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
bibliography: references.bib
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

$\newcommand{\ind}{\, \overset{\text{ind}}{\sim} \,}$


```{r setup}
library(bage)
```

# Introduction

Specification document - a mathematical description of models used by bage.

Note: this is describing the models we want to get to soon, not the ones we currently implement.

# Likelihood {#sec:lik}

## Input data

- outcome variable: events, numbers of people, or some sort of measure on a continuous variable such as income or expenditure
- exposure/size/weights
- disagg by one or more variables. Almost always includes age, sex/gender, and time. May include other variables eg region, ethnicity, education.
- not all combinations of variables present; may be some missing values


## Models

### Poisson {#sec:pois}

Let $y_i$ be a count of events in cell $i = 1, \cdots, n$ and let $w_i$ be the corresponding exposure measure, with the possibility that $w_i \equiv 1$. The likelihood under the Poisson model is then
\begin{align}
  y_i \mid \gamma_i, w_i & \ind \text{Poisson}(\gamma_i w_i) (\#eq:lik-pois-1) \\
  \gamma_i \mid \mu_i, \xi & \ind \text{Gamma}\left(\xi^{-1}, (\mu_i \xi)^{-1}\right),  (\#eq:lik-pois-2)
\end{align}
using the shape-rates parameterisation of the Gamma distribution. Parameter $\xi$ governs dispersion, with
\begin{equation}
  \text{var}(\gamma_i \mid \mu_i, \xi) = \xi \mu_i^2
\end{equation}
and
\begin{equation}
  \text{var}(y_i \mid \mu_i, \xi, w_i) = (1 + \xi \mu_i w_i ) \times \mu_i w_i.
\end{equation}
We allow $\xi$ to equal 0, in which case the model reduces to 
\begin{equation}
  y_i \mid \mu_i, w_i \ind \text{Poisson}(\mu_i w_i).
\end{equation}

For $\xi > 0$, Equations \@ref(eq:lik-pois-1) and \@ref(eq:lik-pois-2) are equivalent to
\begin{equation}
  y_i \sim \text{NegBinom}\left(\xi^{-1}, (1 + \mu_i w_i \xi)^{-1}\right)
\end{equation}
[@norton2018sampling; @simpson2022priors].
This is the format we use internally for estimation. When values for $\gamma_i$ are needed, we generate them on the fly, using the fact that
\begin{equation}
  \gamma_i \mid y_i, \mu_i, \xi \ind \text{Gamma}\left(y_i + \xi^{-1}, w_i + (\xi \mu_i)^{-1}\right).
\end{equation}


### Binomial {#sec:binom}

The likelihood under the binomial model is
\begin{align}
  y_i \mid \gamma_i, w_i & \ind \text{Binomial}(w_i, \gamma_i) (\#eq:lik-binom-1) \\
  \gamma_i \mid \mu_i, \xi & \ind \text{Beta}\left(\xi^{-1} \mu_i, \xi^{-1}(1 - \mu_i)\right). (\#eq:lik-binom-2)
\end{align}
Parameter $\xi$ again governs dispersion, with
\begin{equation}
  \text{var}(\gamma_i \mid \mu_i, \xi) =  \frac{\xi}{1 + \xi} \times \mu_i (1 -\mu_i)
\end{equation}
and
\begin{equation}
  \text{var}(y_i \mid w_i, \mu_i, \xi) =  \frac{\xi w_i + 1}{\xi + 1} \times w_i \mu_i (1 - \mu_i).
\end{equation}

We allow $\xi$ to equal 0, in which case the model reduces to 
\begin{equation}
  y_i \mid w_i, \mu_i \ind \text{Binom}(w_i, \mu_i).
\end{equation}
Equations \@ref(eq:lik-binom-1) and \@ref(eq:lik-binom-2) are then equivalent to
\begin{equation}
  y_i | w_i, \mu_i, \xi \ind \text{BetaBinom}\left(w_i, \xi^{-1} \mu_i, \xi^{-1} (1 - \mu_i) \right),
\end{equation}
which is what we use internally. Values for $\gamma_i$ can be generated using
\begin{equation}
  \gamma_i \mid y_i, w_i, \mu_i, \xi \sim \text{Beta}\left(y_i + \xi^{-1} \mu_i, w_i - y_i + \xi^{-1}(1-\mu_i) \right).
\end{equation}


### Normal {#sec:norm}

\begin{equation}
  y_i \sim \text{N}(\mu_i, w_i^{-1}\xi^2)
\end{equation}
where the $w_i$ are weights.

Response $y_i$ is standardized to have mean 0 and standard deviation 1. We set
\begin{equation}
  y_i = \frac{y_i^{*} - \bar{y}^*}{s^*}
\end{equation}
where the $y_i^*$ are the values originally supplied by the user, and $\bar{y}^*$ and $s^*$ are the mean and standard deviation of the $y_i^*$.


# Model for means

Let $\pmb{\mu} = (\mu_1, \cdots, \mu_n)^{\top}$. Our model for $\pmb{\mu}$ is
\begin{equation}
  \pmb{\mu} = \sum_{m=0}^{M} \pmb{X}^{(m)} \pmb{\beta}^{(m)} + \pmb{Z} \pmb{\zeta} (\#eq:means)
\end{equation}
where

- $\beta^{(0)}$ is an intercept;
- $\pmb{\beta}^{(m)}$, $m=1,\cdots,M$ is a vector with $J_m$ elements describing a main effect or interaction formed from the dimensions of data $\pmb{y}$;
- $\pmb{X}^{(m)}$ is an $n \times J_m$ matrix of 1s and 0s, the $i$th row of which picks out the element of $\pmb{\beta}^{(m)}$ that is used with cell $i$;
- $\pmb{Z}$ is a $n \times P$ matrix of covariates; and
- $\pmb{\zeta}$ is a coefficent vector with $P$ elements.

If a main effect or interaction contains includes a time dimensions, then prior for the term can be decomposed into a trend, cyclical, and seasonal terms,
\begin{equation}
  \pmb{\beta}^{(m)} = \pmb{\psi}^{(m)} + \pmb{\chi}^{(m)} + \pmb{\lambda}^{(m)}
\end{equation}
where

- $\pmb{\psi}^{(m)}$ is a trend;
- $\pmb{\chi}^{(m)}$ is a cyclical effect;
- $\pmb{\lambda}^{(m)}$ is a seasonal effect.

When a term is decomposed, the trend term is required, and the cyclical and seasonal effects are both optional.


# Standardization {#sec:standard}

We adopt notation from @gelman2014bayesian [p. 395]. Let $\beta_{j_i^m}^{(m)}$ denote the element of $\pmb{\beta}^{(m)}$ that contributes to element $i$ of $\pmb{\mu}$.

- $\mu_i$ well identified by data
- For some combinations of prior model for $\pmb{\beta}^{(m)}$ and terms, eg random walk prior, only weakly identify level terms; result is that individual $\beta_{j_i^m}^{(m)}$ are weakly identified; data cannot distinguish between
\begin{equation}
  \beta^{(0)} + \beta_{j_i^1}^{(1)} + \cdots + \beta_{j_i^M}^{(M)}
\end{equation}
and
\begin{equation}
  (\beta^{(0)} + \Delta_0) + (\beta_{j_i^1}^{(1)} + \Delta_1) + \cdots + (\beta_{j_i^M}^{(M)} + \Delta_M),
\end{equation}
where $\sum_{m=0}^M \Delta_m = 0$.
- One solution would be to make stronger priors: eg sum-to-zero constraints; but this sacrifices sparsity, and leads to much slower calculations in TMB
- alternative: postprocessing. Rescale each draw from posterior distribution. For each draw, choose a set of $\Delta_m$ that make draws consistent

Define $\phi_j^m$ as the set of $i$ such that $j_i^m = j$, and $n_j^m$ as the number of elements in $\phi_j^m$.

Algorithm for processing a single draw from the posterior distribution:

- Set $\hat{\beta}^{(0)} = \sum_{i=1}^n \mu_i \big/ n$ and $\epsilon_i^{(0)} = \mu_i - \hat{\beta}^{(0)}$
- For $m = 1, \cdots, M$: Set $\hat{\beta}_j^{(m)} = \sum_{i \in \phi_j^m} \epsilon_i^{(m-1)} \big/ n_j^m$ and $\epsilon_i^{(m)} = \epsilon_i^{(m-1)} - \hat{\beta}_{j_i^m}^{(m)}$


Proof that $\sum_{m=0}^M \hat{\beta}_{j_i^m} = \sum_{m=0}^M \beta_{j_i^m}$. Proof by induction. When $M = 0$, $\hat{\beta}^0 = \sum_{i=1}^n \mu_i \big/ n = \sum_{i=1}^n \beta^0 \big/ n = \beta^0$. Assume true for $k-1$. Then $\sum_{m=0}^k \hat{\beta}_{j_i^m} = \sum_{m=0}^{k-1} \beta_{j_i^m} + \hat{\beta}_{j_i^k}$




# Priors for $\pmb{\beta}$ {#sec:priors}

## Independent normal {#sec:pr-inorm}

### Model

\begin{align}
  \beta_j^{(m)} | \tau_m & \ind \text{N}\left(0, \tau_m^2 \right) \\
  \tau_m & \sim \text{N}^+\left(0, A_{\tau}^{(m)2}\right)
\end{align}

### Contribution to posterior density

\begin{equation}
  \text{N}(\tau_m | 0, A_{\tau}^{(m)2}) \prod_{j=1}^{J_m} \text{N}(\beta_j^{(m)} \mid 0, \tau_m^2) 
\end{equation}


### Simulation

\begin{align}
  \tau_m & \sim \text{N}^+\left(0, A_{\tau}^{(m)2}\right) \\
  \beta_j^{(m)} | \tau_m & \ind \text{N}\left(0, \tau_m^2 \right)
\end{align}


### Prediction

\begin{equation}
    \beta_{J_m+h+1}^{(m)} \sim \text{N}(0, \tau_m^2)
\end{equation}


### Code

```
N(s = 1)
```

- `s` is $A_{\tau}^{(m)}$. Defaults to 1.

## Fixed normal {#sec:pr-fnorm}

### Model

\begin{equation}
  \beta_j^{(m)} | A_{\beta}^{(m)} \ind \text{N}\left(0, A_{\beta}^{(m)2}\right)
\end{equation}

### Contribution to posterior density

\begin{equation}
  \prod_{j=1}^{J_m} \text{N}(\beta_j^{(m)} \mid 0, A_{\beta}^{(m)2}) 
\end{equation}


### Simulation

\begin{equation}
  \beta_j^{(m)} | A_{\beta}^{(m)} \ind \text{N}\left(0, A_{\beta}^{(m)2}\right)
\end{equation}


### Prediction

\begin{equation}
    \beta_{J_m+h+1}^{(m)} \sim \text{N}(0, A_{\beta}^{(m)2})
\end{equation}


### Code

```
NFix(sd = 1)
```

- `sd` is $A_{\tau}^{(m)}$. Defaults to 1.


## First-order random walk {#sec:pr-rw}

### Model

\begin{equation}
  \beta_{j+1}^{(m)} \sim \text{N}(\beta_j^{(m)}, \tau_m^2),
\end{equation}
$j = 1, \cdots, J_m - 1$, together with the soft constraint
\begin{equation}
 \eta^{(m)} \sim \text{N}(0, A_{\eta}^{(m)2}),
\end{equation}
where
\begin{equation}
  \eta^{(m)} = \frac{1}{J_m} \sum_{j=1}^{J_m} \beta_j^{(m)}.
\end{equation}

### Contribution to posterior density

\begin{equation}
  \text{N}(\tau_m | 0, A_{\tau}^{(m)2}) \text{N}(\eta^{(m)} \mid 0, A_{\eta}^{(m)2})  \prod_{j=2}^{J_m} \text{N}\left(\beta_j^{(m)} - \beta_{j-1}^{(m)} \mid 0, \tau_m^2 \right) 
\end{equation}


### Simulation

\begin{align}
  \tau_m & \sim \text{N}^+\left(0, A_{\tau}^{(m)2}\right) \\
  \pmb{\beta}^{(m)} & = \pmb{A}^{-1} \pmb{v}
\end{align}
where
\begin{align}
  \pmb{A} & = \begin{bmatrix} \pmb{D} \\ J_m^{-1} \pmb{1}^{\top} \end{bmatrix} \\
  \pmb{v} & \sim \text{N}\left(\begin{bmatrix} \pmb{0} \\ 0 \end{bmatrix}, \begin{bmatrix} \tau_m^2 \pmb{I} & \pmb{0} \\ \pmb{0}^{\top} & A_{\eta}^{(m)2} \end{bmatrix} \right)
\end{align}
and $\pmb{D}$ is a first-order difference matrix of the form
\begin{equation}
  \begin{bmatrix} -1 & 1 & 0 & 0 \\
                  0 & -1 & 1 & 0 \\
		  0 & 0 & -1 & 1 \end{bmatrix}
\end{equation}


### Prediction

\begin{equation}
  \beta_{J_m+h+1}^{(m)} \sim \text{N}(\beta_{J_m+h}^{(m)}, \tau_m^2)
\end{equation}


### Code

```
RW(s = 1)
```

- `s` is $A_{\tau}^{(m)}$. Defaults to 1.
- Internally, $A_{\eta}^{(m)}$ is set to 0.001.


## Second-order random walk {#sec:pr-rw2}

### Model

\begin{equation}
  \beta_{j+2}^{(m)} \sim \text{N}(2 \beta_{j+1}^{(m)} - \beta_{j}^{(m)}, \tau_m^2),
\end{equation}
  $j = 1, \cdots, J_m - 2$, together with the soft constraints
\begin{align}
 \eta_0^{(m)} & \sim \text{N}(0, A_{\eta, 0}^{(m)2}) \\
 \eta_1^{(m)} & \sim \text{N}(0, A_{\eta, 1}^{(m)2})
\end{align}
where
\begin{align}
  \eta_0^{(m)} & = \frac{1}{J_m} \sum_{j=1}^{J_m} \beta_j^{(m)} \\
  \eta_1^{(m)} & = \frac{\sum_{j=1}^{J_m} \beta_j^{(m)} h_j^{(m)}}{\sum_{j=1}^{J_m} h_j^{(m)2}} \\
  h_j^{(m)} & = -\frac{J_m + 1}{J_m - 1} + \frac{2}{J_m - 1} j \\
\end{align}


### Contribution to posterior density

\begin{equation}
  \text{N}(\tau_m | 0, A_{\tau}^{(m)2}) \text{N}(\eta_0^{(m)} \mid 0, A_{\eta,0}^{(m)2}) \text{N}(\eta_1^{(m)} \mid 0, A_{\eta,1}^{(m)2}) \prod_{j=3}^{J_m} \text{N}\left(\beta_j^{(m)} - 2 \beta_{j-1}^{(m)} + \beta_{j-2}^{(m)} \mid 0, \tau_m^2 \right) 
\end{equation}


### Simulation

\begin{align}
  \tau_m & \sim \text{N}^+\left(0, A_{\tau}^{(m)2}\right) \\
  \pmb{\beta}^{(m)} & = \pmb{A}^{-1} \pmb{v}
\end{align}
where
\begin{align}
  \pmb{A} & = \begin{bmatrix} \pmb{R} \\ J_m^{-1} \pmb{1}^{\top} \\ \tilde{\pmb{h}}^{\top} \end{bmatrix} \\
  \pmb{v} & \sim \text{N}\left(\begin{bmatrix} \pmb{0} \\ 0 \\ 0 \end{bmatrix}, \begin{bmatrix} \tau_m^2 \pmb{I} & \pmb{0} & \pmb{0}\\ \pmb{0}^{\top} & A_{\eta_0}^{(m)2} & 0 \\
  \pmb{0}^{\top} & 0 & A_{\eta_1}^{(m)2} \end{bmatrix} \right) \\
\tilde{\pmb{h}} & = \left(\sum_{j=1}^{J_m} h_j^{(m)2} \right)^{-1} \begin{bmatrix} h_1^{(m)} \\ \vdots \\ h_{J_m}^{(m)} \end{bmatrix}
\end{align}
and $\pmb{R}$ is a second-order difference matrix of the form
\begin{equation}
  \begin{bmatrix} 1 & -2 & 1 & 0 & 0 \\
                  0 & 1 & -2 & 1 & 0 \\
		  0 & 0 & 1 & -2 & 1 \end{bmatrix}
\end{equation}

### Prediction

\begin{equation}
  \beta_{J_m+h+1}^{(m)} \sim \text{N}(2 \beta_{J_m + h}^{(m)} - \beta_{J_m + h -1}^{(m)}, \tau_m^2)
\end{equation}


### Code

```
RW2(s = 1, flat = FALSE)
```

- `s` is $A_{\tau}^{(m)}$. Defaults to 1.
- Internally, $A_{\eta,0}^{(m)}$ is set to 0.001.
- If `flat` is `FALSE` (the default) then, internally, $A_{\eta,1}^{(m)}$ is set to 1. Otherwise it is set to 0.001.


## AR1 {#sec:pr-ar1}

### Model

\begin{align}
  \beta_1^{(m)} & \sim \text{N}(0, \tau_m^2) \\
  \beta_j^{(m)} & = \phi_m \beta_{j-1}^{(m)} + \epsilon_j^{(m)}, \quad j = 2, \cdots, J_m \\
  \epsilon_j^{(m)} & \sim \text{N}\left(0,  (1 - \phi^2) \tau_m^2\right), \quad j = 2, \cdots, J_m \\
  \phi_m & = a_{0,m} + (a_{1,m} - a_{0,m}) \phi_m^{\prime} \\
  \phi_m^{\prime} & \sim \text{Beta}(2, 2) \\
  \tau_m & \sim \text{N}^+\left(0, A_{\tau}^{(m)2}\right).
\end{align}
This is adapted from the specification used for AR1 densities in [TMB](http://kaskr.github.io/adcomp/classdensity_1_1AR1__t.html). We require that $-1 < a_{0m} < a_{1m} < 1$.


### Contribution to posterior density

\begin{equation}
  \text{N}(\tau_m | 0, A_{\tau}^{(m)2}) \text{Beta}( \phi_m^{\prime} | 2, 2) \text{N}\left(\beta_1^{(m)} \mid 0, \tau_m^2 \right)  \prod_{j=2}^{J_m} \text{N}\left(\beta_j^{(m)} \mid \phi_m \beta_{j-1}^{(m)}, (1 - \phi_m^2) \tau_m^2 \right) 
\end{equation}


### Prediction

\begin{equation}
  \beta_{J_m + h + 1}^{(m)} \sim \text{N}\left(\phi_m \beta_{J_m + h}^{(m)}, (1 - \phi_m^2) \tau_m^2\right)
\end{equation}



### Code

```
AR1(min = 0.8, max = 0.98, s = 1)
```

- `min` is $a_{0m}$
- `max` is $a_{1m}$
- `s` is $A_{\tau}^{(m)}$. Defaults to 1.

The defaults for `min` and `max` are based on the defaults for function `ets()` in R package **forecast** [@hyndman2008automatic].


## Linear {#sec:pr-lin}

### Model

\begin{align}
  \beta_j^{(m)} & = h_j^{(m)} \eta_m + \epsilon_j^{(m)} \\
   h_j^{(m)} & = -\frac{J_m + 1}{J_m - 1} + \frac{2}{J_m - 1} j \\
  \eta_m & \sim \text{N}\left(0, A_{\eta}^{(m)2}\right) \\
  \epsilon_j^{(m)} & \sim \text{N}(0, \tau_m^2) \\
  \tau_m & \sim \text{N}^+\left(0, A_{\tau}^{(m)2}\right)
\end{align}


### Contribution to posterior density

\begin{equation}
  \text{N}(\tau_m | 0, A_{\tau}^{(m)2})   \text{N}(\eta_m | 0, A_{\eta}^{(m)2}) \prod_{j=1}^{J_m} \text{N}\left(\beta_j^{(m)} \mid h_j^{(m)} \eta_m, \tau_m^2 \right) 
\end{equation}


### Prediction

\begin{equation}
  \beta_{J_m + h + 1}^{(m)} \sim \text{N}(\beta_{J_m + h}^{(m)} + \tfrac{2}{J_m - 1}\eta_m, \tau_m^2)
\end{equation}


### Code

```
Lin(sd = NULL, s = 1)
```

- `sd` is $A_{\eta}^{(m)}$. Defaults to 1.
- `s` is $A_{\tau}^{(m)}$. Defaults to 1.


## P-Spline {#sec:pr-spline}

### Model

\begin{equation}
  \pmb{\beta}^{(m)} = \pmb{B}^{(m)} \pmb{\alpha}^{(m)}
\end{equation}
where $\pmb{B}^{(m)}$ is a $J_m \times K_m$ matrix of B-splines, and $\pmb{\alpha}^{(m)}$ has a second-order random walk prior (Section \@ref(sec:pr-rw2)).

$\pmb{B}^{(m)} = (\pmb{b}_1^{(m)}(\pmb{j}), \cdots, \pmb{b}_{K_m}^{(m)}(\pmb{j}))$, with $\pmb{j} = (1, \cdots, J_m)^{\top}$. The B-splines are centered, so that $\pmb{1}^{\top} \pmb{b}_k^{(m)}(\pmb{j}) = 0$, $k = 1, \cdots, K_m$.


### Contribution to posterior density

\begin{equation}
  \text{N}(\tau_m | 0, A_{\tau}^{(m)2}) \text{N}(\eta_0^{(m)} \mid 0, A_{\eta,0}^{(m)})  \text{N}(\eta_1^{(m)} \mid 0, A_{\eta,1}^{(m)}) \prod_{k=3}^{K_m} \text{N}\left(\alpha_k^{(m)} - 2 \alpha_{k-1}^{(m)} + \alpha_{k-2}^{(m)} \mid 0, \tau_m^2 \right) 
\end{equation}
where
\begin{align}
  \eta_0^{(m)} & = \frac{1}{K_m} \sum_{k=1}^{K_m} \alpha_k^{(m)} \\
  \eta_1^{(m)} & = \frac{\sum_{k=1}^{K_m} \alpha_k^{(m)} h_k^{(m)}}{\sum_{k=1}^{K_m} h_k^{(m)2}} \\
  h_k^{(m)} & = -\frac{K_m + 1}{K_m - 1} + \frac{2}{K_m - 1} j \\
\end{align}


### Prediction

Main effects with a P-Spline prior cannot be predicted.


### Code

```
Spline(n = NULL, s = 1)
```

- `n` is $K_m$. Defaults to $\max(0.7 J_m, 4)$. *This is the current default for BayesRates. Could we use a smaller multiplier?*
- `s` is the $A_{\tau}^{(m)}$ from the second-order random walk prior. Defaults to 1.


## SVD {#sec:pr-svd}

### Model

\begin{align}
  \pmb{\beta}^{(m)} & = \pmb{F}^{(m)} \pmb{\alpha}^{(m)} + \pmb{g}^{(m)} \\
  \alpha_k^{(m)} & \sim \text{N}(0, 1), \quad k^{(m)} = 1, \cdots, K^{(m)}
\end{align}
where $\pmb{F}^{(m)}$ is a $J_m \times K_m$ matrix, and $\pmb{g}^{(m)}$ is a vector with $J_m$ elements, both derived from a singular value decomposition (SVD) of an external dataset of age-specific values. The construction of $\pmb{F}^{(m)}$ and $\pmb{g}^{(m)}$ is described in Appendix \@ref(app:svd). The construction involves centering and scaling, which allows us to use a simple prior for $\pmb{\alpha}^{(m)}$.

TODO - Describe age-sex interactions.

### Contribution to posterior density

\begin{equation}
  \prod_{k=1}^{K_m} \text{N}\left(\alpha_k^{(m)} \mid 0, 1 \right) 
\end{equation}

### Prediction

Main effects with a SVD prior cannot be predicted.


### Code

```
SVD(scaled_svd, n = 5, indep = TRUE)
```

## Known {#sec:pr-known}

### Model

Elements of $\pmb{\beta}^{(m)}$ are treated as known with certainty.

### Contribution to posterior density

Known priors make no contribution to the posterior density.

### Prediction

Main effects with a known prior cannot be predicted.


### Code

```
Known(values)
```

- `values` is a vector containing the $\beta_j^{(m)}$.


# Priors for intercept {#sec:pr-int}

The intercept can be given an fixed-normal prior (Section \@ref(sec:pr-fnorm)) or a Known prior (Section \@ref(sec:pr-known)). If a fixed-norm prior is used, the default value for scale $A_{\beta}^{(0)}$ is 10 rather than 1.


# Priors for interactions {#sec:pr-interact}

*Not written yet*


# Cyclical effects {#sec:pr-cyclical}


approximate abs function: https://math.stackexchange.com/questions/728094/approximate-x-with-a-smooth-function?noredirect=1&lq=1


approximate error function: https://stats.stackexchange.com/questions/7200/evaluate-definite-interval-of-normal-distribution/7206#7206




"A cyclic pattern exists when data exhibit rises and falls that are not of fixed period", as opposed to a seasonal pattern that has a fixed period [Hyndman](https://robjhyndman.com/hyndsight/cyclicts/).


## Model

Cyclical effects can only be used with a time main effect. The prior for cyclical effects is an AR model, with the number of terms, $K_m$, chosen by the user,

\begin{equation}
  \chi_j \sim \text{N}\left(\phi_1^{(\chi)} \chi_{j-1} + \cdots + \phi_{K_m}^{(\chi)} \chi_{j-{K_m}},  \omega_{\chi}^2\right), \quad j = K_m + 1, \cdots, J_m.
\end{equation}
Internally, TMB derives a value for $\omega_{\chi}$ that gives every term $\chi_j$ the same marginal variance. We denote this marginal variance $\tau_{\chi}^2$, and assign it a prior
\begin{equation}
  \tau_{\chi}  \sim \text{N}^+(0, A_{\chi}^2).
\end{equation}
Each of the $\phi_k^{(\chi)}$ is restricted to the interval $(-1, 1)$. We assign each $\phi_k^{(\chi)}$ the prior
\begin{align}
  \phi_k^{(\chi)\prime} & \sim \text{Beta}(2, 2) \\
  \phi_k^{(\chi)} & =  2 \phi_k^{(\chi)\prime} - 1.
\end{align}


## Contribution to posterior density

\begin{equation}
  \text{N}^+\left(\tau_{\chi} | 0, A_{\chi}^2 \right) \prod_{k=1}^{K_m} \text{Beta}\left( \phi_k^{(\chi)\prime} | 2, 2 \right) p\left( \chi_1, \cdots, \chi_{J_m} | \phi_1^{(\chi)}, \cdots, \phi_{K_m}^{(\chi)}, \tau_{\chi} \right) 
\end{equation}
where $p\left( \chi_1, \cdots, \chi_{J_m} | \phi_1^{(\chi)}, \cdots, \phi_{K_m}^{(\chi)}, \tau_{\chi} \right)$ is calculated internally by TMB.

## Prediction

\begin{equation}
  \chi_{J_m + h } \sim \text{N}\left(\phi_1^{(\chi)} \chi_{J_m + h - 1} + \cdots + \phi_{K_m}^{(\chi)} \chi_{J_m + h - {K_m}}, \tau_{\chi}^2\right)
\end{equation}


## Code

```
set_cyclical(x, n = 3, s = 1)
```

- `x` is the model object
- `n` is $K_m$
- `s` is $A_{\chi}$



# Seasonal effects {#sec:pr-season}

## Model

Seasonal effects are applied to a single main effect or interaction $\pmb{\beta}^{(m)}$. They are modelled by a vector $\pmb{\lambda}$ with $J_m$ elements. Seasonal effects are divided into $S$ batches, corresponding to $S$ seasons, where $2 \le S \le \tfrac{1}{2} J_m$. Each batch of seasonal effects follows a random walk, and there is a soft constrain on the sum across all batches.

\begin{align}
  \lambda_j & \sim \text{N}(\lambda_{j - S}, \tau_{\lambda}^2 ) \\ 
  \tau_{\lambda} & \sim \text{N}^+\left(0, A_{\lambda}^2 \right) \\
  \sum_{j=1}^{J_m} \lambda_j & \sim \text{N}(0, 1)
\end{align}


## Contribution to posterior density

\begin{equation}
  \text{N}(\tau_{\lambda} | 0, A_{\lambda}^2) \text{N}\left( {\textstyle\sum}_{j=1}^{J_m} \lambda_j \mid 0, 1 \right)  \prod_{j=2}^{J_m} \text{N}\left(\lambda_j - \lambda_{j-1} \mid 0, \tau_{\lambda}^2 \right) 
\end{equation}

## Prediction

TODO - write

## Code

```
set_season(x, n, s = 1)
```

- `x` is the model object
- `n` is $S$
- `s` is $A_{\lambda}$


# Covariates {#sec:pr-cov}

## Model

The columns of matrix $\pmb{Z}$ are assumed to be standardised to have mean 0 and standard deviation 1.  $\pmb{Z}$ does not contain a column for an intercept.

We implement two priors for coefficient vector $\pmb{\zeta}$. The first prior is designed for the case where $P$, the number of colums of $\pmb{Z}$, is small, and most $\zeta_p$ are likely to distinguishable from zero. The second prior is designed for the case where $P$ is large, and only a few $\zeta_p$ are likely to be distinguishable from zero.


### Standard prior

\begin{align}
  \zeta_p | \varphi & \ind \text{N}(0, \varphi^2) \\
  \varphi & \sim \text{N}^+(0, 1)
\end{align}


### Shrinkage prior

Regularised horseshoe prior [@piironen2017hyperprior]

\begin{align}
  \zeta_p | \omega_p, \varphi & \ind \text{N}(0, \omega_p^2 \varphi^2) \\
  \omega_p & \sim \text{Cauchy}^+(0, 1) \\
  \varphi & \sim \text{Cauchy}^+(0, A_{\varphi}^2) \\
  A_{\varphi} & = \frac{p_0}{p_0 + P} \frac{\hat{\sigma}}{\sqrt{n}}
\end{align}
where $p_0$ is an initial guess at the number of $\zeta_p$ that are non-zero, and $\hat{\sigma}$ is obtained as follows:

- Poisson. Using maximum likelihood, fit the GLM
\begin{align}
  y_i & \sim \text{Poisson}(w_i \gamma_i) \\
  \log \gamma_i & = \sum_{m=0}^{M} \pmb{X}^{(m)} \pmb{\beta}^{(m)},
\end{align}
and set
\begin{equation}
  \hat{\sigma} = \frac{1}{n}\sum_{i=1}^n \frac{1}{w_i \hat{\gamma}_i}.
\end{equation}
- Binomial. Using maximum likelihood, fit the GLM
\begin{align}
  y_i & \sim \text{binomial}(w_i, \gamma_i) \\
  \text{logit} \gamma_i & = \sum_{m=0}^{M} \pmb{X}^{(m)} \pmb{\beta}^{(m)},
\end{align}
and set
\begin{equation}
  \hat{\sigma} = \frac{1}{n}\sum_{i=1}^n \frac{1}{w_i \hat{\gamma}_i (1 - \hat{\gamma}_i)}.
\end{equation}
- Normal. Using maximum likelihood, fit the linear model
\begin{equation}
  y_i \sim \text{N}\left(\sum_{m=0}^{M} \pmb{X}^{(m)} \pmb{\beta}^{(m)}, w_i^{-1}\xi^2 \right)
\end{equation}
and set $\hat{\sigma} = \hat{\xi}$.


The quantities used for Poisson and binomial likelihoods are derived from normal approximations to GLMs [@piironen2017hyperprior; @gelman2014bayesian, section 16.2].

## Contribution to posterior density

TODO - write

## Prediction

TODO - write

## Code

```
set_covariates(formula, data = NULL, n_coef = NULL)
```

- `formula` is a one-sided R formula describing the covariates to be used
- `data` A data frame. If a value for `data` is supplied, then `formula` is interpreted in the context of this data frame. If a value for `data` is not supplied, then `formula` is interpreted in the context of the data frame used for the original call to `mod_pois()`, `mod_binom()`, or `mod_norm()`.
- `n_coef` is the effective number of non-zero coefficients. If a value is supplied, the shrinkage prior is used; otherwise the standard prior is used.


Examples:
```
set_covariates(~ mean_income + distance * employment)
set_covariates(~ ., data = cov_data, n_coef = 5)
```


# Priors for dispersion terms

## Model

### Poisson

\begin{equation}
  p(\xi) = \frac{A_{\xi}}{2 \sqrt{\xi}}e^{-A_{\xi} \sqrt{\xi}} (\#eq:prior-xi-pois)
\end{equation}
[@simpson2022priors]

TODO - choice of $A_{\xi}$.

To draw from $p(\xi)$, we use inverse sampling. We have
\begin{equation}
  F(\xi) = \int_0^{\xi} \frac{A_{\xi}}{2 \sqrt{\xi}} e^{-A_{\xi} \sqrt{\xi}} = 1 - e^{-A_{\xi} \sqrt{\xi}}.
\end{equation}

Inverting gives
\begin{equation}
  F^{-1}(p) = \left(\frac{\log(1 - p)}{A}\right)^2 (\#eq:prior-xi-inv).
\end{equation}

Drawing $u \sim \text{Unif}(0, 1)$ and plugging the value into \@ref(eq:prior-xi-inv) yields variates with the required distribution.


### Binomial

It seems sensible to use the same prior for $\xi$ as we do with the Poisson likelihood, since in both cases, the $\xi^{-1}$ is the number of prior cases. But we need to verify this. I started in Appendix \@ref(app:disp), but got stuck.


### Normal

\begin{equation}
  \xi \sim \text{Exp}(A_{\xi})
\end{equation}

[@simpson2022priors]

## Code

```
set_disp(s = 1)
```

- scale is $A_{\xi}$


# Deriving outputs

Running TMB yields a set of means $\pmb{m}$, and a precision matrix $\pmb{Q}^{-1}$, which together define the approximate joint posterior distribution of

- $\pmb{\beta}^{(m)}$ for terms with independent normal, fixed normal, multivariate normal, random walk, second-order random walk, AR1, Linear, and Linear-AR1 priors,
- $\pmb{\alpha}$ for terms with Spline and SVD priors,
- hyper-parameters for $\pmb{\beta}^{(m)}$ and $\pmb{\alpha}^{(m)}$ typically transformed to another scale, such as a log scale,
- dispersion term $\xi$, and
- seasonal effects $\pmb{\lambda}$, together with associated hyper-parameters $\tau_{\lambda}$ (on a log scale).

We use $\tilde{\pmb{\theta}}$ to denote a vector containing all these quantities.

We perform a Cholesky decomposition of $\pmb{Q}^{-1}$, to obtain $\pmb{R}$ such that
\begin{equation}
  \pmb{R}^{\top} \pmb{R} = \pmb{Q}^{-1}
\end{equation}
We store $\pmb{R}$ as part of the model object.

We draw generate values for $\tilde{\pmb{\theta}}$ by generating a vector of standard normal variates $\pmb{z}$, back-solving the equation
\begin{equation}
  \pmb{R} \pmb{v} = \pmb{z}
\end{equation}
and setting
\begin{equation}
  \tilde{\pmb{\theta}} = \pmb{v} + \pmb{m}.
\end{equation}

Next we convert any transformed hyper-parameters back to the original units, and insert values for $\pmb{\beta}^{(m)}$ for terms that have Known priors. We denote the resulting vector $\pmb{\theta}$.

Finally we draw from the distribution of $\pmb{\gamma} \mid \pmb{y}, \pmb{\theta}$ using the methods described in Section \@ref(sec:lik).



# Simulation

To generate one set of simulated values, we start with values for exposure, trials, or weights,  $\pmb{w}$, and possibly covariates $\pmb{Z}, then go through the following steps:

1. Draw values for any parameters in the priors for the $\pmb{\beta}^{(m)}$, $m = 1, \cdots, M$.
1. Conditional on the values drawn in Step 1, draw values the $\pmb{\beta}^{(m)}$, $m = 0, \cdots, M$.
1. If the model contains seasonal effects, draw the standard deviation $\kappa_m$, and then the effects $\pmb{\lambda}^{(m)}$.
1. If the model contains covariates, draw $\varphi$ and $\omega_p$ where necessary, draw coefficient vector $\pmb{\zeta}$.
1. Use values from steps 2--4 to form the linear predictor $\sum_{m=0}^{M} \pmb{X}^{(m)} (\pmb{\beta}^{(m)} + \pmb{\lambda}^{(m)}) + \pmb{Z} \pmb{\zeta}$.
1. Back-transform the linear predictor, to obtain vector of cell-specific parameters $\pmb{\mu}$.
1. If the model contains a dispersion parameter $\xi$, draw values from the prior for $\xi$.
1. In Poisson and binomial models, use $\pmb{\mu}$ and, if present, $\xi$ to draw $\pmb{\gamma}$.
1. In Poisson and binomial models, use $\pmb{\gamma}$ and $\pmb{w}$ to draw $\pmb{y}$; in normal models, use $\pmb{\mu}$, $\xi$, and $\pmb{w}$ to draw $\pmb{y}$.


# Replicate data

## Model

### Poisson likelihood

#### Condition on $\pmb{\gamma}$

\begin{equation}
  y_i^{\text{rep}} \sim \text{Poisson}(\gamma_i w_i)
\end{equation}

#### Condition on $(\pmb{\mu}, \xi)$

\begin{align}
  y_i^{\text{rep}} & \sim \text{Poisson}(\gamma_i^{\text{rep}} w_i) \\
  \gamma_i^{\text{rep}} & \sim \text{Gamma}(\xi^{-1}, (\xi \mu_i)^{-1})
\end{align}
which is equivalent to
\begin{equation}
  y_i^{\text{rep}} \sim \text{NegBinom}\left(\xi^{-1}, (1 + \mu_i w_i \xi)^{-1}\right)
\end{equation}

### Binomial likelihood

#### Condition on $\pmb{\gamma}$

\begin{equation}
  y_i^{\text{rep}} \sim \text{Binomial}(w_i, \gamma_i)
\end{equation}

#### Condition on $(\pmb{\mu}, \xi)$

\begin{align}
  y_i^{\text{rep}} & \sim \text{Binomial}(w_im \gamma_i^{\text{rep}}) \\
  \gamma_i^{\text{rep}} & \sim \text{Beta}\left(\xi^{-1} \mu_i, \xi^{-1}(1 - \mu_i)\right)
\end{align}

### Normal likelihood

\begin{equation}
  y_i^{\text{rep}} \sim \text{N}(\gamma_i, \xi^2 / w_i)
\end{equation}

## Code

```
replicate_data(x, condition_on = c("fitted", "expected"), n = 20)
```

# Appendices

## Definitions {#app:defn}


| Quantity | Definition                                          |
|:---------|:----------------------------------------------------|
| $i$      | Index for cell, $i = 1, \cdots, n$.
| $y_i$    | Value for outcome variable.              |
| $w_i$    | Exposure, number of trials, or weight. |
| $\gamma_i$ | Super-population rate, probability, or mean. |
| $\mu_i$ | Cell-specific mean. |
| $\xi$   | Dispersion parameter. |
| $g()$ | Log, logit, or identity function. |
| $m$ | Index for intercept, main effect, or interaction. $m = 0, \cdots, M$. |
| $\beta^{(0)}$ | Intercept. |
| $\pmb{\beta}^{(m)}$ | Main effect or interaction. $m = 1, \cdots, M$.  |
| $\beta_j^{(m)}$ | $j$th element of $\pmb{\beta}^{(m)}$. $j = 1, \cdots, J_m$. |
| $\pmb{X}^{(m)}$ | Matrix mapping $\pmb{\beta}^{(m)}$ to $\pmb{y}$. |
| $\pmb{Z}$ | Matrix of covariates. |
| $\pmb{\zeta}$ | Parameter vector for covariates $\pmb{Z}^{(m)}$. |
| $A_0$ | Scale parameter in prior for intercept $\beta^{(0)}$. |
| $\tau_m$ | Standard deviation parameter for main effect or interaction. |
| $A_{\tau}^{(m)}$ | Scale parameter in prior for $\tau_m$. |
| $\pmb{\alpha}^{(m)}$ | Parameter vector for P-spline and SVD priors. |
| $\alpha_k^{(m)}$ | $k$th element of $\pmb{\alpha}^{(m)}$. $k = 1, \cdots, K_m$. |
| $\pmb{V}^{(m)}$ | Covariance matrix for multivariate normal prior. |
| $h_j^{(m)}$ | Linear covariate |
| $\eta^{(m)}$ | Parameter specific to $m$th main effect or interaction. |
| $A_{\eta}^{(m)}$ | Standard deviation in normal prior for $\eta_m$. |
| $\phi_m$ | Correlation coefficient in AR1 densities. |
| $a_{0m}$, $a_{1m}$ | Minimum and maximum values for $\phi_m$. |
| $\pmb{B}^{(m)}$ | B-spline matrix in P-spline prior. |
| $\pmb{b}_k^{(m)}$ | B-spline. $k = 1, \cdots, K_m$. |
| $\pmb{F}^{(m)}$ | Matrix in SVD prior. |
| $\pmb{g}^{(m)}$ | Offset in SVD prior. |
| $\pmb{\chi}$ | Cyclical effect. |
| $\phi_1^{(\chi)}$ | Corelation coefficient in model for cyclical effect |
| $\phi_2^{(\chi)}$ | Corelation coefficient in model for cyclical effect |
| $\tau_{\chi}$ | Standard deviation parameter for cyclical effect. |
| $\pmb{\lambda}$ | Seasonal effect. |
| $\tau_{\lambda}$ | Standard deviation parameter for seasonal effect. |
| $\varphi$ | Global shrinkage parameter in shrinkage prior. |
| $A_{\varphi}$ | Scale term in prior for $\varphi$. |
| $\omega_p$ | Local shrinkage parameter in shrinkage prior. |
| $p_0$ | Expected number of non-zero coefficients in $\pmb{\zeta}$. |
| $\hat{\sigma}$ | Empirical scale estimate in prior for $\varphi$. |
| $\pi$ | Vector of hyper-parameters |

 
## SVD prior for age  {#app:svd}

Let $\pmb{A}$ be a matrix of age-specific estimates from an international database, transformed to take values in the range $(-\infty, \infty)$. Each column of $\pmb{A}$ represesents one set of age-specific estimates, such as log mortality rates in Japan in 2010, or logit labour participation rates in Germany in 1980.

Let $\pmb{U}$, $\pmb{D}$, $\pmb{V}$ be the matrices from a singular value decomposition of $\pmb{A}$, where we have retained the first $K$ components. Then
\begin{equation}
  \pmb{A} \approx \pmb{U} \pmb{D} \pmb{V}. (\#eq:svd1)
\end{equation}

Let $m_k$ and $s_k$ be the mean and sample standard deviation of the elements of the $k$th row of $\pmb{V}$, with $\pmb{m} = (m_1, \cdots, m_k)^{\top}$ and $\pmb{s} = (s_1, \cdots, s_k)^{\top}$. Then
\begin{equation}
  \tilde{\pmb{V}} = (\text{diag}(\pmb{s}))^{-1} (\pmb{V} - \pmb{m} \pmb{1}^{\top})
\end{equation}
is a standardized version of $\pmb{V}$.

We can rewrite \@ref(eq:svd1) as
\begin{align}
  \pmb{A} & \approx \pmb{U} \pmb{D} (\text{diag}(\pmb{s}) \tilde{\pmb{V}} + \pmb{m} \pmb{1}^{\top}) \\
  & = \pmb{F} \tilde{\pmb{V}} + \pmb{m} \pmb{1}^{\top}, (\#eq:svd2)
\end{align}
where $\pmb{F} = \pmb{U} \pmb{D} \text{diag}(\pmb{s})$ and $\pmb{m} = \pmb{U} \pmb{D} \pmb{m}$.

Let $\tilde{\pmb{v}}_l$ be a randomly-selected column from $\tilde{\pmb{V}}$. From the construction of $\tilde{\pmb{V}}$, and the orthogonality of the rows of $\pmb{V}$, we have $\text{E}[\tilde{\pmb{v}}_l] = \pmb{0}$ and $\text{var}[\tilde{\pmb{v}}_l] = \pmb{I}$. This implies that if $\pmb{z}$ is a vector of standard normal variables, then
\begin{equation}
  \pmb{F} \pmb{z} + \pmb{g}
\end{equation}
looks approximately like a randomly-selected column from the original data matrix $\pmb{A}$.


## Derivation of prior for dispersion term in binomial model {#app:disp}


The Kullback-Liebler divergence $\text{KL}(f_1 || f_0)$ of two beta distributions is

\begin{equation}
  \log \frac{\Gamma(\alpha_1 + \beta_1) \Gamma(\alpha_0) \Gamma(\beta_0)}{\Gamma(\alpha_0 + \beta_0) \Gamma(\alpha_1) \Gamma(\beta_1)}  + (\alpha_1 - \alpha_0) \left(\psi(\alpha_1) - \psi(\alpha_1 + \beta_1)\right)
  + (\beta_1 - \beta_0) \left(\psi(\beta_1) - \psi(\alpha_1 + \beta_1)\right) (\#eq:kl-beta) 
\end{equation}
https://math.stackexchange.com/questions/257821/kullback-liebler-divergence#comment564291_257821

Using the parameterisation $v = 1 / \xi = \alpha + \beta$, $\mu = \alpha / (\alpha + \beta)$, this becomes
\begin{equation}
  \log \frac{\Gamma(v_1) \Gamma(\mu v_0) \Gamma((1 - \mu) v_0)}{\Gamma(v_0) \Gamma(\mu v_1) \Gamma((1 - \mu) v_1)} + (v_1 - v_0) \mu (\psi(\mu v_1) - \psi(v_1)) + (v_1 - v_0) (1 - \mu) (\psi((1-\mu) v_1) - \psi(v_1))
\end{equation}



# References



% ## Multivariate normal {#sec:pr-mnorm}


% ### Model

% \begin{align}
%   \pmb{\beta}^{(m)} & \sim \text{N}\left(\pmb{0},  \tau_m^2 \pmb{V}^{(m)} \right) \\
%   \tau_m & \sim \text{N}^+\left(0, A_{\tau}^{(m)2}\right)
% \end{align}
% where $\pmb{V}^{(m)}$ is a $J_m \times J_m$ symmetric positive-definite matrix supplied by the user.

% ### Contribution to posterior density

% \begin{equation}
%   \text{N}(\tau_m | 0, A_{\tau}^{(m)2}) \text{N}\left(\pmb{\beta}^{(m)} \mid \pmb{0},  \tau_m^2 \pmb{V}^{(m)} \right)
% \end{equation}

% ### Simulation

% \begin{align}
%   \tau_m & \sim \text{N}^+\left(0, A_{\tau}^{(m)2}\right) \\
%   \pmb{\beta}^{(m)} & \sim \text{N}\left(\pmb{0},  \tau_m^2 \pmb{V}^{(m)} \right)
% \end{align}


% ### Prediction

% Main effects with a multivariate normal prior cannot be predicted.


% ### Code

% ```
% MVN(V, s = 1)
% ```

% - `V` is $\pmb{V}^{(m)}$
% - `s` is $A_{\tau}^{(m)}$. Defaults to 1.
