---
title: "Mathematical description of models used in bage"
output:
  bookdown::html_document2:
    base_format: rmarkdown::html_vignette
    fig_caption: yes
    toc: true
    toc_depth: 2
    number_sections: true
pkgdown:
  as_is: true
vignette: >
  %\VignetteIndexEntry{Mathematical description of models used in bage}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
bibliography: references.bib
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

$\newcommand{\ind}{\, \overset{\text{ind}}{\sim} \,}$


```{r setup}
library(bage)
```

# Introduction

Specification document. Aims to give complete mathematical description of models used by bage.

Note: this is describing the models we want to get to soon, not the ones we currently implement.

# Likelihood

## Poisson

Let $y_i$ be a count of events in cell $i = 1, \cdots, n$ and let $w_i$ be the corresponding exposure measure, with the possibility that $w_i \equiv 1$. The likelihood under the Poisson model is then
\begin{align}
  y_i \mid \gamma_i, w_i & \ind \text{Poisson}(\gamma_i w_i) (\#eq:lik-pois-1) \\
  \gamma_i \mid \mu_i, \xi & \ind \text{Gamma}\left(\xi^{-1}, (\mu_i \xi)^{-1}\right),  (\#eq:lik-pois-2)
\end{align}
where the Gamma distribution has a shape-rate parameterisation. Parameter $\xi$ governs dispersion, with
\begin{equation}
  \text{var}(\gamma_i \mid \mu_i, \xi) = \xi \mu_i^2
\end{equation}
and
\begin{equation}
  \text{var}(y_i \mid \mu_i, \xi, w_i) = (1 + \xi \mu_i w_i ) \times \mu_i w_i.
\end{equation}
We allow $\xi$ to equal 0, in which case the model reduces to 
\begin{equation}
  y_i \mid \mu_i, w_i \ind \text{Poisson}(\mu_i w_i).
\end{equation}

For $\xi > 0$, Equations \@ref(eq:lik-pois-1) and \@ref(eq:lik-pois-2) are equivalent to
\begin{equation}
  y_i \sim \text{NegBinom}\left(\xi^{-1}, (1 + \mu_i w_i \xi)^{-1}\right)
\end{equation}
[@norton2018sampling; @simpson2022priors].
This is the format we use internally, for estimation. When values for $\gamma_i$ are needed, we generate them on the fly, using the fact that
\begin{equation}
  \gamma_i \mid y_i, \mu_i, \xi \ind \text{Gamma}\left(y_i + \xi^{-1}, w_i + (\xi \mu_i)^{-1}\right).
\end{equation}


## Binomial

The likelihood under the binomial model is
\begin{align}
  y_i \mid \gamma_i, w_i & \ind \text{Binomial}(w_i, \gamma_i) (\#eq:lik-binom-1) \\
  \gamma_i \mid \mu_i, \xi & \ind \text{Beta}\left(\xi^{-1} \mu_i, \xi^{-1}(1 - \mu_i)\right). (\#eq:lik-binom-2)
\end{align}
Parameter $\xi$ again governs dispersion, with
\begin{equation}
  \text{var}(\gamma_i \mid \mu_i, \xi) =  \frac{\xi}{1 + \xi} \times \mu_i (1 -\mu_i)
\end{equation}
and
\begin{equation}
  \text{var}(y_i \mid w_i, \mu_i, \xi) =  \frac{\xi w_i + 1}{\xi + 1} \times w_i \mu_i (1 - \mu_i).
\end{equation}

We allow $\xi$ to equal 0, in which case the model reduces to 
\begin{equation}
  y_i \mid w_i, \mu_i \ind \text{Binom}(w_i, \mu_i).
\end{equation}
For $\xi > 0$, Equations \@ref(eq:lik-binom-1) and \@ref(eq:lik-binom-1) are equivalent to
\begin{equation}
  y_i | w_i, \mu_i, \xi \ind \text{BetaBinom}\left(w_i, (1-\xi)\xi^{-1}\mu_i, (1-\xi)\xi^{-1}(1 - \mu_i)\right),
\end{equation}
which is what we use internally. Values for $\gamma_i$ can be generated using
\begin{equation}
  \gamma_i \mid y_i, w_i, \mu_i, \xi \sim \text{Beta}\left(y_i + (1-\xi)\xi^{-1}\mu_i, w_i - y_i + (1-\xi)\xi^{-1}(1-\mu_i)\right).
\end{equation}


## Normal

\begin{equation}
  y_i \sim \text{N}(\mu_i, w_i^{-1}\xi^2).
\end{equation}

## Log-Normal

\begin{equation}
  y_i \sim \text{LogNorm}(\mu_i, w_i^{-1}\xi^2).
\end{equation}


# Model for means

Our model for means is
\begin{equation}
  g(\pmb{\mu}) = \sum_{m=0}^{M} \pmb{X}^{(m)} \pmb{\beta}^{(m)} + \pmb{Z} \pmb{\zeta} + \pmb{o} (\#eq:means)
\end{equation}
where

- $g$ is log in Poisson models, logit in binomial models, and identity in normal and log-normal models;
- $\beta^{(0)}$ is an intercept;
- $\pmb{\beta}^{(m)}$, $m=1,\cdots,M$ is a main effect or interaction with $J_m$ elements;
- $\pmb{X}^{(m)}$ is an $n \times J_m$ matrix of 1s and 0s, the $i$th row of which picks out the element of $\pmb{\beta}^{(m)}$ that is used with cell $i$;
- $\pmb{Z}$ is a $n \times P$ matrix of covariates;
- $\pmb{\zeta}$ is a vector with $P$ elements; and
- $\pmb{o}$ is an offset containing known values.

Priors for the $\pmb{\beta}^{(m)}$ are described in sections \@ref(sec:pr-int)--\@ref(sec:pr-interact). Covariate matrix $\pmb{Z}$ and the prior for vector $\pmb{\zeta}$ are discussed in Section \@ref(â€­sec:cov).

# Scale term

In many places in the model, a parameter $s$ is used as a default value for scale. The value for $s$ depends on the model likelihood:
\begin{equation}
  s = \begin{cases} 1 & \text{ with Poisson, binomial, or log-normal likelihoods} \\
  \sqrt{\sum_{i=1}^n (y_i - \bar{y})^2/(n-1)} & \text{ with normal likelihoods.} \end{cases}  (\#eq:defn-s)
\end{equation}



# Prior for intercept {#sec:pr-int}

\begin{equation}
  \beta^{(0)} \sim \text{N}(0, A_0^2)
\end{equation}

By default, $A_0 = 10s$, with $s$ defined in Equation \@ref(eq:defn-s).

# Overall design of priors for main effects {#sec:pr-main-over}

## Model

In all priors for  main effects, we require, for identifiability, that $\text{E}\left(\sum_{j=1}^{J_m} \beta_j^{(m)}\right) = 0$. In the case of random walk priors, the easiest way to implement this requirement is to impose the stronger condition that $\sum_{j=1}^{J_m} \beta_j^{(m)} = 0$.

With the exception of the SVD prior (Section \@ref(sec:pr-svd)), we default variances in a way that is consistent across priors. The priors all have the form
\begin{equation}
  \text{<value>} = \text{<optional linear trend>} + \text{<error>}.
\end{equation}
We treat the independent normal prior (described in Section \@ref(sec:pr-inorm)) as a baseline. In all priors, after removing any linear trend, the average value for the marginal variance of the $\beta_j^{(m)}$ equals the marginal variance of the $\beta_j^{(m)}$ in the independent normal prior:
\begin{equation}
 \frac{1}{J_m} \sum_{j=1}^{J_m} \text{var}(\beta_j^{(m)}) = \tau_m^2. (\#eq:constraint-var)
\end{equation}
We assign a half-normal prior to $\tau_m$,
\begin{equation}
  \tau_m \sim \text{N}^+(A_m^2).
\end{equation}
The default for $A_m$ is $s$, as defined in Equation \@ref(eq:defn-s).

## Standard format

All priors can be put into the format
\begin{align}
  \pmb{\beta}^{(m)} & = \pmb{H}^{(m)} \pmb{x}^{(m)} \\
  x_k^{(m)} & \sim \text{N}(0, \sigma_k^{(m)}), \quan k = 1, \dots, K_m
\end{align}

Matrix $\pmb{H}^{(m)}$ is fixed and known, except in the case of AR1 priors and linear priors with AR1 errors (sections \@ref(sec:pr-ar1) and \@ref(sec:pr-lin-ar1)), where it depends on correlation coefficient $\phi$.

Vector $\pmb{d}^{(m)}$ depends on least one unknown parameter, such as $\tau_m$, except in the case of the SVD prior (Section \@ref(sec:pr-svd)).


## Code

`set_prior_effect(<effect> ~ <fun>)`

eg

`set_prior_effect(region ~ N())`

`set_prior_effect(time ~ Lin(scale = 0.2))`



# Priors for main effects {#sec:pr-main-specific}

## Independent normal {#sec:pr-inorm}

### Model

\begin{align}
  \beta_j^{(m)} | \tau^2 & \ind \text{N}\left(0, \tau_m^2 \right) \\
  \tau_m & \sim \text{N}^+(A_m^2)
\end{align}

### Standard format

\begin{align}
  \pmb{H}^{(m)} & = \pmb{I}_{J_m} \\
  \pmb{Q}^{(m)} & = \tau_m^{-2} \pmb{I}_{J_m}
\end{align}

### Code

`N(scale = NULL)`

- `scale` is $A_m$. Defaults to $s$, as defined in Equation \@ref(eq:defn-s), if user does not supply a value.

## Multivariate normal {#sec:pr-cnorm}

### Model

\begin{align}
  \beta^{(m)} & \sim \text{N}\left(\pmb{0}, k_m \tau_m^2 \pmb{V}^{(m)} \right) \\
  \tau_m & \sim \text{N}^+(A_m^2)
\end{align}
where $\pmb{V}^{(m)}$ is a $J_m \times J_m$ symmetric positive-definite matrix supplied by the user, and $k_m = J_m  \left(\text{tr}(\pmb{V}^{(m)})\right)^{-1}$.


### Standard format

\begin{align}
  \pmb{H}^{(m)} & =  \pmb{L}^{(m)} \\
  \pmb{Q}^{(m)} & = k_m^{-1} \tau_m^{-2} I_{J_m}
\end{align}
where $\pmb{L}^{(m)}$ is the lower triangular matrix obtained from a Cholesky decomposition of $\pmb{V}^{(m)}$.

### Code

`MVN(V, scale = NULL)`

- `V` is $\pmb{V}^{(m)}$
- `scale` is $A_m$. Defaults to $s$, as defined in Equation \@ref(eq:defn-s), if user does not supply a value.


## First-order random walk {#sec:pr-rw}

### Model

\begin{align}
  \pmb{\beta}^{(m)} & = \pmb{D}_m^{\top} (\pmb{D}_m \pmb{D}_m^{\top})^{-1} \pmb{\epsilon}^{(m)} \\
  \pmb{\epsilon}^{(m)} & \sim \text{N}\left(\pmb{0}, k_m \tau_m^2 \pmb{I}_{J_m - 1}\right) \\ (\#eq:pr-rw-eps)
  \tau_m & \sim \text{N}^+(A_m^2)
\end{align}
where $\pmb{D}_m$ is a $(J_m - 1) \times J_m$ first-order difference matrix of the form
\begin{equation}
  \begin{bmatrix} -1 & 1 & 0 & 0 \\
                  0 & -1 & 1 & 0 \\
		  0 & 0 & -1 & 1 \end{bmatrix},
\end{equation}
and $\pmb{I}_{J_m - 1}$ is a $(J_m - 1) \times (J_m - 1)$ identity matrix.

Note that $\pmb{D}_m \pmb{\beta}^{(m)} =  \pmb{D}_m \pmb{D}_m^{\top} (\pmb{D}_m \pmb{D}_m^{\top})^{-1} \pmb{\epsilon}^{(m)} = \pmb{\epsilon}^{(m)}$ and that $\pmb{1}^{\top} \pmb{\beta}^{(m)} =  \pmb{1}^{\top} \pmb{D}_m^{\top} (\pmb{D}_m \pmb{D}_m^{\top})^{-1} \pmb{\epsilon}^{(m)} = 0$.

The $k_m$ in Equation \@ref(eq:pr-rw-eps) equals $J_m \left(\text{tr}(\pmb{D}_m^{\top} (\pmb{D}_m \pmb{D}_m^{\top})^{-1} ((\pmb{D}_m \pmb{D}_m^{\top})^{-1})^{\top} \pmb{D}_m)\right)^{-1}$.

### Standard format

\begin{align}
  \pmb{H}^{(m)} & = \pmb{D}_m^{\top} (\pmb{D}_m \pmb{D}_m^{\top})^{-1} \\
  \pmb{Q}^{(m)} & = k_m^{-1} \tau_m^{-2} \pmb{I}_{J_m - 1}
\end{align}

### Code

`RW(scale = NULL)`

- `scale` is $A_m$. Defaults to $s$, as defined in Equation \@ref(eq:defn-s), if user does not supply a value.


## Second-order random walk {#sec:pr-rw2}

### Model

\begin{align}
  \pmb{\beta}^{(m)} & =  \pmb{l}^{(m)} \lambda_m + \pmb{D}_{2m}^{\top} (\pmb{D}_{2m} \pmb{D}_{2m}^{\top})^{-1} \pmb{\epsilon}^{(m)} \\
  \lambda_m & \sim \text{N}(0, B_m^2) \\
  \pmb{\epsilon}^{(m)} & \sim \text{N}(\pmb{0}, k_m \tau_m^2 \pmb{I}_{J_m - 2}) (\#eq:pr-rw2-eps) \\
  \tau_m & \sim \text{N}^+(A_m^2)
\end{align}
where $\pmb{l}^{(m)}$ is a vector, the $j$th element of which is
\begin{equation}
  l_j^{(m)}  = -\frac{J_m + 1}{J_m - 1} + \frac{2}{J_m - 1} j,
\end{equation}
and $\pmb{D}_{2m}$ is a second-order difference matrix formed by multiplying two first-order difference matrices.
The $\pmb{l}^{(m)} \lambda_m$ term is needed because the columns of $\pmb{D}_{2m}^{\top} (\pmb{D}_{2m} \pmb{D}_{2m}^{\top})^{-1}$ do not span the space defined by $\sum_{j=1}^{J_m}{\beta_j^{(m)}} = 0$. See, for instance, @currie2002flexible [336].

We have $\pmb{D}_{2m} \pmb{\beta}^{(m)} =  \pmb{D}_{2m}^{\top} \pmb{l}^{(m)} \lambda_m + \pmb{D}_{2m} \pmb{D}_{2m}^{\top} (\pmb{D}_{2m} \pmb{D}_{2m}^{\top})^{-1} \pmb{\epsilon}^{(m)} = \pmb{\epsilon}^{(m)}$ and $\pmb{1}^{\top} \pmb{\beta}^{(m)} = 0$.

The $k_m$ in Equation \@ref(eq:pr-rw2-eps) equals $J_m \left(\text{tr}(\pmb{D}_{2m}^{\top} (\pmb{D}_{2m} \pmb{D}_{2m}^{\top})^{-1} ((\pmb{D}_{2m} \pmb{D}_{2m}^{\top})^{-1})^{\top} \pmb{D}_{2m})\right)^{-1}$.


### Standard format

\begin{align}
  \pmb{H}^{(m)} & = \begin{bmatrix} \pmb{l}^{(m)} & \pmb{D}_{2m}^{\top} (\pmb{D}_{2m} \pmb{D}_{2m}^{\top})^{-1} \end{bmatrix} \\
  \pmb{Q}^{(m)} & = \begin{bmatrix} B_m^2 & \pmb{0}^{\top} \\
                                    \pmb{0} & k_m^{-1} \tau_m^{-2} \pmb{I}_{J_m - 2} \end{bmatrix}
\end{align}



### Code

`RW2(sd = NULL, scale = NULL)`

- `sd` is $B_m$. Defaults to $s$, as defined in Equation \@ref(eq:defn-s), if user does not supply a value.
- `scale` is $A_m$. Defaults to $s$ if user does not supply a value.


## AR1 {#sec:pr-ar1}

### Model

\begin{align}
  \beta_1^{(m)} & \sim \text{N}(0, \tau_m^2) \\
  \beta_j^{(m)} & \sim \phi_m \beta_{j-1}^{(m)} + \epsilon_j^{(m)}, \quad j = 2, \cdots, J_m \\
  \epsilon_j^{(m)} & \sim \text{N}\left(0,  (1 - \phi^2) \tau_m^2\right), \quad j = 2, \cdots, J_m \\
  \phi_m & = a_{0,m} + (a_{1,m} - a_{0,m}) \phi_m^{\prime} \\
  \phi_m^{\prime} & \sim \text{Beta}(2, 2) \\
  \tau_m & \sim \text{N}^+(A_m^2).
\end{align}
This is adapted from the specification used for AR1 densities in [TMB](http://kaskr.github.io/adcomp/classdensity_1_1AR1__t.html). We require that $-1 < a_{0m} < a_{1m} < 1$.

### Standard format

\begin{align}
  \pmb{H}^{(m)} & = \pmb{L}^{(m)} \\
  \pmb{Q}^{(m)} & = \frac{1 - \phi^2}{k_m \tau_m^2} \pmb{T}^{(m)}
\end{align}
where $\pmb{L}^{(m)}$ is the lower triangular matrix from the Cholesky decomposition of 
\begin{equation}
  \begin{bmatrix} 1 & \phi & \phi^2 & \phi^3 & \phi^4\\
                  \phi & 1 & \phi & \phi^2 & \phi^3\\
                  \phi^2 & \phi & 1 & \phi & \phi^2 \\
                  \phi^3 & \phi^2 & \phi & 1 & \phi \\
		  \phi^4 & \phi^3 & \phi^2 & \phi & 1 \end{bmatrix},
\end{equation}
and has the form
  \begin{bmatrix} 1 & 0 & 0 & 0 & 0 \\
                  \phi & \sqrt(1 - \phi^2) & 0 & 0 & 0 \\
                  \phi^2 & \phi \sqrt(1 - \phi^2) & \sqrt(1 - \phi^2) & 0 & 0 \\
                  \phi^3 & \phi^2 \sqrt(1 - \phi^2) & \phi \sqrt(1 - \phi^2) & \sqrt(1 - \phi^2) & \phi \\
		  \phi^4 & \phi^3 \sqrt(1 - \phi^2) & \phi^2 \sqrt(1 - \phi^2) & \phi \sqrt(1 - \phi^2) & \sqrt(1 - \phi^2) \end{bmatrix}.
\end{equation}



### Code

`AR1(min = 0.8, max = 0.98, scale = NULL)`

- `min` is $a_{0m}$
- `max` is $a_{1m}$
- `scale` is $A_m$. Defaults to $s$, as defined in Equation \@ref(eq:defn-s), if user does not supply a value.

The defaults for `min` and `max` are based on the defaults for function `ets()` in R package **forecast** [@hyndman2008automatic].


## Linear with independent errors {#sec:pr-lin}

### Model

\begin{align}
  \pmb{\beta}^{(m)} & = \pmb{l}^{(m)} \lambda_m + \pmb{\epsilon}^{(m)} \\
  \epsilon_j^{(m)} & \sim \text{N}(0, \tau_m^2) \\
  \lambda_m & \sim \text{N}(0, B_m^2) \\
  \tau_m & \sim \text{N}^+(A_m^2)
\end{align}
where $\pmb{l}^{(m)}$ is defined as in Section \@ref(sec:pr-rw2).

### Standard format

\begin{align}
  \pmb{H}^{(m)} & = \begin{bmatrix} \pmb{l}^{(m)} & \pmb{I}_{J_m} \end{bmatrix} \\
  \pmb{Q}^{(m)} & = \begin{bmatrix} B_m^2 & \pmb{0}^{\top} \\
                                    \pmb{0} & k_m^{-1} \tau_m^{-2} \pmb{I}_{J_m} \end{bmatrix}
\end{align}


### Code

`Lin(sd = NULL, scale = NULL)`

- `sd` is $B_m$. Defaults to $s$, as defined in Equation \@ref(eq:defn-s), if user does not supply a value.
- `scale` is $A_m$. Defaults to $s$ if user does not supply a value.


## Linear with AR1 errors {#sec:pr-lin-ar1}

### Model

\begin{align}
  \pmb{\beta}^{(m)} & = \pmb{l}^{(m)} \lambda_m + \pmb{\epsilon}^{(m)} \\
  \epsilon_1^{(m)} & \sim \text{N}(0, \tau_m^2) \\
  \epsilon_j^{(m)} & \sim \phi_m \epsilon_{j-1}^{(m)} + \epsilon_j^{(m)}, \quad j = 2, \cdots, J_m \\
  \phi_m & = a_{0,m} + (a_{1,m} - a_{0,m}) \phi_m^{\prime} \\
  \phi_m^{\prime} & \sim \text{Beta}(2, 2) \\
  \tau_m & \sim \text{N}^+(A_m^2)
\end{align}
where $k_m = (1 - \phi_m^2)$ and $-1 < a_{0m} < a_{1m} < 1$.


### Standard format

\begin{align}
  \pmb{H}^{(m)} & = \begin{bmatrix} \pmb{l}^{(m)} & \pmb{I}_{J_m} \end{bmatrix} \\
  \pmb{Q}^{(m)} & = \frac{1 - \phi^2}{k_m \tau_m^2} \pmb{T}^{(m)}
\end{align}
where $\pmb{T}^{(m)}$ has the form
\begin{equation}
  \begin{bmatrix} 1 & -\phi & 0 & 0 & 0\\
                  -\phi & 1 + \phi^2 & -\phi & 0 & 0 \\
                  0 & -\phi & 1 + \phi^2 & -\phi & 0 \\
                  0 & 0 & -\phi & 1 + \phi^2 & -\phi \\
		  0 & 0 & 0 & -\phi & 1 \end{bmatrix}.
\end{equation}



### Code

`LinAR1(sd = NULL, min = 0.8, max = 0.98, scale = NULL)`

- `sd` is $B_m$. Defaults to $s$, as defined in Equation \@ref(eq:defn-s), if user does not supply a value.
- `min` is $a_{0m}$.
- `max` is $a_{1m}$.
- `scale` is $A_m$. Defaults to $s$ if user does not supply a value.


## P-Spline {#sec:pr-spline}

### Model

\begin{equation}
  \pmb{\beta}^{(m)} = \pmb{B}^{(m)} \pmb{\alpha}^{(m)}
\end{equation}
where $\pmb{B}^{(m)} = (\pmb{b}_1^{(m)}(\pmb{j}), \cdots, \pmb{b}_{K_m}^{(m)}(\pmb{j}))$ is a $J_m \times K_m$ matrix of B-splines, with $\pmb{j} = (1, \cdots, J_m)^{\top}$. The B-splines are centered, so that $\pmb{1}^{\top} \pmb{b}_k^{(m)}(\pmb{j}) = 0$, $k = 1, \cdots, K_m$.

$\pmb{\alpha}^{(m)}$ has a second-order random walk prior, as described in Section \@ref(sec:pr-rw2).


### Standard format

\begin{align}
  \pmb{H}^{(m)} & = \begin{bmatrix} \pmb{B}^{(m)} \pmb{l}^{(m)} & \pmb{B}^{(m)} \pmb{D}_{2m}^{\top} (\pmb{D}_{2m} \pmb{D}_{2m}^{\top})^{-1} \end{bmatrix} \\
  \pmb{Q}^{(m)} & = \begin{bmatrix} B_m^2 & \pmb{0}^{\top} \\
                                    \pmb{0} & k_m^{-1} \tau_m^{-2} \pmb{I}_{K_m - 2} \end{bmatrix}
\end{align}
where $\pmb{l}^{(m)}$ is a vector with $K_m$ elements, the $k$th element of which is
\begin{equation}
  l_k^{(m)}  = -\frac{K_m + 1}{K_m - 1} + \frac{2}{K_m - 1} k,
\end{equation}
and $\pmb{D}_{2m}$ has dimension $(K_m - 2) \times K_m$.

### Code

`Spline(df, scale = NULL)`

- `df` is $K_m$. Defaults to $\max(0.7 J_m, 4)$. *This is the current default for BayesRates. Could we use a smaller multiplier?*
- `scale` is the $A_m$ from the second-order random walk prior. Defaults to $s$, as defined in Equation \@ref(eq:defn-s), if user does not supply a value.

## SVD {#sec:pr-svd}

### Model

The SVD prior is used only for age. The prior is
\begin{align}
  \pmb{\beta}^{\text{age}} & = \pmb{F}^{\text{age}} \pmb{\alpha}^{\text{age}} + \pmb{a} \\
  \alpha_j^{\text{age}} & \sim \text{N}(0, 1)
\end{align}
where $\pmb{F}^{\text{age}}$ is a $J_{\text{age}} \times K_{\text{age}}$ matrix, and $\pmb{a}$ is a vector with $J_{\text{age}}$ elements, both derived from a singular value decomposition (SVD) of an external dataset of age-specific values. The construction of $\pmb{F}^{\text{age}}$ and $\pmb{a}$ is described in Appendix \@ref(app:svd-age). The construction involves centering and scaling, which allows us to use a simple prior for $\pmb{\alpha}$.

Internally, we include $\pmb{a}$ in the offset term $\pmb{o}$ in Equation \@ref(eq:means). The SVD prior without the $\pmb{a}$ has the same format as other main effects priors.

### Standard format

\begin{align}
  \pmb{H}^{(m)} & = \pmb{F}^{\text{age}} \\
  \pmb{Q}^{(m)} & = \pmb{I}_{K_m}
\end{align}



### Code

`SVD(transform, translate)`

`SVD_HMD()`

`SVD_HFD()`

- `transform` is $\pmb{F}^{\text{age}}$
- `translate` is $\pmb{a}$
- functions such as `SVD_HMD()` and `SVD_HFD()` (for Human Mortality Database and Human Fertility Database) provide pre-packaged values.

### Combined dimensions


# Priors for second-order interactions {#sec:pr-interact-2}


# Priors for higher-order interactions {#sec:pr-interact-high}


# Covariates {#sec:cov}




## Generic covariates

Regularised horseshoe prior [@piironen2017hyperprior]



## Seasonal effects

### Model

A main effect for time can be extended through the addition of a seasonal effect. Seasonal effects are modelled by a vector $\pmb{\Lambda}$ with $S \ge 2$ elements. The seasonal effect in period $j$ is $\Lambda_{s_j}$ where $s_j = (j \bmod S) + 1$.

We model $\pmb{\Lambda}$ as a draw from a multivariate normal distribution, constrained to sum to zero,
\begin{align}
  \pmb{\Lambda} & = \pmb{G}_m \pmb{\varepsilon}^{(m)} \\
  \pmb{\varepsilon}^{(m)} & \sim \text{N}\left(\pmb{0}, k_m \tau_m^2 \pmb{I}_{J_m-1}\right) \\ 
  \tau_m & \sim \text{N}^+(A_m^2)
\end{align}
where
\begin{equation}
  \pmb{G}_m = \frac{1}{\sqrt{J_m}} 
  \begin{bmatrix} \pmb{1}^{\top} \\
                  \pmb{I}_{J_m - 1} - \frac{1}{J_m - 1} \pmb{1} \pmb{1}^{\top} \end{bmatrix},
\end{equation}
and $k_m = J_m / (J_m - 1)$.

### Code

`set_season(<term> ~ Season(m, scale = NULL))`

- `m` is the number of seasons, i.e. the number of elements of $\Lambda$
- `scale` is $A_m$. Defaults to $s$, as defined in Equation \@ref(eq:defn-s), if user does not supply a value.




# Priors for dispersion terms

## Poisson

\begin{equation}
  p(\xi) = \frac{A}{2 \sqrt{\xi}}e^{-A \sqrt{\xi}}
\end{equation}
[@simpson2022priors] for some value of $A$. TODO - choice of $A$.

## Binomial

\begin{equation}
  p(\xi) = \frac{A}{2 \sqrt{\xi}}e^{-A \sqrt{\xi}}
\end{equation}
for some value of $A$. TODO - choice of $A$.


## Normal

\begin{equation}
  \xi \sim \text{N}^+(0, A_m^2)
\end{equation}

## Log-Normal

\begin{equation}
  \xi \sim \text{N}^+(0, A_m^2)
\end{equation}




# Appendices

## Definitions {#app:defn}


| Quantity | Definition                                          |
|:---------|:----------------------------------------------------|
| $i$      | Index for cell, $i = 1, \cdots, n$.
| $y_i$    | Value for outcome variable.              |
| $w_i$    | Exposure, number of trials, or weight. |
| $\gamma_i$ | Super-population rate, probability, or mean. |
| $\mu_i$ | Cell-specific mean. |
| $\xi$   | Dispersion parameter. |
| $s$ | Generic scale term used across model. Equal to 1 or $\text{sd}(y_k)$. |
| $g$ | Log, logit, or identity function. |
| $\beta^{(0)}$ | Intercept. |
| $\pmb{\beta}^{(m)}$ | Main effect or interaction. $m = 1, \cdots, M$.  |
| $\beta_j^{(m)}$ | $j$th element of $\pmb{\beta}^{(m)}$. $j = 1, \cdots, J_m$. |
| $\tau_m^2$ | Variance parameter for main effect or interaction. |
| $\lambda^{(m)}$ | Parameter specific to $m$th main effect or interaction. |


## SVD prior for age  {#app:svd-age}

Let $\pmb{M}$ be a matrix of age-specific estimates from an international database, transformed to take values in the range $(-\infty, \infty)$. Each column of $\pmb{M}$ represesents one set of age-specific estimates, such as log mortality rates in Japan in 2010, or logit labour participation rates in Germany in 1980.

Let $\pmb{U}$, $\pmb{D}$, $\pmb{V}$ be the matrices from a singular value decomposition of $\pmb{M}$, where we have retained the first $K$ components (with $K$ much less than $A$). Then
\begin{equation}
  \pmb{M} \approx \pmb{U} \pmb{D} \pmb{V}. (\#eq:svd1)
\end{equation}

Let $g_k$ and $h_k$ be the mean and sample standard deviation of the elements of the $k$th row of $\pmb{V}$, with $\pmb{g} = (g_1, \cdots, g_K)^{\top}$ and $\pmb{h} = (h_1, \cdots, h_K)^{\top}$. Then
\begin{equation}
  \tilde{\pmb{V}} = (\text{diag}(\pmb{h}))^{-1} (\pmb{V} - \pmb{g} \pmb{1}^{\top})
\end{equation}
is a standardized version of $\pmb{V}$.

We can rewrite \@ref(eq:svd1) as
\begin{align}
  \pmb{M} & \approx \pmb{U} \pmb{D} (\text{diag}(\pmb{h}) \tilde{\pmb{V}} + \pmb{g} \pmb{1}^{\top}) \\
  & = \pmb{F} \tilde{\pmb{V}} + \pmb{a} \pmb{1}^{\top}, (\#eq:svd2)
\end{align}
where $\pmb{F} = \pmb{U} \pmb{D} \text{diag}(\pmb{h})$ and $\pmb{a} = \pmb{U} \pmb{D} \pmb{g}$.

Let $\tilde{\pmb{v}}_l$ be a randomly-selected column from $\tilde{\pmb{V}}$. From the construction of $\tilde{\pmb{V}}$, and the orthogonality of the rows of $\pmb{V}$, we have $\text{E}[\tilde{\pmb{v}}_l] = \pmb{0}$ and $\text{var}[\tilde{\pmb{v}}_l] = \pmb{I}$. This implies that if $\pmb{z}$ is a vector of standard normal variables, then
\begin{equation}
  \pmb{F} \pmb{z} + \pmb{a}
\end{equation}
looks approximately like a randomly-selected column from the original data matrix $\pmb{M}$.


## SVD prior for age-sex interactions  {#app:svd-agesex}



## Derivation of prior for dispersion term in binomial model {#app:disp}


The Kullback-Liebler divergence $KL(f_1 || f_0)$ of two beta distributions is

\begin{equation}
  \log \frac{\Gamma(\alpha_1 + \beta_1) \Gamma(\alpha_0) \Gamma(\beta_0)}{\Gamma(\alpha_0 + \beta_0) \Gamma(\alpha_1) \Gamma(\beta_1)}  + (\alpha_1 - \alpha_0) \left(\psi(\alpha_1) - \psi(\alpha_1 + \beta_1)\right)
  + (\beta_1 - \beta_0) \left(\psi(\beta_1) - \psi(\alpha_1 + \beta_1)\right)
\end{equation}
https://math.stackexchange.com/questions/257821/kullback-liebler-divergence#comment564291_257821

Using the parameterisation $v = \alpha + \beta$, $\mu = \alpha / (\alpha + \beta)$, this becomes
\begin{equation}
  \log \frac{\Gamma(v_1) \Gamma(\mu v_0) \Gamma((1 - \mu) v_0)}{\Gamma(v_0) \Gamma(\mu v_1) \Gamma((1 - \mu) v_1)} + (v_1 - v_0) \mu (\psi(\mu v_1) - \psi(v_1)) + (v_1 - v_0) (1 - \mu) (\psi((1-\mu) v_1) - \psi(v_1))
\end{equation}

If we substitute the approximation (valid for large $x$) $\log \Gamma(x) \approx (x - \frac{1}{2})\log x - x + \frac{1}{2} \log(2\pi)$
[https://en.wikipedia.org/wiki/Gamma_function] into the first term we obtain

\begin{align}
  & \left(v_1 - \frac{1}{2}\right) \log v_1 - v_1 \\
  & + \left(\mu v_0 - \frac{1}{2}\right) \left(\log \mu + \log v_0 \right) - \mu v_0 \\
  & + \left((1-\mu) v_0 - \frac{1}{2}\right) \left( \log (1-\mu) + \log v_0 \right) - (1-\mu) v_0 \\
  & - \left(v_0 - \frac{1}{2}\right) \log v_0 + v_0 \\
  & - \left(\mu v_1 - \frac{1}{2}\right) \left(\log \mu + \log v_1 \right) + \mu v_1 \\
  & - \left((1-\mu) v_1 - \frac{1}{2}\right) \left( \log (1-\mu) + \log v_1 \right) + (1-\mu) v_1 \\
=  & \left(v_1 - \frac{1}{2} - \mu v_1 + \frac{1}{2} - (1-\mu) v_1 + \frac{1}{2} \right) \log v_1 \\
  & + \left(\mu v_0 - \frac{1}{2} + (1-\mu) v_0 - \frac{1}{2} - v_0 + \frac{1}{2} \right) \log v_0 \\
  & + \left(\mu v_0 - \frac{1}{2} - \mu v_1 + \frac{1}{2} \right) \log \mu \\
  & + \left((1 - \mu) v_0 - \frac{1}{2} - (1 - \mu) v_1 + \frac{1}{2} \right) \log (1 - \mu) \\
  & + \left(-1 + \mu + (1 - \mu) \right) v_1 \\
  & + \left(-\mu - (1 - \mu) + 1 \right) v_0 \\
= & \frac{1}{2} \log v_1 - \frac{1}{2} \log v_0 - (v_1 - v_0) \mu \log \mu - (v_1 - v_0) (1 - \mu) \log (1-\mu) \\
= & \frac{1}{2} \log \frac{v_1}{v_0} - (v_1 - v_0) \left(\mu \log \mu + (1 - \mu) \log (1-\mu) \right)
\end{align}


\begin{equation}
 (v_1 - v_0) (\mu \psi(\mu v_1) + (1 - \mu) \psi((1 - \mu) v_1) - \psi(v_1))
\end{equation}

The baseline (binomial) model is obtained by letting $v_0$ go to infinity. When this happens, the first term above goes to zero. The second term goes to infinity, but we are hoping that the $v_0$ will drop out later.

Using the approximation $\psi(x) \approx \log x - \frac{1}{2}x$, the second term becomes
\begin{equation}
  (v_1 - v_0) \left(\mu \log \mu + (1 - \mu) \log (1 - \mu) + \mu (1 - \mu) v_1 \right)
\end{equation}



# References


